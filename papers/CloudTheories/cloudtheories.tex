\documentclass[10pt,journal,cspaper,compsoc]{IEEEtran}
\usepackage{listings}

%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[12pt,journal,compsoc]{../sty/IEEEtran}

\begin{document}

% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Cloud Theory Infected:\\ Using MapReduce To Exhaustively Verify Behaviour}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Julian~Friedman, Manuel~Oriol}% <-this % stops a space

\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem{ Julian Friedman is an Emerging Technology Specialist working for IBM Cloud Labs and IBM Emerging Technology Services, and a Research Engineer on the Large Scale Complex IT Systems project.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: julz.friedman@uk.ibm.com}
\IEEEcompsocthanksitem{Manuel Oriol is with York University [..]}}% <-this % stops a space
\thanks{}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~6, No.~1, January~2007}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2007 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2007 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)




% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEcompsoctitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
\IEEEcompsoctitleabstractindextext{%
\begin{abstract}
%\boldmath
The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.

The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.The abstract goes here.
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway. In particular, the Computer Society does
% not want either math or citations to appear in the abstract.

% Note that keywords are not normally used for peer review papers.
\begin{IEEEkeywords}
Test generation, Cloud Computing, Map/Reduce, unit Testing, Hadoop
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEcompsoctitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynotcompsoctitleabstractindextext when compsoc mode
% is not selected <OR> if conference mode is selected - because compsoc
% conference papers position the abstract like regular (non-compsoc)
% papers do!
\IEEEdisplaynotcompsoctitleabstractindextext
% \IEEEdisplaynotcompsoctitleabstractindextext has no effect when using
% compsoc under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% Computer Society journal papers do something a tad strange with the very
% first section heading (almost always called "Introduction"). They place it
% ABOVE the main text! IEEEtran.cls currently does not do this for you.
% However, You can achieve this effect by making LaTeX jump through some
% hoops via something like:
%
%\ifCLASSOPTIONcompsoc
%  \noindent\raisebox{2\baselineskip}[0pt][0pt]%
%  {\parbox{\columnwidth}{\section{Introduction}\label{sec:introduction}%
%  \global\everypar=\everypar}}%
%  \vspace{-1\baselineskip}\vspace{-\parskip}\par
%\else
%  \section{Introduction}\label{sec:introduction}\par
%\fi
%
% Admittedly, this is a hack and may well be fragile, but seems to do the
% trick for me. Note the need to keep any \label that may be used right
% after \section in the above as the hack puts \section within a raised box.

\IEEEPARstart{T}{heories} are a powerful technique allowing tests to be written about properties which hold against a large series of values whenever an assumptions about these values is true[ref]. They contrast with more traditional scenario-based tests which assert only how code behaves for a particular set of values. The advantage of theories is that they can make more general and useful statements about the code, and often describe more completely what a test aims to prove. 

A simple traditional unit test is presented below using JUnit 4.4 syntax:

\begin{lstlisting}[language=java]
@Test
public void greaterThanThree() {
   assertThat(foo(3),
                   is(greaterThan(3)));
}
\end{lstlisting}

A theory, by contrast, makes an assertion which is true for all values in a particular range, as below:

\begin{lstlisting}[language=java]
@Theory
public void alwaysGtThree(int input) {
   assertThat(foo(input), 
                   is(greaterThan(3)));
}
\end{lstlisting}


Theories, of course, are only as good as the sample of the input domain on which they are tested. Various approaches have been used to address this. At the most basic, the programmer or tester can provide a fixed set of input values for the tests. This approach is simple and efficient, but in most cases provides little guarantee that the theory has really been proven against the full input domain. Randomly selecting values from the space of possible values has been used with some success to fill this gap [ref], using an automated tool to generate input values based on introspection of composite objects and random generation of primitive values. Although one may guess that this would leave much of the input space uncovered, experimental results suggest that in many cases this will uncover a good proportion of the bugs in a given program [ref Meyer, Oriol, “Satisfying Test Preconditions through Guided Object Selection”]. Further, search-based techniques attempt to generate input values intelligently, for example using evolutionary algorithms [ref], white-box analysis [ref] and invariant-finding tools such as Daikon [ref]. The JUnit Factory web service [ref] uses a variety of these techniques and heuristics to generate input values for theories. 

For many applications these techniques are sufficient, however there are cases in which they fail. In particular, a stochastic technique relies on being able to quickly generate enough inputs to cover the input domain, and an input domain which is relatively evenly distributed such that bugs are distributed within it such that a sampling method will find most of the bugs. Ideally a  stochastic or directed technique is roughly equivalent to an opinion poll; it provides a good approximation of the result of the eventual election at a fraction of the cost of polling the entire population. Such a technique is effective (mostly) for elections, but not for other scenarios. For example if attempting to find a murderer in a large population, DNA testing every thousandth person will meet limited success. In the absence of knowledge of how a bug is likely to be distributed in the input domain, stochastic testing is likely to be useful only as a tool for finding bugs, but not as a tool which will assure us that bugs do not exist (or that a property truly holds for an entire input domain).

Cloud computing provides IT resources as a service in a pay-per-use manner. Services such as Amazon's EC2 and IBM's Smart Business Cloud provide access to large numbers of "on demand" virtual machines which can be paid for by the hour. The availablility of these resources can make techniques which previously would be prohibitively expensive practical to use.

Up to now the idea of exhaustively testing a code base, by generating all of the inputs for a particular theory has seemed likely to be prohibitively expensive. In this paper we consider using Cloud Computing resources to partition Theory tests across a large number of machines in order to thoroughly test an application. We argue that for certain applications, particularly safety critical systems, by making this technique cost effective and easy to use, this approach may supplement tools such as formal methods and static analysis in providing assurance of the behaviour of the application. Further, if succesful this technique could be extended to general Design By Contract verification.

The paper is structured as follows. Section II describes the architecture of our solution for partitioning and distributing theory tests across a set of machines using Map/Reduce. Section III discusses our experimental results running against a variety of open source systems. Section IV is a discussion of these results and the practical limits of these techniques. Section V concludes with thoughts about related work and future research directions. 

% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.

\section{Architecture}

The open-source hadoop implementation of the Map/Reduce paradigm is used to distribute the jobs. Using this basic framework to run the tests enables the use of the many existing, relatively cheap map/reduce clusters which are available on a pay-per-hour basis, such as Amazon’s Elastic Map Reduce service [ref]. 

Hadoop can be thought of as the combination of a distributed file system and a task management framework which effectively distributes data-parallel tasks against the filesystem. Much of the power of the system is its ability to send the code to the data rather than pulling data to the code as in more traditional systems. Further, the simple nature of Map/Reduce allows all of a large class of problems which can be expressed in it to be parallelized and optimized by a relatively simple framework [ref]. 

Map/Reduce is an algorithmic skeleton[ref] which encourages the expression of a problem in a parallelizable form by splitting the work in to two phases. The first, 'map', phase runs a series of independent jobs over partitions of the input data. The second, 'reduce' phase combines the results of the map jobs in to a final result. 

In our case each test case is fully independent (in fact, this is not only more efficient, but needed to ensure each test is unaffected by the others) but must be run against a large number of input values; this pattern is highly parallelizable using a series of map tasks against the input data. The input to each map task is a subset of the range of values for a particular theory parameter. The reduce jobs receive the passes and failures from the map phase and combine them in to a single summary result. 

We have constructed a JUnit theory runner which automatically creates a map/reduce job based on the given parameters and returns passes and failures to JUnit, allowing the results to appear within the IDE. Because it is important that the tool be easy to use within a programmers workflow, we have also created a plugin for the popular Eclipse Java IDE which automatically packages the project under test in to a JAR file and runs the test cases across a remote cloud using Amazon's Elastic Map Reduce service[ref], returning the results in the standard graphical view with the IDE. This allows one click verification of the theories in a project and the remote environment creates virtual machines for the test run and destroys them afterwards.

[Diagram]

\subsection{Pluggable Test Data generation}

For simple inputs, such as integers, we can generate all values very simply, however for many real world cases we cannot trivially generate inputs. A particularly difficult case is textual input. While we could, of course, generate every possible piece of text under a certain length as input this (very) quickly becomes intractable. Luckily, for many applications with textual inputs, there is a smaller domain of valid inputs which we want to investigate. As an example we may wish to test against all valid HTML pages constructed from a particular set of tags under a given length, or all pages found on the internet in the case of a parsing tool; a much smaller space than all possible texts of a given length. 

Our tool supports pluggable test data generation factories which can be used to provide valid candidate inputs for a given input domain. We support two mechanisms for the use of these factories. In many cases the test designer is concerned with a particular input domain and will wish to specify a factory directly in a similar manner to tools such as JUnit [ref] allow for local test data generation, however in others a class may itself be able to provide a factory which can generate valid instances of itself. For this reason we allow a java annotation on a target class to suggest a factory which can be used to create it. The key advantage of this technique is that these factories can be used to automatically create other classes which take the base class as a constructor argument throough recursion, and can be used to create mock objects [need more detail on these probably] for collaborator classes.

\subsection{Limiting the test size safely}

[Argue that we can split the program in to a pass known to be safe under certain preconditions, and a pass that we are confident limits input to a particular postcondition. Each can be tested independently without testing the full space of inputs. ]

\subsection{Generation of Mock Collaborator Objects}

[..]

\subsection{Pre-distribution of test data}

Large data-sets tend to be expensive to create and distribute. Further, in many cases multiple tests require the same test data. Rather than generating the data during the test, as existing solutions tend to do, our approach is to create the test data the first time a test is run if it does not exist. When the test is run we check the distributed filesystem for a directory named uniquely for the test data generation strategy and version. If it does not exist the factory is allowed to produce the test data and write it to the given distributed directory. If a factory changes its generation method significantly, for example to fix a bug or add a new case, it can update a static public integer within its definition to indicate we should consider regenerating the test data.

\subsection{Integration with Eclipse}

[..]

\section{Validation}

We have validated our approach by exhaustively testing a number of open source packages. 

[.. Fill out all of this section ..]

\section{Discussion}

[This overlaps too much with the intro I think, need to rewrite to discuss the results in validation once we have them]

Broadly we can consider two ways to cover a large input space. We might decide to fully explore the entire space, or we may take a random sampling of the space. One could say this is the difference between an election and an opinion poll. For many purposes sampling the space, particularly using an intelligent sampling which is weighted towards the most likely errors (for example using invariants analysis[ref] or other white-box techniques [refs]) will discover a large portion of the errors in a program at a fraction of the testing cost. So where is the value in exhaustive testing? This essentially occurs in two cases: 

Firstly there are applications for which a sampling method does not provide the necessary assurance, safety-critical systems being the primary example. These systems have often resorted to tools such as formal methods which also introduce a large cost and can fail due to differences between the formal model and the eventual implementation[ref]. We argue that in this case exhaustive testing can be a practical tool, if large scale computing resources is accessible and easily usable on a pay-per-use basis, as it increasingly is today.

Secondly, for some applications it is difficult for an automatic tool to discover the inputs which will adequately cover the input space, in particular for structured textual inputs the full space of potential inputs is extremely large but only a small subset are valid inputs for the program. In this case the majority of randomly generated test data is likely to be quickly rejected without exercising much code (for example consider testing a Java compiler against randomly generated textual strings; the majority will be almost instantly rejected). White box tools may fare better here, being able to adaptively tailor the inputs to attempt to explore all invariants, but again doing this with a large space of textual inputs becomes extremely difficult [ref]. 

For both of these cases, we assert that exhaustive testing is sometimes a necessary evil. This is particularly valuable in combination with regression testing, for example, testing that a new compiler version produces the same output as a previous version against a large body of existing code. Randomly selecting classes is unlikely to find the one or two edge cases where behaviour subtly varies. 

\subsection{Other Use Cases}

[ - Verify that a virus checker does not produce false positive results for any page within WikiPedia unless that page is black listed 
  - Whiley uses theorem prover.. Supplement with CTs
- ..]

\section{Conclusion}

The availability of relatively cheap, pay-per-hour "utility" computing resources provided by Cloud Computing services brings techniques which would previously have been impractical in to the reach of software engineers. We have demonstrated that exhaustively testing properties of a codebase against many types of inputs is now feasible. Our technique goes beyond simply finding bugs in a codebase and allows for assurance that a particular property holds for all given input values. We have shown that using Map/Reduce resources we can achieve [measured speed-up] in scaling and distributing this task. 
\\

\subsection{Related Work}

Cloud Theories build heavily on work on Theory Tests and Design by Contract. [JML, JUnit, TestNG, Theory Infected refs]

Stochastic testing tools such as Yeti and JCrasher [..]

Rather than exhaustively testing a codebase we can attempt to find bugs by intelligently adapting our input data towards values likely to cause bugs by analysing code invariants or branch structure. Tools such as [ref DEVONThink/SearchBased papers]

\subsection{Future Directions}

We have shown that exhaustive and theory-based tests against large input domains can be effectively parallelized using Map/Reduce techniques and demonstrated that for many problems these techniques may now be a cost-effective complement to tools such as Formal Methods and Static Analysis. Further research will look to investigate whether similar techniques can also be adopted to parallelize and scale other testing techniques such as invariant-based test data selection and other search-based techniques, particularly with distributed genetic algorithms. Extending the model here to testing JML-based contracts would allow exhaustive specification of code contracts as well as Theories.

[..]

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE CS typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE CS papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% However, Computer Society journals sometimes do use bottom floats - bear
% this in mind when choosing appropriate optional arguments for the
% figure/table environments.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.




% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%



% use section* for acknowledgement
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors would like to thank...The authors would like to thank...The authors would like to thank...The authors would like to thank...The authors would like to thank...The authors would like to thank...The authors would like to thank...The authors would like to thank...The authors would like to thank...The authors would like to thank...The authors would like to thank...The authors would like to thank...The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
%This is an example of a book reference
H. Kopka and P.W. Daly, \emph{A Guide to {\LaTeX}}, third ed. Harlow, U.K.: Addison-Wesley, 1999.


%This is an example of a Transactions article reference
%D.S. Coming and O.G. Staadt, "Velocity-Aligned Discrete Oriented Polytopes for Dynamic Collision Detection," IEEE Trans. Visualization and Computer Graphics, vol.�14,� no.�1,� pp. 1-12,� Jan/Feb� 2008, doi:10.1109/TVCG.2007.70405.

%This is an example of a article from a conference proceeding
%H. Goto, Y. Hasegawa, and M. Tanaka, "Efficient Scheduling Focusing on the Duality of MPL Representation," Proc. IEEE Symp. Computational Intelligence in Scheduling (SCIS '07), pp. 57-64, Apr. 2007, doi:10.1109/SCIS.2007.367670.

%This is an example of a PrePrint reference
%J.M.P. Martinez, R.B. Llavori, M.J.A. Cabo, and T.B. Pedersen, "Integrating Data Warehouses with Web Data: A Survey," IEEE Trans. Knowledge and Data Eng., preprint, 21 Dec. 2007, doi:10.1109/TKDE.2007.190746.

%Again, see the IEEEtrans_HOWTO.pdf for several more bibliographical examples. Also, more style examples
%can be seen at http://www.computer.org/author/style/transref.htm
\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text %here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text %here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text %here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text %here.Biography text here.Biography text here.Biography text here.Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}



