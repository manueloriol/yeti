
@Inproceedings{Simula.se.735,
  author = {Arcuri, Andrea and Iqbal, Muhammad Zohaib and Briand, Lionel},
  title = {Formal Analysis of the Effectiveness and Predictability of Random Testing},
  year = {2010},
  abstract = {There has been a lot of work to shed light on whether Random Testing is actually a useful testing technique. Despite its simplicity, several successful real-world applications appear in the literature. Although it is not going to solve all possible testing problems, random testing is an essential tool in the hands of software testers. In this paper, we address general questions about random testing, such as how long random testing needs on average to achieve testing targets (e.g., coverage), how does it scale and how likely is it to yield similar results if we re-run random testing on the same testing problem. Due to its simplicity that makes the mathematical analysis of random testing tractable, we provide precise and rigorous answers to these questions. Our formal results can be applied to most types of software and testing criteria. Simulations are carried out to provide further support to our formal results. The obtained results are then used to assess the validity of empirical analyses reported in the literature.},
  booktitle = {ACM International Conference on Software Testing and Analysis (ISSTA)},
  editor = {Paolo Tonella, 	Alessandro Orso},
  publisher = {ACM},
  isbn = {978-1-60558-823-0}
}

@inproceedings{DBLP:conf/iceccs/Oriol2010,
  author    = {Manuel Oriol and Sotirios Tassis},
  title     = {Testing .NET Code with YETI},
  booktitle = {15th IEEE International Conference on Engineering of Complex
               Computer Systems, ICECCS 2010, Oxford, United Kingdom, 22-26 March,
               2010},
  year      = {2010},
  crossref  = {DBLP:conf/iceccs/2010},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}
@proceedings{DBLP:conf/iceccs/2010,
  title     = {15th IEEE International Conference on Engineering of Complex
               Computer Systems, ICECCS 2010, Oxford, United Kingdom, 22-26 March,
               2010},
  booktitle = {ICECCS},
  publisher = {IEEE Computer Society},
  year      = {2009},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@Inproceedings{CMOP:08:FFMTRTUR,
  author = 	 {Ilinca Ciupa and Manuel Oriol and Bertrand Meyer and Alexander Pretschner },
  title = 	 {Finding Faults: Manual Testing vs. Random+ Testing vs. User Reports},
  bookTitle =	 {IEEE International Symposium on Software Reliability Engineering (ISSRE) },
  month =        {Nov},
  year =         {2008},
}
@INPROCEEDINGS{Claessen00quickcheck:a,
    author = {Koen Claessen and John Hughes},
    title = {QuickCheck: A Lightweight Tool for Random Testing of Haskell Programs},
    booktitle = {ACM SIGPLAN Notices},
    year = {2000},
    pages = {268--279},
    publisher = {ACM Press}
}

@inproceedings{CLOM:08:AARTOO,
  author = 	 {Ilinca Ciupa and Andreas Leitner and Manuel Oriol and Bertrand Meyer },
  title = 	 "{ARTOO: Adaptive Random Testing for Object-Oriented Software}",
  bookTitle =	 {International Conference on Software Engineering (ICSE 2008)},
  month =        {april},
  year =         {2008},
  
}
@Inproceedings{CPLOM:08:PRTOOS,
  author = 	 {Ilinca Ciupa and Alexander Pretschner and Andreas Leitner and Manuel Oriol and Bertrand Meyer},
  title = 	 {On the Predictability of Random Tests for Object-Oriented Software},
  bookTitle =	 {International Conference On Software Testing, Verification And Validation (ICST 2008)},
  month =        {July},
  year =         {2008},
}

@INPROCEEDINGS{Bousquet2004,
  author = {Lydie du Bousquet and Yves Ledru and Oliver Maury and Catherine Oriat
	and Jean-Louis Lanet},
  title = {Case Study in JML-Based Software Validation.},
  booktitle = {ASE},
  year = {2004},
  pages = {294-297},
  abstract = {This paper reports on a testing case study applied to a small Java
	application, partially specified in JML. It illustrates that JML
	can easily be integrated with classical testing tools based on combinatorial
	techniques and random generation. It also reveals difficulties to
	reuse, in a testing context, JML annotations written for a proof
	process.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/duBousquet04_mycomments.doc},
  crossref = {DBLP:conf/kbse/2004},
  ee = {http://csdl.computer.org/comp/proceedings/ase/2004/2131/00/21310294abs.htm},
  pdf = {D:\Ilinca\Docs\Related work\Case Study in JML-Based Software Validation.pdf},
  read = {Yes},
  relevance = {5},
  special_observations = {Met Yves Ledru at TAROT summer school 2005 in Paris},
  url = {http://www-lsr.imag.fr/Les.Personnes/Yves.Ledru/PublicationsYL.html},
}

@ARTICLE{Goodenough1975,
  author = {John B. Goodenough and Susan L. Gerhart},
  title = {Toward a theory of test data selection},
  journal = {IEEE Transactions On Software Engineering},
  year = {1975},
  volume = {1(2)},
  pages = {156--173},
  abstract = {This paper examines the theoretical and
	
	practical role of testing in software development.
	
	We prove a fundamental theorem showing that
	
	properly structured tests are capable of demonstrating
	
	the absence of errors in a program. The
	
	theorem's proof hinges on our definition of test
	
	reliability and validity , but its practical utility
	
	hinges on being able to show when a test is actually
	
	reliable . We explain what makes tests
	
	unreliable (for example, we show by example
	
	why testing all program statements, predicates,
	
	or paths is not usually sufficient to insure test
	
	reliability) , and we outline a possible approach
	
	to developing reliable tests . We also show how
	
	the analysis required to define reliable tests can
	
	help in checking a program's design and specifications
	
	as well as in preventing and detecting
	
	implementation errors.},
  crossref = {testing, proofs of correctness},
  owner = {Ilinca},
  read = {yes (not all)},
  relevance = {5},
  timestamp = {2006.04.05},
}

@INPROCEEDINGS{Offutt1999a,
  author = {A. Jefferson Offutt and Yiwei Xiong and Shaoying Liu},
  title = {Criteria for Generating Specification-Based Tests},
  booktitle = {Proceedings of the Fifth IEEE International Conference on Engineering
	of Complex Computer Systems (ICECCS '99)},
  year = {1999},
  pages = {119-131},
  month = {October},
  abstract = {This paper presents general criteria for generating test inputs from
	state-based specifications. Software testing can only be formalized
	and quantified when a solid basis for test generation can be defined.
	Formal specifications of complex systems represent a significant
	opportunity for testing because they precisely describe what functions
	the software is supposed to provide in a form that can easily be
	manipulated. These techniques provide coverage criteria that are
	based on the specifications, and are made up of several parts, including
	test prefixes that contain inputs necessary to put the software
	into the appropriate state for the test values. The test generation
	process includes several steps for transforming specifications to
	tests. Empirical results from a comparative case study application
	of these criteria are presented.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Offutt99_mycomments.doc},
  crossref = {DBLP:conf/iceccs/1999},
  ee = {http://csdl.computer.org/comp/proceedings/iceccs/1999/0434/00/04340119abs.htm},
  pdf = {D:\Ilinca\Docs\Related work\Criteria for Generating Specification-based Tests.pdf},
  read = {yes},
  relevance = {5},
}

@INPROCEEDINGS{Petrenko2001,
  author = {Alexandre Petrenko},
  title = {Specification Based Testing: Towards Practice.},
  booktitle = {Ershov Memorial Conference},
  year = {2001},
  pages = {287-300},
  abstract = {Specification based testing facilities are gradually becoming software
	production aids. The paper shortly considers the current state of
	the art, original ISPRAS/RedVerst experience, and outlines the ways
	for further research and testing tool development. Both conceptual
	and technical problems of novel specification based testing technologies
	introduction are considered.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Petrenko01_mycomments.doc},
  crossref = {DBLP:conf/ershov/2001},
  ee = {http://link.springer.de/link/service/series/0558/bibs/2244/22440287.htm},
  pdf = {D:\Ilinca\Docs\Related work\SpecificationBasedTesting_TowardsPractice.pdf},
  read = {yes},
  relevance = {3},
  url = {http://www.springerlink.com/app/home/contribution.asp?wasp=d9cd04e55a35405ca80a5450d1b8feb6&referrer=parent&backto=issue,29,52;journal,1260,2055;linkingpublicationresults,1:105633,1},
}

@INPROCEEDINGS{dAmorim2006,
  author = {Marcelo d'Amorim and Carlos Pacheco and Darko Marinov and Tao Xie
	and Michael D. Ernst},
  title = {An empirical comparison of automated generation and classification
	techniques for object-oriented unit testing},
  booktitle = {ASE 2006: Proceedings of the 21st Annual International Conference
	on Automated Software Engineering},
  year = {2006},
  address = {Tokyo, Japan},
  month = {September~20--22,},
  abstract = {Testing involves two major activities: generating test inputs and
	determining whether they reveal faults. Automated test generation
	techniques include random generation and symbolic execution. Automated
	test classification techniques include ones based on uncaught exceptions
	and violations of operational models inferred from manually provided
	tests. Previous research on unit testing for object-oriented programs
	developed three pairs of these techniques: model-based random testing,
	exception-based random testing, and exception-based symbolic testing.
	We develop a novel pair, model-based symbolic testing. We also empirically
	compare all four pairs of these generation and classification techniques.
	The results show that the pairs are complementary (i.e., reveal
	faults differently), with their respective strengths and weaknesses.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\an_empirical_comparison_of_automated_generation_and_classification_techniques_for_OO_unit_testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.11.10},
  url = {http://people.csail.mit.edu/cpacheco/publications/testgen-ase2006-abstract.html},
}

@TECHREPORT{Andrews2006,
  author = {J. H. Andrews and S. Haldar and Y. Lei and C. H. Li},
  title = {Randomized Unit Testing: Tool Support and Best Practices},
  institution = {Department of Computer Science, University of Western Ontario},
  year = {2006},
  number = {663},
  month = {January},
  abstract = {Randomization has long been used in testing, but it has not achieved
	widespread acceptance due to a lack of tool support and a failure
	to establish recognized best practices. In this paper, we describe
	RUTE-J, a Java package intended to provide tool support for randomized
	Java unit testing. We also discuss the best practices we have identified
	in our research on randomized unit testing. We report on case studies
	and an experiment in which we applied RUTE-J to various public-domain
	Java classes, finding failures even in mature software and supporting
	the claim that RUTE-J is an efficient, effective tool for unit testing.
	Finally, we compare the use of randomized unit testing to the use
	of other tools such as model checkers, and discuss the tradeoffs.
	We conclude that when best practices are followed, randomized unit
	testing with tool support is useful both as a preparation for full
	software model checking, and in its own right.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Randomized unit testing - tool support and best practices.pdf},
  read = {browsed through},
  relevance = {5},
  timestamp = {2006.04.09},
  url = {http://www.csd.uwo.ca/faculty/andrews/papers/},
}

@INPROCEEDINGS{Andrews2006a,
  author = {James H. Andrews and Susmita Haldar and Yong Lei and Felix Chun Hang
	Li},
  title = {Tool support for randomized unit testing},
  booktitle = {RT '06: Proceedings of the 1st international workshop on Random testing},
  year = {2006},
  pages = {36--45},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {There are several problem areas that must be addressed when applying
	
	randomization to unit testing. As yet no general, fully automated
	
	solution that works for all units has been proposed. We
	
	therefore have developed RUTE-J, a Java package intended to help
	
	programmers do randomized unit testing in Java. In this paper, we
	
	describe RUTE-J and illustrate how it supports the development
	
	of per-unit solutions for the problems of randomized unit testing.
	
	We report on an experiment in which we applied RUTE-J to the
	
	standard Java TreeMap class, measuring the efficiency and effectiveness
	
	of the technique. We also illustrate the use of randomized
	
	testing in experimentation, by adapting RUTE-J so that it generates
	
	randomized minimal covering test suites, and measuring the
	
	effectiveness of the test suites generated.},
  doi = {http://doi.acm.org/10.1145/1145735.1145741},
  isbn = {1-59593-457-X},
  location = {Portland, Maine},
  pdf = {D:\Ilinca\Docs\Related work\tool_support_for_randomized_unit_testing.pdf},
  read = {no},
  relevance = {6},
}

@ARTICLE{Artho2003,
  author = {C. Artho and H. Barringer and A. Goldberg and K. Havelund
	
	 and S. Khurshid and M. Lowry and C. Pasareanu and G. Ro\c{s}u and
	K. Sen
	
	 and W. Visser and R. Washington},
  title = {Combining Test Case Generation with Run-time Verification},
  journal = {ASM issue of Theoretical Computer Science},
  year = {2003},
  note = {To appear},
  abstract = {Software testing is typically an ad-hoc process where human testers
	manually write test
	
	inputs and descriptions of expected test results, perhaps automating
	their execution in a
	
	regression suite. This process is cumbersome and costly. This paper
	reports results on a
	
	framework to further automate this process. The framework consists
	of combining automated
	
	test case generation based on systematically exploring the input domain
	of the program
	
	with runtime verification, where execution traces are monitored and
	verified against
	
	properties expressed in temporal logic. The input domain of the program
	is explored using
	
	a model checker extended with symbolic execution. Properties are formulated
	in an expressive
	
	temporal logic. A methodology is advocated that generates properties
	specific to each
	
	input instance rather than formulating properties uniformly true for
	all inputs. Capabilities
	
	for analyzis of concurrency errors are planned to be integrated with
	temporal logic monitoring.
	
	The paper describes an application of the technology to a planetary
	rover controller.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Artho03_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\Combining test case generation and runtime verification.pdf},
  publisher = {Elsevier},
  read = {yes (but not entirely)},
  relevance = {5},
  url = {http://research.nii.ac.jp/~cartho/papers/asm-tcs.pdf},
}

@INPROCEEDINGS{Balcer1989,
  author = {M. Balcer and W. Hasling and T. Ostrand},
  title = {Automatic generation of test scripts from formal test specifications},
  booktitle = {TAV3: Proceedings of the ACM SIGSOFT '89 third symposium on Software
	testing, analysis, and verification},
  year = {1989},
  pages = {210--218},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {TSL is a language for writing formal test specifications of the functions
	of a software system. The test specifications are compiled into
	executable test scripts that establish test environments, assign
	values to input variables, perform necessary setup and cleanup operations,
	run the test cases, and check the correctness of test results. TSL
	is a working system that has been used to test commercial software
	in a production environment.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Balcer89_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/75308.75332},
  isbn = {0-89791-342-6},
  location = {Key West, Florida, United States},
  pdf = {D:\Ilinca\Docs\Related work\Automatic generation of test scripts from formal test specifications.pdf},
  read = {yes},
  relevance = {3},
  special_observations = {This paper is very similar to Ostrand88: it builds on the same ideas
	and doesn't bring any significant new information.},
}

@INPROCEEDINGS{Beyer2004,
  author = {Dirk Beyer and Adam J. Chlipala and Thomas A. Henzinger and Ranjit
	Jhala and Rupak Majumdar},
  title = {Generating Tests from Counterexamples},
  booktitle = {Proceedings of the 26th International Conference on Software Engineering
	(ICSE'04, Edinburgh)},
  year = {2004},
  pages = {326-335},
  publisher = {IEEE Computer Society Press},
  abstract = {We have extended the software model checker Blast to automatically
	generate test suites that guarantee full coverage with respect to
	a given predicate. More precisely, given a C program and a target
	predicate p, Blast determines the set L of program locations which
	program execution can reach with p true, and automatically generates
	a set of test vectors that exhibit the truth of p at all locations
	in L. We have used Blast to generate test suites and to detect dead
	code in C programs with up to 30K lines of code. The analysis and
	test-vector generation is fully automatic (no user intervention)
	and exact (no false positives).},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Beyer04_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\generating_tests_from_counterexamples.pdf},
  read = {yes},
  relevance = {5},
  url = {sherry.ifi.unizh.ch/beyer04generating.html},
}

@INPROCEEDINGS{Boshernitsan2006,
  author = {Marat Boshernitsan and Roongko Doong and Alberto Savoia},
  title = {From daikon to agitator: lessons and challenges in building a commercial
	tool for developer testing},
  booktitle = {ISSTA '06: Proceedings of the 2006 international symposium on Software
	testing and analysis},
  year = {2006},
  pages = {169--180},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/1146238.1146258},
  isbn = {1-59593-263-1},
  location = {Portland, Maine, USA},
  pdf = {D:\Ilinca\Docs\Related work\From_Daikon_to_Agitator.pdf},
}

@INPROCEEDINGS{Boyapati2002,
  author = {Chandrasekhar Boyapati and Sarfraz Khurshid and Darko Marinov},
  title = {Korat: automated testing based on Java predicates},
  booktitle = {Proceedings of the 2002 ACM SIGSOFT International Symposium on Software
	Testing and Analysis (ISSTA 2002), Rome, Italy},
  year = {2002},
  abstract = {This paper presents Korat, a novel framework for automated testing
	of Java programs. Given a formal specification for a method, Korat
	uses the method precondition to automatically generate all (nonisomorphic)
	test cases up to a given small size. Korat then executes the method
	on each test case, and uses the method postcondition as a test oracle
	to check the correctness of each output. To generate test cases
	for a method, Korat constructs a Java predicate (i.e., a method
	that returns a boolean) from the method's pre-condition. The heart
	of Korat is a technique for automatic test case generation: given
	a predicate and a bound on the size of its inputs, Korat generates
	all (nonisomorphic) inputs for which the predicate returns true.
	Korat exhaustively explores the bounded input space of the predicate
	but does so efficiently by monitoring the predicate's executions
	and pruning large portions of the search space. This paper illustrates
	the use of Korat for testing several data structures, including
	some from the Java Collections Framework. The experimental results
	show that it is feasible to generate test cases from Java predicates,
	even when the search space for inputs is very large. This paper
	also compares Korat with a testing framework based on declarative
	specifications. Contrary to our initial expectation, the experiments
	show that Korat generates test cases much faster than the declarative
	framework.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Boyapati02_mycomments.doc},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Korat - Automated Testing Based on Java Predicates.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.08.24},
}

@INPROCEEDINGS{Briand2002,
  author = {L. C. Briand and Y. Labiche and H. Sun},
  title = {Investigating the use of analysis contracts to support fault isolation
	in object oriented code},
  booktitle = {ISSTA '02: Proceedings of the 2002 ACM SIGSOFT international symposium
	on Software testing and analysis},
  year = {2002},
  pages = {70--80},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {A number of activities involved in testing software are known to be
	difficult and time consuming. Among them is the isolation of faults
	once failures have been detected. In this paper, we investigate
	how the instrumentation of contracts could address this issue. Contracts
	are known to be a useful technique to specify the precondition and
	postcondition of operations and class invariants, thus making the
	definition of object-oriented analysis or design elements more precise.
	Our aim in this paper is to reuse and instrument contracts to ease
	testing. A thorough case study is run where we define contracts,
	instrument them using a commercial tool, and assess the benefits
	and limitations of doing so to support the isolation of faults.
	We then draw practical conclusions regarding the applicability of
	the approach and its limitations.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Briand02_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/566172.566183},
  isbn = {1-58113-562-9},
  location = {Roma, Italy},
  pdf = {D:\Ilinca\Docs\Related work\Investigating the use of analysis contracts to support fault isolation in object oriented code.pdf},
  read = {yes},
  relevance = {6},
  url = {http://portal.acm.org/citation.cfm?doid=566172.566183},
}

@INPROCEEDINGS{Ceballos2005,
  author = {Rafael Ceballos and Rafael Martinez Gasca and Diana Borrego},
  title = {Constraint satisfaction techniques for diagnosing errors in design
	by contract software},
  booktitle = {SAVCBS '05: Proceedings of the 2005 conference on Specification and
	verification of component-based systems},
  year = {2005},
  pages = {11},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Design by Contract enables the development of more reliable and robust
	software applications. In this paper, a methodology that diagnoses
	errors in software is proposed. This is based on the combination
	of Design by Contract, Model-based Diagnosis and Constraint Programming.
	Contracts are specified by using assertions. These assertions together
	with an abstraction of the source code are transformed into constraints.
	The methodology detects if the contracts are consistent, and if
	there are incompatibilities between contracts and source code. The
	process is automatic and is based on constraint programming.},
  comments = {Not very understandable and does not go into any detail},
  doi = {http://doi.acm.org/10.1145/1123058.1123070},
  isbn = {1-59593-371-9},
  location = {Lisbon, Portugal},
  read = {yes},
  relevance = {5},
}

@ARTICLE{Chan1996,
  author = {F. T. Chan and T. Y. Chen and I. K. Mak and Y. T. Yu},
  title = {Proportional sampling strategy: guidelines for software testing practitioners},
  journal = {Information and Software Technology},
  year = {1996},
  volume = {38},
  pages = {775--782},
  number = {12},
  abstract = {Recently, several sufficient conditions have been developed that guarantee
	partition testing to have a higher probability of detecting at least
	one failure than random testing. One of these conditions is that
	the number of test cases selected from each partition is proportional
	to the size of the partition. We call such a method of allocating
	test cases the proportional sampling strategy. Although this condition
	is not the most general one, it is the most easily and practically
	applicable one. In this paper, we discuss how the proportional sampling
	strategy can be applied effectively in practice. Some practical
	issues that need to be attended are identified and guidelines to
	deal with these issues are suggested.},
  citeseerurl = {http://citeseer.ist.psu.edu/context/1179915/0},
  keywords = {Partition testing; Random testing; Selection of test data},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Proportional sampling strategy.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://www.ingentaconnect.com/content/els/09505849/1996/00000038/00000012/art01103;jsessionid=11wv0vigm8pia.alice},
}

@INPROCEEDINGS{Chan2002,
  author = {Kwok Ping Chan and Tsong Yueh Chen and Dave Towey},
  title = {Restricted Random Testing},
  booktitle = {Proceedings of the 7th International Conference on Software Quality},
  year = {2002},
  pages = {321 - 330},
  publisher = {Springer-Verlag, London, UK},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Restricted random testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.04.11},
}

@INPROCEEDINGS{Chen2004a,
  author = {T.Y. Chen and R. Merkel and P.K. Wong and G. Eddy},
  title = {Adaptive random testing through dynamic partitioning},
  booktitle = {Proceedings of the Fourth International Conference on Quality Software},
  year = {2004},
  volume = {00},
  pages = {79 - 86},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society},
  abstract = {Adaptive random testing (ART) describes a family of algorithms for
	generating random test cases that have been experimentally demonstrated
	to have greater fault-detection capacity than simple random testing.
	We outline and demonstrate two new ART algorithms, and demonstrate
	experimentally that they offer similar performance advantages, with
	considerably lower overhead than other ART algorithms.},
  doi = {http://doi.ieeecomputersociety.org/10.1109/QSIC.2004.1357947},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Adaptive Random Testing through dynamic partitioning.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.04.11},
  url = {http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=1357947},
}

@INPROCEEDINGS{Chen2003,
  author = {T. Y. Chen and F. C. Kuo and R. G. Merkel and S. P. Ng},
  title = {Mirror adaptive random testing},
  booktitle = {Proceedings of the Third International Conference on Quality Software},
  year = {2003},
  pages = {4 - 11},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society},
  doi = {10.1109/QSIC.2003.1319079},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Mirror adaptive random testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.04.11},
}

@INPROCEEDINGS{Chen2004,
  author = {T. Y. Chen and H. Leung and I. K. Mak},
  title = {Adaptive Random Testing},
  booktitle = {Advances in Computer Science - ASIAN 2004: Higher-Level Decision
	Making. 9th Asian Computing Science Conference. Proceedings},
  year = {2004},
  editor = {Michael J. Maher},
  publisher = {Springer-Verlag GmbH},
  abstract = {In this paper, we introduce an enhanced form of random testing called
	Adaptive Random Testing. Adaptive random testing seeks to distribute
	test cases more evenly within the input space. It is based on the
	intuition that for non-point types of failure patterns, an even
	spread of test cases is more likely to detect failures using fewer
	test cases than ordinary random testing. Experiments are performed
	using published programs. Results show that adaptive random testing
	does outperform ordinary random testing significantly (by up to
	as much as 50%) for the set of programs under study. These results
	are very encouraging, providing evidences that our intuition is
	likely to be useful in improving the effectiveness of random testing.},
  doi = {10.1007/b103476},
  pdf = {D:\Ilinca\Docs\Related work\Adaptive Random Testing.pdf},
  read = {yes},
  relevance = {6},
}

@INPROCEEDINGS{Chen2005,
  author = {Tsong Yueh Chen and Robert Merkel},
  title = {Quasi-random testing},
  booktitle = {ASE '05: Proceedings of the 20th IEEE/ACM international Conference
	on Automated software engineering},
  year = {2005},
  pages = {309--312},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Quasi-random sequences, also known as low-discrepancy or low-dispersion
	sequences, are sequences of points in an n-dimensional unit hypercube.
	These sequences have the property that points are spread more evenly
	throughout the cube than random point sequences, which result in
	regions where there are clusters of points and others that are sparsely
	populated. Based on the observation that program faults tend to
	lead to contiguous failure regions within a program's input domain,
	and that an even spread of random tests enhances the failure detection
	effectiveness for certain failure patterns, we examine the use of
	these sequences as a replacement for random sequences in automated
	testing.The limited number of quasi-random sequences available from
	the standard algorithms poses significant practical problems for
	use when testing real programs, and especially for evaluating its
	effectiveness. We examine the use of randomised quasi-random sequences,
	which are permuted in a nondeterministic fashion but still retain
	their low discrepancy properties, to overcome this problem, and
	show that testing using randomised quasi-random sequences is often
	significantly more effective than random testing.},
  doi = {http://doi.acm.org/10.1145/1101908.1101957},
  isbn = {1-59593-993-4},
  location = {Long Beach, CA, USA},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Quasi-random testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.05.19},
}

@ARTICLE{Chen1994,
  author = {T. Y. Chen and Y. T. Yu},
  title = {On the Relationship Between Partition and Random Testing},
  journal = {IEEE Transactions on Software Engineering},
  year = {1994},
  volume = {20},
  pages = {977--980},
  number = {12},
  abstract = {Weyuker and Jeng have investigated the conditions that affect the
	performance of partition testing and have compared analytically
	the fault-detecting ability of partition testing and random testing.
	This paper extends and generalizes some of their results. We give
	more general ways of characterizing the worst case for partition
	testing, along with a precise characterization of when this worst
	case is as good as random testing. We also find that partition testing
	is guaranteed to perform at least as well as random testing so long
	as the number of test cases selected is in proportion to the size
	of the subdomains.},
  address = {Piscataway, NJ, USA},
  doi = {http://dx.doi.org/10.1109/32.368132},
  issn = {0098-5589},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\On the relationship between partition and random testing.pdf},
  publisher = {IEEE Press},
  read = {yes},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://portal.acm.org/citation.cfm?id=203108&dl=GUIDE&coll=GUIDE#},
}

@INPROCEEDINGS{Ciupa2007,
  author = {Ilinca Ciupa and Andreas Leitner and Manuel Oriol and Bertrand Meyer},
  title = {Experimental Assessment of Random Testing for Object-Oriented Software},
  booktitle = {Proceedings of ISSTA'07: International Symposium on Software Testing
	and Analysis 2007},
  year = {2007},
  owner = {Ilinca},
  timestamp = {2007.04.26},
}

@INPROCEEDINGS{Ciupa2006,
  author = {Ilinca Ciupa and Andreas Leitner and Manuel Oriol and Bertrand Meyer},
  title = {Object distance and its application to adaptive random testing of
	object-oriented programs},
  booktitle = {RT '06: Proceedings of the 1st international workshop on Random testing},
  year = {2006},
  pages = {55--63},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/1145735.1145744},
  isbn = {1-59593-457-X},
  location = {Portland, Maine},
}

@INPROCEEDINGS{Ciupa2005,
  author = {I. Ciupa and A. Leitner.},
  title = {Automatic Testing Based on Design by Contract},
  booktitle = {Proceedings of Net.ObjectDays 2005 (6th Annual International Conference
	on Object-Oriented and Internet-based Technologies, Concepts, and
	Applications for a Networked World)},
  year = {2005},
  pages = {545-557},
  month = {September 19-22},
  pdf = {D:\Ilinca\Docs\papers\SOQUA_05\Automatic_Testing_Based_on_Design_by_Contract.pdf},
}

@INPROCEEDINGS{Cousot2000,
  author = {P. Cousot and R. Cousot},
  title = {Abstract Interpretation Based Program Testing},
  booktitle = {Proceedings of the SSGRR 2000 Computer \& eBusiness International
	Conference},
  year = {2000},
  address = {Compact disk paper 248 and electronic proceedings \url{http://www.ssgrr.it/en/ssgrr2000/proceedings.htm},
	L'Aquila, Italy},
  month = {July 31 -- August 6},
  publisher = {Scuola Superiore G{.} Reiss Romoli},
  abstract = {Every one can daily experiment that programs are bugged. Software
	bugs can have severe if not catastrophic consequences in computer-based
	safety critical applications. This impels the development of formal
	methods, whether manual, computer-assisted or automatic, for verifying
	that a program satisfies a specification. Among the automatic formal
	methods, program static analysis can be used to check for the absence
	of run-time errors. In this case the specification is provided by
	the semantics of the programming language in which the program is
	written. Abstract interpretation provides a formal theory for approximating
	this semantics, which leads to completely automated tools where
	run-time bugs can be statically and safely classified as unreachable,
	certain, impossible or potential. We discuss the extension of these
	techniques to abstract testing where specifications are provided
	by the programmers. Abstract testing is compared to program debugging
	and model-checking.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Cousot00_mycomments.doc},
  isbn = {88-85280-52-8},
  pdf = {D:\Ilinca\Docs\Related work\Abstract interpretation-based program testing.pdf},
  read = {yes},
  relevance = {5},
  special_observations = {Abstract interpretation was formalized by Patrick Cousot (with this
	paper?)},
  url = {http://www.di.ens.fr/~cousot/COUSOTpapers/SSGRRP-00-PC-RC.shtml},
}

@INPROCEEDINGS{Csallner2006,
  author = {Christoph Csallner and Yannis Smaragdakis},
  title = {DSD-Crasher: A Hybrid Analysis Tool for Bug Finding},
  booktitle = {International Symposium on Software Testing and Analysis (ISSTA)},
  year = {2006},
  pages = {245--254},
  month = jul,
}

@INPROCEEDINGS{Csallner2005,
  author = {Christoph Csallner and Yannis Smaragdakis},
  title = {CnC: Combining Static Checking and Testing},
  booktitle = {International Conference on Software Engineering (ICSE)},
  year = {2005},
  pages = {422--431},
  month = may,
}

@ARTICLE{Csallner2004,
  author = {Christoph Csallner and Yannis Smaragdakis},
  title = {JCrasher: an automatic robustness tester for Java},
  journal = {Software: Practice and Experience},
  year = {2004},
  volume = {34},
  pages = {1025--1050},
  number = {11},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1002/spe.602},
  issn = {0038-0644},
  pdf = {D:\Ilinca\Docs\Related work\JCrasher_An_automatic_robustness_tester_for_Java.pdf},
  publisher = {John Wiley \& Sons, Inc.},
}

@INPROCEEDINGS{Demers1995,
  author = {F.-N. Demers and J. Malenfant},
  title = {Reflection in logic, functional and object-oriented programming:
	a short comparative study},
  booktitle = {Proc. IJCAI'95, Workshop on Reflection and Metalevel Architectures
	and their Applications in AI},
  year = {1995},
  pages = {29-38},
  abstract = {Reflection is a wide-ranging concept that has been studied independently
	in many different areas of science in general, and computer science
	in particular. Even in the sub-area of programming languages, it
	has been applied to different paradigms, especially the logic, functional
	and objectoriented ones. Partly because of different past influences,
	but also because researchers in these communities scarcely talk
	to each others, concepts have evolved separately, sometimes to the
	point where it is...},
  citeseerurl = {http://citeseer.ist.psu.edu/106401.html},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\reflection-in-logic-functional.pdf},
  read = {yes},
  timestamp = {2006.02.20},
  url = {http://citeseer.ist.psu.edu/106401.html},
}

@ARTICLE{Duran1984,
  author = {J. Duran and S. Ntafos},
  title = {An Evaluation of Random Testing},
  journal = {IEEE Transactions on Software Engineering},
  year = {1984},
  volume = {SE-10},
  pages = {438 -- 444},
  month = {July},
  citeseerurl = {http://citeseer.ist.psu.edu/context/77236/0},
  owner = {Ilinca},
  read = {no},
  relevance = {5},
  timestamp = {2006.04.09},
}

@INPROCEEDINGS{Gall2007,
  author = {Pascale Le Gall and Nicolas Rapin and Assia Touil},
  title = {Symbolic execution techniques for refinement testing},
  booktitle = {Proceedings of the International Conference on Tests and Proofs (TAP)
	2007},
  year = {2007},
  volume = {LNCS},
  publisher = {Springer-Verlag},
  abstract = {We propose an approach to test whether an abstract specification
	
	is refined or not by a more concrete one. The specifications are
	
	input / output symbolic transition systems (IOSTS). The refinement
	relation
	
	requires that all traces of the abstract system are also traces of
	the
	
	concrete system, up to some signature inclusion. Our work takes inspiration
	
	from the conformance testing area. Symbolic execution techniques
	
	allow us to select traces of the abstract system and to submit them
	on
	
	the concrete specification. Each trace execution leads to a verdict
	Fail,
	
	Pass or Warning. The verdict Pass is provided with a formula which
	
	has to be verified by the values only manipulated at the level of
	the
	
	concrete specification in order to ensure the refinement relation.
	The
	
	verdict Warning reports that the concrete specification has not been
	
	sufficiently explored to give a reliable verdict. This is thus a partial
	verification
	
	process, related to the quality of the set of selected traces and
	
	of the exploration of the concrete specification. Our approach has
	been
	
	implemented and is demonstrated on a simple example.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\symbolic_execution_techniques_for_refinement_testing.pdf},
  read = {yes},
  relevance = {4},
  review = {They propose a method for testing if a specification refines another
	one. They target IOSTS (inout-output symbolic transition systems).},
  timestamp = {2007.02.16},
}

@INPROCEEDINGS{Godefroid2005,
  author = {Patrice Godefroid and Nils Klarlund and Koushik Sen},
  title = {DART: directed automated random testing},
  booktitle = {PLDI '05: Proceedings of the 2005 ACM SIGPLAN conference on Programming
	language design and implementation},
  year = {2005},
  pages = {213--223},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {We present a new tool, named DART, for automatically testing software
	that combines three main techniques: (1) automated extraction of
	the interface of a program with its external environment using static
	source-code parsing; (2) automatic generation of a test driver for
	this interface that performs random testing to simulate the most
	general environment the program can operate in; and (3) dynamic
	analysis of how the program behaves under random testing and automatic
	generation of new test inputs to direct systematically the execution
	along alternative program paths. Together, these three techniques
	constitute Directed Automated Random Testing, or DART for short.
	The main strength of DART is thus that testing can be performed
	completely automatically on any program that compiles -- there is
	no need to write any test driver or harness code. During testing,
	DART detects standard errors such as program crashes, assertion
	violations, and non-termination. Preliminary experiments to unit
	test several examples of C programs are very encouraging.},
  doi = {http://doi.acm.org/10.1145/1065010.1065036},
  isbn = {1-59593-056-6},
  location = {Chicago, IL, USA},
  pdf = {D:\Ilinca\Docs\Related work\dart.pdf},
  read = {no},
  relevance = {5},
}

@INPROCEEDINGS{Guo2006,
  author = {Philip J. Guo and Jeff H. Perkins and Stephen McCamant and Michael
	D. Ernst},
  title = {Dynamic inference of abstract types},
  booktitle = {ISSTA '06: Proceedings of the 2006 International Symposium on Software
	Testing and Analysis},
  year = {2006},
  pages = {255--265},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {An abstract type groups variables that are used for related purposes
	
	in a program. We describe a dynamic unification-based analysis
	
	for inferring abstract types. Initially, each run-time value gets
	a
	
	unique abstract type. A run-time interaction among values indicates
	
	that they have the same abstract type, so their abstract types
	
	are unified. Also at run time, abstract types for variables are accumulated
	
	from abstract types for values. The notion of interaction
	
	may be customized, permitting the analysis to compute finer
	
	or coarser abstract types; these different notions of abstract type
	
	are useful for different tasks. We have implemented the analysis
	
	for compiled x86 binaries and for Java bytecodes. Our experiments
	
	indicate that the inferred abstract types are useful for program comprehension,
	
	improve both the results and the run time of a follow-on
	
	program analysis, and are more precise than the output of a comparable
	
	static analysis, without suffering from overfitting.},
  doi = {http://doi.acm.org/10.1145/1146238.1146268},
  isbn = {1-59593-263-1},
  location = {Portland, Maine, USA},
  pdf = {D:\Ilinca\Docs\Related work\dynamic_inference_of_abstract_types.pdf},
}

@ARTICLE{Hamlet1990,
  author = {D. Hamlet and R. Taylor},
  title = {Partition testing does not inspire confidence},
  journal = {IEEE Transactions on Software Engineering},
  year = {1990},
  volume = {16 (12)},
  pages = {1402--1411},
  month = {December},
  abstract = {Theoretical models are used to study partition testing in the abstract
	and to describe the circumstances under which it should perform
	well at failure detection. Partition testing is shown to be more
	valuable when the partitions are narrowly based on expected failures
	and there is a good chance that failures occur. It is concluded
	that for gaining confidence from successful tests, partition testing
	as usually practiced has little value.},
  owner = {Ilinca},
  read = {no},
  relevance = {4},
  timestamp = {2006.04.09},
}

@INCOLLECTION{Hamlet1994,
  author = {R. Hamlet},
  title = {Random testing},
  booktitle = {Encyclopedia of Software Engineering},
  publisher = {Wiley},
  year = {1994},
  editor = {J. Marciniak},
  pages = {970--978},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Random Testing.pdf},
  read = {no},
  relevance = {6},
  timestamp = {2006.04.14},
  url = {citeseer.ist.psu.edu/hamlet94random.html},
}

@INPROCEEDINGS{Hartman2004,
  author = {A. Hartman and K. Nagin},
  title = {The AGEDIS tools for model based testing},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
	on Software testing and analysis},
  year = {2004},
  pages = {129--132},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {We describe the tools and interfaces created by the AGEDIS project,
	a European Commission sponsored project for the creation of a methodology
	and tools for automated model driven test generation and execution
	for distributed systems. The project includes an integrated environment
	for modeling, test generation, test execution, and other test related
	activities. The tools support a model based testing methodology
	that features a large degree of automation and also includes a feedback
	loop integrating coverage and defect analysis tools with the test
	generator and execution framework. Prototypes of the tools have
	been tried in industrial settings providing important feedback for
	the creation of the next generation of tools in this area.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Hartman04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007529},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
  pdf = {D:\Ilinca\Docs\Related work\The AGEDIS tools for model based testing.pdf},
  read = {yes},
  relevance = {4},
  url = {http://portal.acm.org/citation.cfm?doid=1007529#},
}

@ARTICLE{Jezequel2001,
  author = {Jean-Marc Jezequel and Daniel Deveaux and Yves Le Traon},
  title = {Reliable Objects: a Lightweight Approach Applied to Java},
  journal = {IEEE Software},
  year = {2001},
  volume = {18(4)},
  pages = {76--83},
  month = {July/August},
  abstract = {Small scale software developments need specific low cost and low overhead
	methods and tools to deliver quality products within tight time
	and budget constraints. This is particularly true of testing, because
	of its cost and impact on final product reliability. We propose
	a lightweight approach to embed tests into components, making them
	self testable. We also propose a method to evaluate testing efficiency,
	based on mutation techniques, which ultimately provides an estimation
	of a component's quality. This allows the software developer to
	consciously trade reliability for resources. Our methodology has
	been implemented in the Eiffel, Java, C++ and Perl languages. The
	Java implementation, built on top of iContract, is outlined here.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Jezequel01_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\reliable objects - a lightweight approach applied to Java.pdf},
  read = {yes},
  relevance = {6},
}

@INPROCEEDINGS{Khurshid2003,
  author = {Sarfraz Khurshid and Corina S. Pasareanu and Willem Visser},
  title = {Generalized Symbolic Execution for Model Checking and Testing.},
  booktitle = {Proceedings of the Ninth International Conference on Tools and Algorithms
	for the Construction and Analysis of Systems (TACAS 2003)},
  year = {2003},
  volume = {LNCS 2619},
  pages = {553-568},
  publisher = {Springer-Verlag},
  abstract = {Modern software systems, which often are concurrent and manipulate
	complex data structures must be extremely reliable. We present a
	novel framework based on symbolic execution, for automated checking
	of such systems. We provide a two-fold generalization of traditional
	symbolic execution based approaches. First, we define a source to
	source translation to instrument a program, which enables standard
	model checkers to perform symbolic execution of the program. Second,
	we give a novel symbolic execution algorithm that handles dynamically
	allocated structures (e.g., lists and trees), method preconditions
	(e.g., acyclicity), data (e.g., integers and strings) and concurrency.
	The program instrumentation enables a model checker to automatically
	explore different program heap configurations and manipulate logical
	formulae on program data (using a decision procedure). We illustrate
	two applications of our framework: checking correctness of multi-threaded
	programs that take inputs from unbounded domains with complex structure
	and generation of non-isomorphic test inputs that satisfy a testing
	criterion. Our implementation for Java uses the Java PathFinder
	model checker.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Khurshid03_mycomments.doc},
  ee = {http://link.springer.de/link/service/series/0558/bibs/2619/26190553.htm},
  pdf = {D:\Ilinca\Docs\Related work\Generalized symbolic execution for model checking and testing.pdf},
  read = {yes},
  relevance = {5},
}

@MISC{Leavens2005,
  author = {Gary T. Leavens and Yoonsik Cheon},
  title = {Design by Contract with JML},
  howpublished = {on web page},
  month = {January},
  year = {2005},
  note = {documentation on JML, not a conference paper},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Leavens05_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\jmldbc.pdf},
  read = {yes},
  relevance = {5},
  url = {www.cs.caltech.edu/~cs141/resources/JML/docs/jmltutorial/jmldbc.pdf},
}

@MISC{Leitner2005-2007,
  author = {Andreas Leitner and Ilinca Ciupa},
  title = {AutoTest},
  howpublished = {http://se.inf.ethz.ch/people/leitner/auto\_test/},
  year = {2005 - 2007},
  owner = {Ilinca},
  timestamp = {2006.04.14},
  url = {http://se.inf.ethz.ch/people/leitner/auto\_test/},
}

@INPROCEEDINGS{Leitner2007,
  author = {Andreas Leitner and Ilinca Ciupa and Bertrand Meyer and Mark Howard},
  title = {Reconciling Manual and Automated Testing: the AutoTest Experience},
  booktitle = {Proceedings of the 40th Hawaii International Conference on System
	Sciences - 2007, Software Technology},
  year = {2007},
  month = {January 3-6},
}

@ARTICLE{Levenshtein1965,
  author = {Vladimir I. Levenshtein},
  title = {Binary codes capable of correcting deletions, insertions, and reversals},
  journal = {Doklady Akademii Nauk SSSR},
  year = {1965},
  volume = {163},
  pages = {845-848},
  number = {4},
  owner = {Ilinca},
  timestamp = {2006.04.20},
}

@INPROCEEDINGS{Liu2007,
  author = {Lisa (Ling) Liu and Bertrand Meyer and Bernd Schoeller},
  title = {Using Contracts and Boolean Queries to Improve the Quality of Automatic
	Test Generation},
  booktitle = {Proceedings of the International Conference on Tests and Proofs (TAP)
	2007},
  year = {2007},
  volume = {LNCS},
  publisher = {Springer-Verlag},
  abstract = {Since test cases cannot be exhaustive, any effective test case generation
	strategy must identify the execution states most likely to uncover
	bugs. The key issue is to define criteria for selecting such interesting
	states.
	
	
	If the units being tested are classes in object-oriented programming,
	it seems attractive to rely on the boolean queries present in each
	class, which indeed define criteria on the states of the corresponding
	objects, and  in contract-equipped O-O software  figure prominently
	in preconditions, postconditions and invariants. As these queries
	are part of the class specification and hence relevant to its clients,
	one may conjecture that the resulting partition of the state space
	is also relevant for tests.
	
	
	We explore this conjecture by examining whether relying on the boolean
	queries of a class to extract abstract states improves the results
	of black-box testing. The approach uses proof techniques to generate
	objects that satisfy the class invariants, then performs testing
	by relying on postconditions as test oracles.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\using_contracts_and_boolean_queries_to_improve_the_quality_of_automatic_test_generation.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2007.02.16},
}

@ARTICLE{Mankefors2003,
  author = {S. Mankefors and R. Torkar and A. Boklund},
  title = {New Quality Estimations in Random Testing},
  journal = {Proceedings of the 14th International Symposium on Software Reliability
	Engineering (ISSRE 2003)},
  year = {2003},
  volume = {00},
  pages = {468-478},
  abstract = {By reformulating the issue of random testing into an equivalent problem
	we are able to introduce a new kind of quality estimations based
	on Monte Carlo integration and the central limit theorem. This method
	also provides a limited but working "success theory" in the case
	of no detected failures. In an empirical evaluation using hundreds
	of billions of simulated tests we furthermore find a very good match
	between the quality estimations presented in this article and the
	true failure frequencies. Both simple modulus defects as well as
	seeded defects in two extensively employed numerical routines were
	subject to investigation in the empirical work.},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ISSRE.2003.1251067},
  issn = {1071-9458},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\New quality estimations in random testing.pdf},
  publisher = {IEEE Computer Society},
  read = {only beginning},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=1251067},
}

@INPROCEEDINGS{Mayer2005,
  author = {Johannes Mayer},
  title = {Lattice-Based Adaptive Random Testing},
  booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated
	Software Engineering (ASE 2005)},
  year = {2005},
  series = {ACM},
  pages = {333-336},
  publisher = {ACM Press, New York, NY, USA},
  abstract = {Adaptive Random Testing (ART) denotes a family of testing algorithms
	that have a better performance compared to pure random testing with
	respect to the number of test cases necessary to detect the first
	failure. Many of these algorithms, however, are not very efficient
	regarding runtime. A new ART algorithm is presented that has a better
	performance than all other ART methods for the block failure pattern.
	Its runtime is linear in the number of test cases selected, which
	is nearly as efficient as pure random testing, as opposed to most
	other ART methods. This new ART algorithm selects the test cases
	based on a lattice.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Lattice-based adaptive random testing.pdf},
  read = {yes (not all)},
  relevance = {5},
  timestamp = {2006.04.14},
}

@INPROCEEDINGS{Mayer2005a,
  author = {Johannes Mayer},
  title = {Adaptive Random Testing by Bisection with Restriction},
  booktitle = {Proceedings of the Seventh International Conference on Formal Engineering
	Methods (ICFEM 2005)},
  year = {2005},
  series = {LNCS 3785},
  pages = {251-263},
  publisher = {Springer-Verlag, Berlin},
  abstract = {Random Testing is a strategy to select test cases based on pure randomness.
	Adaptive Random Testing (ART), a family of algorithms, improves
	pure Random Testing by taking common failure pattern into account.
	The bestin terms of the number of test cases necessary to detect
	the first failureART algorithms, however, are too runtime inefficient.
	Therefore, a modification of a fast, but not so good ART algorithm,
	namely ART by Bisection, is presented. This modification requires
	much less test cases than the original method while retaining its
	computational efficiency.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Adaptive Random Testing by Bisection with Restriction.pdf},
  read = {no},
  relevance = {5},
  timestamp = {2006.04.14},
}

@INPROCEEDINGS{Meinke2004,
  author = {Karl Meinke},
  title = {Automated black-box testing of functional correctness using function
	approximation},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
	on Software testing and analysis},
  year = {2004},
  pages = {143--153},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {We consider black-box testing of functional correctness as a special
	case of a satisfiability or constraint solving problem. We introduce
	a general method for solving this problem based on function approximation.
	We then describe some practical results obtained for an automated
	testing algorithm using approximation by piecewise polynomial functions.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Meinke04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007532},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
  pdf = {D:\Ilinca\Docs\Related work\Automated blckbox testing of functional correctness using function approximation.pdf},
  read = {yes},
  relevance = {4},
  url = {http://portal.acm.org/citation.cfm?id=1007532},
}

@ARTICLE{Menzies2000,
  author = {Tim Menzies and Bojan Cukic},
  title = {When to Test Less},
  journal = {IEEE Software},
  year = {2000},
  volume = {17},
  pages = {107-112},
  number = {5},
  month = {September - October},
  abstract = {Small-scale software projects usually cant afford to implement time-consuming
	and expensive tests. However, the authors show that in a surprisingly
	large number of cases, a small number of randomly selected tests
	will adequately probe the software.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\When to test less.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2006.04.10},
}

@BOOK{Meyer1997,
  title = {Object-Oriented Software Construction, 2nd edition},
  publisher = {Prentice Hall},
  year = {1997},
  author = {Bertrand Meyer},
  owner = {Ilinca},
  timestamp = {2007.01.23},
}

@INPROCEEDINGS{Meyer2007,
  author = {Bertrand Meyer and Ilinca Ciupa and Andreas Leitner and Lisa (Ling)
	Liu},
  title = {Automatic Testing of Object-Oriented Software},
  booktitle = {Proceedings of SOFSEM 2007 (Current Trends in Theory and Practice
	of Computer Science)},
  year = {2007},
  editor = {Jan van Leeuwen},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer-Verlag},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\papers\sofsem07\sofsem07.pdf},
  timestamp = {2007.02.08},
}

@BOOK{Myers1979,
  title = {The Art of Software Testing},
  publisher = {John Wiley \& Sons},
  year = {1979},
  author = {Glenford J. Myers},
  owner = {Ilinca},
  read = {no},
  relevance = {5},
  timestamp = {2006.04.09},
}

@INPROCEEDINGS{Offutt1996,
  author = {A. Jefferson Offutt and J. Huffman Hayes},
  title = {A semantic model of program faults},
  booktitle = {ISSTA '96: Proceedings of the 1996 ACM SIGSOFT international symposium
	on Software testing and analysis},
  year = {1996},
  pages = {195--200},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Program faults are artifacts that are widely studied, but there are
	many aspects of faults that we still do not understand. In addition
	to the simple fact that one important goal during testing is to
	cause failures and thereby detect faults, a full understanding of
	the characteristics of faults is crucial to several research areas
	in testing. These include fault-based testing, testability, mutation
	testing, and the comparative evaluation of testing strategies. In
	this workshop paper, we explore the fundamental nature of faults
	by looking at the differences between a syntactic and semantic characterization
	of faults. We offer definitions of these characteristics and explore
	the differentiation. Specifically, we discuss the concept of "size"
	of program faults --- the measurement of size provides interesting
	and useful distinctions between the syntactic and semantic characterization
	of faults. We use the fault size observations to make several predictions
	about testing and present preliminary data that supports this model.
	We also use the model to offer explanations about several questions
	that have intrigued testing researchers.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Offutt96_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/229000.226317},
  isbn = {0-89791-787-1},
  location = {San Diego, California, United States},
  pdf = {D:\Ilinca\Docs\Related work\A semantic model of program faults.pdf},
  read = {yes},
  relevance = {5},
  url = {http://portal.acm.org/citation.cfm?id=226317#},
}

@ARTICLE{Offutt1999,
  author = {A. Jefferson Offutt and Zhenyi Jin and Jie Pan},
  title = {The dynamic domain reduction procedure for test data generation},
  journal = {Softw. Pract. Exper.},
  year = {1999},
  volume = {29},
  pages = {167--193},
  number = {2},
  abstract = {(Summary) Test data generation is one of the most technically challenging
	steps of testing software, but most commercial systems currently
	incorporate very little automation for this step. This paper presents
	results from a project that is trying to find ways to incorporate
	test data generation into practical test processes. The results
	include a new procedure for automatically generating test data that
	incorporates ideas from symbolic evaluation, constraint-based testing,
	and dynamic test data generation. It takes an initial set of values
	for each input, and dynamically pushes the values through the
	control-flow graph of the program, modifying the sets of values
	as branches in the program are taken. The result is usually a set
	of values for each input parameter that has the property that any
	choice from the sets will cause the path to be traversed. This procedure
	uses new analysis techniques, offers improvements over previous
	research results in constraint based testing, and combines several
	steps into one coherent process. The dynamic nature of this procedure
	yields several benefits. Moving through the control flow graph dynamically
	allows path constraints to be resolved immediately, which is more
	efficient both in space and time, and more often successful than
	constraint-based testing. This new procedure also incorporates an
	intelligent search technique based on bisection. The dynamic nature
	of this procedure also allows certain improvements to be made in
	the handling of arrays, loops, and expressions; language features
	that are traditionally difficult to handle in test data generation
	systems. The paper presents the test data generation procedure,
	examples to explain the working of the procedure, and results from
	a proof-of-concept implementation. Copyright 1999 John Wiley & Sons,
	Ltd.},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1002/(SICI)1097-024X(199902)29:2<167::AID-SPE225>3.3.CO;2-M},
  issn = {0038-0644},
  pdf = {D:\Ilinca\Docs\Related work\The dynamic domain reduction procedure for test data generation.pdf},
  publisher = {John Wiley \& Sons, Inc.},
  read = {no},
  relevance = {4},
  url = {http://portal.acm.org/citation.cfm?id=309101#},
}

@TECHREPORT{Oriat2004,
  author = {Catherine Oriat},
  title = {Jartege: a Tool for Random Generation of Unit Tests for Java Classes},
  institution = {Centre National de la Recherche Scientifique, Institut National Polytechnique
	de Grenoble, Universite Joseph Fourier Grenoble I},
  year = {2004},
  number = {RR-1069-I},
  month = {June},
  abstract = {This report presents Jartege, a tool which allows random generation
	of unit tests for Java classes specified in JML. JML (Java Modeling
	Language) is a specification language for Java which allows one
	to write invariants for classes, and pre- and postconditions for
	operations. As in the JML-JUnit tool, we use JML specifications
	on the one hand to eliminate irrelevant test cases, and on the other
	hand as a test oracle. Jartege randomly generates test cases, which
	consist of a sequence of constructor and method calls for the classes
	under test. The random aspect of the tool can be parameterized by
	associating weights to classes and operations, and by controlling
	the number of instances which are created for each class under test.
	The practical use of Jartege is illustrated by a small case study.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Oriat04_mycomments.doc},
  keywords = {Testing, unit testing, random generation of test cases, Java, JML},
  pdf = {D:\Ilinca\Docs\Related work\Jartege_A_tool_for_random_generation_of_unit_tests_for_Java_classes.pdf},
  read = {yes},
  relevance = {6},
  url = {http://arxiv.org/abs/cs.PL/0412012},
}

@ARTICLE{Ostrand1988,
  author = {T. J. Ostrand and M. J. Balcer},
  title = {The category-partition method for specifying and generating fuctional
	tests},
  journal = {Commun. ACM},
  year = {1988},
  volume = {31},
  pages = {676--686},
  number = {6},
  abstract = {A method for creating functional test suites has been developed in
	which a test engineer analyzes the system specification, writes
	a series of formal test specifications, and then uses a generator
	tool to produce test descriptions from which test scripts are written.
	The advantages of this method are that the tester can easily modify
	the test specification when necessary, and can control the complexity
	and number of the tests by annotating the tests specification with
	constraints.},
  address = {New York, NY, USA},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Ostrand88_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/62959.62964},
  issn = {0001-0782},
  pdf = {D:\Ilinca\Docs\Related work\The category-partition method.pdf},
  publisher = {ACM Press},
  read = {yes},
  relevance = {5},
  url = {http://portal.acm.org/citation.cfm?id=62964},
}

@INPROCEEDINGS{Pacheco2005,
  author = {Carlos Pacheco and Michael D. Ernst},
  title = {Eclat: Automatic generation and classification of test inputs},
  booktitle = {ECOOP 2005 --- Object-Oriented Programming, 19th European Conference},
  year = {2005},
  address = {Glasgow, Scotland},
  month = {July~25--29,},
  abstract = {This paper describes a technique that selects, from a large set of
	test inputs, a small subset likely to reveal faults in the software
	under test. The technique takes a program or software component,
	plus a set of correct executions  say, from observations of the
	software running properly, or from an existing test suite that a
	user wishes to enhance. The technique first infers an operational
	model of the software's operation. Then, inputs whose operational
	pattern of execution differs from the model in specific ways are
	suggestive of faults. These inputs are further reduced by selecting
	only one input per operational pattern. The result is a small portion
	of the original inputs, deemed by the technique as most likely to
	reveal faults. Thus, the technique can also be seen as an error-detection
	technique. The paper describes two additional techniques that complement
	test input selection. One is a technique for automatically producing
	an oracle (a set of assertions) for a test input from the operational
	model, thus transforming the test input into a test case. The other
	is a classification-guided test input generation technique that
	also makes use of operational models and patterns. When generating
	inputs, it filters out code sequences that are unlikely to contribute
	to legal inputs, improving the efficiency of its search for fault-revealing
	inputs. We have implemented these techniques in the Eclat tool,
	which generates unit tests for Java classes. Eclat's input is a
	set of classes to test and an example program execution  say, a
	passing test suite. Eclat's output is a set of JUnit test cases,
	each containing a potentially fault-revealing input and a set of
	assertions at least one of which fails. In our experiments, Eclat
	successfully generated inputs that exposed fault-revealing behavior;
	we have used Eclat to reveal real errors in programs. The inputs
	it selects as fault-revealing are an order of magnitude as likely
	to reveal a fault as all generated inputs.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Pacheco05_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\Eclat Automatic Generation and Classification of Test Inputs.pdf},
  read = {yes},
  relevance = {4},
  url = {http://pag.csail.mit.edu/pubs/classify-tests-ecoop2005-abstract.html},
}

@INPROCEEDINGS{Pacheco2007,
  author = {C. Pacheco and S. K. Lahiri and M. D. Ernst and T. Ball},
  title = {Feedback-directed random test generation},
  booktitle = {ICSE '07: Proceedings of the 29th International Conference on Software
	Engineering},
  year = {2007},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Feedback-directed random test generation.pdf},
  relevance = {6},
  timestamp = {2007.09.12},
  url = {http://people.csail.mit.edu/cpacheco/},
}

@INPROCEEDINGS{Richardson1989,
  author = {D. Richardson and O. O'Malley and C. Tittle},
  title = {Approaches to specification-based testing},
  booktitle = {TAV3: Proceedings of the ACM SIGSOFT '89 third symposium on Software
	testing, analysis, and verification},
  year = {1989},
  pages = {86--96},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Current software testing practices focus, almost exclusively, on the
	implementation, despite widely acknowledged benefits of testing
	based on software specifications. We propose approaches to specification-based
	testing by extending a wide variety of implementation-based testing
	techniques to be applicable to formal specification languages. We
	demonstrate these approaches for the Anna and Larch specification
	languages.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Richardson89_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/75308.75319},
  isbn = {0-89791-342-6},
  location = {Key West, Florida, United States},
  pdf = {D:\Ilinca\Docs\Related work\Approaches to specification-based testing.pdf},
  read = {yes},
  relevance = {5},
  special_observations = {this is the original paper formalizing specification-based testing},
  url = {http://portal.acm.org/citation.cfm?id=75319},
}

@ARTICLE{Rosenblum1995,
  author = {David S. Rosenblum},
  title = {A Practical Approach to Programming With Assertions},
  journal = {IEEE Trans. Softw. Eng.},
  year = {1995},
  volume = {21},
  pages = {19--31},
  number = {1},
  abstract = {Embedded assertions have been recognized as a potentially powerful
	tool for automatic runtime detection of software faults during debugging,
	testing, maintenance and even production versions of software systems.
	Yet despite the richness of the notations and the maturity of the
	techniques and tools that have been developed for programming with
	assertions, assertions are a development tool that has seen little
	widespread use in practice. The main reasons seem to be that (1)
	previous assertion processing tools did not integrate easily with
	existing programming environments, and (2) it is not well understood
	what kinds of assertions are most effective at detecting software
	faults. This paper describes experience using an assertion processing
	tool that was built to address the concerns of ease-of-use and effectiveness.
	The tool is called APP, an Annotation PreProcessor for C programs
	developed in UNIX-based development environments. APP has been used
	in the development of a variety of software systems over the past
	five years. Based on this experience, the paper presents a classification
	of the assertions that were most effective at detecting faults.
	While the assertions that are described guard against many common
	kinds of faults and errors, the very commonness of such faults demonstrates
	the need for an explicit, high-level, automatically checkable specification
	of required behavior. It is hoped that the classification presented
	in this paper will prove to be a useful first step in developing
	a method of programming with assertions.},
  address = {Piscataway, NJ, USA},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Rosenblum95_mycomments.doc},
  doi = {http://dx.doi.org/10.1109/32.341844},
  issn = {0098-5589},
  pdf = {D:\Ilinca\Docs\Related work\A practical approach to programming with assertions.pdf},
  publisher = {IEEE Press},
  read = {yes},
  relevance = {5},
  special_observations = {This paper received the ICSE most influential paper award in 2002.},
  url = {http://portal.acm.org/citation.cfm?id=203111&coll=GUIDE&dl=GUIDE&CFID=46749401&CFTOKEN=25541718},
}

@INPROCEEDINGS{Satpathy2007,
  author = {Manoranjan Satpathy and Michael Butler and Michael Leuschel and S.
	Ramesh},
  title = {Automatic Testing from Formal Specifications},
  booktitle = {Proceedings of the International Conference on Tests and Proofs (TAP)
	2007},
  year = {2007},
  volume = {LNCS},
  publisher = {Springer-Verlag},
  abstract = {In this article, we consider model oriented formal specifica-
	
	tion languages. We generate test cases by performing symbolic execution
	
	over a model, and from the test cases obtain a Java program. This
	Java
	
	program acts as a test driver and when it is run in conjunction with
	the
	
	implementation then testing is performed in an automatic manner. Our
	
	approach makes the testing cycle fully automatic. The main contribution
	
	of our work is that we perform automatic testing even when the models
	
	are non-deterministic.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\automatic_testing_from_formal_specifications.pdf},
  read = {yes},
  relevance = {4},
  timestamp = {2007.02.16},
}

@INPROCEEDINGS{Shukla2004,
  author = {R. Shukla and P. Strooper and D. Carrington},
  title = {A Framework for Reliability Assessment of Software Components},
  booktitle = {Component-Based Software Engineering: 7th International Symposium,
	CBSE 2004, Edinburgh, UK, May 24-25, 2004. Proceedings},
  year = {2004},
  editor = {Ivica Crnkovic, Judith A. Stafford, Heinz W. Schmidt, et al.},
  volume = {3054 / 2004},
  pages = {272--279},
  publisher = {Springer-Verlag GmbH},
  abstract = {This paper proposes a conceptual framework for the reliability assessment
	of software components that incorporates test case execution and
	output evaluation. Determining an operational profile and test output
	evaluation are two difficult and important problems that must be
	addressed in such a framework. Determining an operational profile
	is difficult, because it requires anticipating the future use of
	the component. An expected result is needed for each test case to
	evaluate the test result and a test oracle is used to generate these
	expected results. The framework combines statistical testing and
	test oracles implemented as self-checking versions of the implementations.
	The framework is illustrated using two examples that were chosen
	to identify the issues that must be addressed to provide tool support
	for the framework.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Shukla04_mycomments.doc},
  doi = {10.1007/b97813},
  pdf = {D:\Ilinca\Docs\Related work\A Framework for Reliability Assessment of Software Components.pdf},
  read = {yes},
  relevance = {4},
  url = {http://springerlink.metapress.com/app/home/contribution.asp?wasp=ff1c3b88abb7441aa86174e572d4c35e&referrer=parent&backto=searcharticlesresults,1,1;},
}

@ARTICLE{Tillmann2006,
  author = {Nikolai Tillmann and Wolfram Schulte},
  title = {Unit Tests Reloaded: Parameterized Unit Testing with Symbolic Execution},
  journal = {IEEE Software},
  year = {2006},
  volume = {23},
  pages = {38--47},
  number = {4},
  abstract = {Unit tests are becoming popular. Are there ways to automate the generation
	of good unit tests? Parameterized unit tests are unit tests that
	depend on inputs. PUTs describe behavior more concisely than traditional
	unit tests. We use symbolic execution techniques and constraint
	solving to find inputs for PUTs that achieve high code coverage,
	to turn existing unit tests into PUTs, and to generate entirely
	new PUTs that describe an existing implementation's behavior. Traditional
	testing benefits from these techniques because test inputs - including
	the behavior of entire classes - can often be generated automatically
	from compact PUTs.},
  address = {Los Alamitos, CA, USA},
  doi = {http://dx.doi.org/10.1109/MS.2006.117},
  issn = {0740-7459},
  pdf = {D:\Ilinca\Docs\Related work\not read yet\Unit_tests_reloaded_parameterized_unit_testing_with_symbolic_execution.pdf},
  publisher = {IEEE Computer Society Press},
  read = {no},
  relevance = {6},
  url = {http://portal.acm.org/citation.cfm?id=1159169.1159389&coll=&dl=acm&CFID=15151515&CFTOKEN=6184618#},
}

@INPROCEEDINGS{Tonella2004,
  author = {Paolo Tonella},
  title = {Evolutionary testing of classes},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
	on Software testing and analysis},
  year = {2004},
  pages = {119--128},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Object oriented programming promotes reuse of classes in multiple
	contexts. Thus, a class is designed and implemented with several
	usage scenarios in mind, some of which possibly open and generic.
	Correspondingly, the unit testing of classes cannot make too strict
	assumptions on the actual method invocation sequences, since these
	vary from application to application. In this paper, a genetic algorithm
	is exploited to automatically produce test cases for the unit testing
	of classes in a generic usage scenario. Test cases are described
	by chromosomes, which include information on which objects to create,
	which methods to invoke and which values to use as inputs. The proposed
	algorithm mutates them with the aim of maximizing a given coverage
	measure. The implementation of the algorithm and its application
	to classes from the Java standard library are described.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Tonella04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007528},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
  pdf = {D:\Ilinca\Docs\Related work\Evolutionary Testing of Classes.pdf},
  read = {yes},
  relevance = {6},
  url = {http://portal.acm.org/citation.cfm?id=1007528},
}

@TECHREPORT{Venolia2005,
  author = {Gina D. Venolia and Robert DeLine and Thomas LaToza},
  title = {Software Development at Microsoft Observed},
  institution = {Microsoft Research},
  year = {2005},
  number = {MSR-TR-2005-140},
  month = {October},
  abstract = {To understand Microsoft developers typical tools and work habits
	and their level of satisfaction with these, we performed two surveys
	and eleven interviews with developers across all business divisions.
	This report provides a summary of the resulting data. From the set
	of potential problems we gave them, the top three that Microsoft
	developers agree they have are: understanding the rationale behind
	a piece of code (66%); having to switch tasks often because of requests
	from teammates or managers (62%); and being aware of changes to
	code elsewhere that impact their own code (61%). The most notable
	take-away from the data is that developers go to great lengths to
	create and maintain rich mental models of code and dont rely on
	external representations. The mental nature of these models requires
	frequent, disruptive, face-to-face meetings to keep individuals
	models in sync, which greatly slow the rate at which a newcomer
	to a team can become productive. These interruptions also burden
	more senior development team members, as they have to recover what
	they were doing in the code following the interruption from team
	members.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Software_development_at_microsoft_observed.pdf},
  read = {no},
  relevance = {5},
  timestamp = {2006.11.10},
  url = {http://research.microsoft.com/research/pubs/view.aspx?type=Technical%20Report&id=994},
}

@INPROCEEDINGS{Visser2004,
  author = {Willem Visser and Corina S. P\&\#462;s\&\#462;reanu and Sarfraz Khurshid},
  title = {Test input generation with java PathFinder},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
	on Software testing and analysis},
  year = {2004},
  pages = {97--107},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {We show how model checking and symbolic execution can be used to generate
	test inputs to achieve structural coverage of code that manipulates
	complex data structures. We focus on obtaining branch-coverage during
	unit testing of some of the core methods of the red-black tree implementation
	in the Java TreeMap library, using the Java PathFinder model checker.
	Three different test generation techniques will be introduced and
	compared, namely, straight model checking of the code, model checking
	used in a black-box fashion to generate all inputs up to a fixed
	size, and lastly, model checking used during white-box test input
	generation. The main contribution of this work is to show how efficient
	white-box test input generation can be done for code manipulating
	complex data, taking into account complex method preconditions.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Visser04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007526},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
  pdf = {D:\Ilinca\Docs\Related work\Test input generation with Java PathFinder.pdf},
  read = {yes},
  relevance = {6},
  url = {http://portal.acm.org/citation.cfm?id=1007526},
}

@INPROCEEDINGS{Werner2004,
  author = {Edith Werner and Helmut Neukirchen and Jens Grabowski},
  title = {{Self-adaptive Functional Testing of Services Working in Continuously
	Changing Contexts}},
  booktitle = {Proceedings of the 3rd Workshop on System Testing and Validation
	(SV04), December 2004, Fraunhofer Book Series},
  year = {2004},
  month = dec,
  abstract = {Web Services and ad-hoc networks are examples for systems that work
	in continuously changing contexts. The services provided by such
	systems can be tested in a laboratory environment using the standard
	conformance testing methodology. Testing the inter-working of services
	with other services in all contexts is an impossible task, because
	the possible usages of services may not be known or may increase
	due to the development of new services. Thus, errors may occur and
	have to be detected and handled at runtime. Our approach to this
	problem is self-adaptive functional testing, which is a combination
	of monitoring and active testing at runtime. In order to locate
	an error which has been detected during monitoring, existing test
	cases are adapted automatically to the specific situation and applied
	afterwards at runtime.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Werner04_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\Self_Adaptive_Functional_Testing_of_Services.pdf},
  read = {yes},
  relevance = {2},
  special_observations = {Met Edith Werner and Helmut Neukirchen in TAROT summer school in Paris
	(July 2005).},
}

@ARTICLE{Weyuker1991,
  author = {E.J. Weyuker and B. Jeng},
  title = {Analyzing Partition Testing Strategies},
  journal = {IEEE Transactions on Software Engineering},
  year = {1991},
  volume = {17},
  pages = {703-711},
  number = {7},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/32.83906},
  issn = {0098-5589},
  publisher = {IEEE Computer Society},
}

@INPROCEEDINGS{Xie2005,
  author = {Tao Xie and Darko Marinov and Wolfram Schulte and David Notkin},
  title = {Symstra: A Framework for Generating Object-Oriented Unit Tests using
	Symbolic Execution},
  booktitle = {Proceedings of the 11th International Conference on Tools and Algorithms
	for the Construction and Analysis of Systems (TACAS 05)},
  year = {2005},
  pages = {365--381},
  month = {April},
  abstract = {Object-oriented unit tests consist of sequences of method invocations.
	Behavior of an invocation depends on the methods arguments and
	the state of the receiver at the beginning of the invocation. Correspondingly,
	generating unit tests involves two tasks: generating method sequences
	that build relevant receiver object states and generating relevant
	method arguments. This paper proposes Symstra, a framework that
	achieves both test generation tasks using symbolic execution of
	method sequences with symbolic arguments. The paper defines symbolic
	states of object-oriented programs and novel comparisons of states.
	Given a set of methods from the class under test and a bound on
	the length of sequences, Symstra systematically explores the object-state
	space of the class and prunes this exploration based on the state
	comparisons. Experimental results show that Symstra generates unit
	tests that achieve higher branch coverage faster than the existing
	test-generation techniques based on concrete method arguments.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Xie05_mycomments.doc},
  location = {Edinburgh, UK},
  pdf = {D:\Ilinca\Docs\Related work\Symstra.pdf},
  read = {yes},
  relevance = {6},
  url = {http://www.cs.washington.edu/homes/taoxie/publications/tacas05.pdf},
}

@INPROCEEDINGS{Xie2005a,
  author = {Tao Xie and David Notkin},
  title = {Automatically Identifying Special and Common Unit Tests for Object-Oriented
	Programs},
  booktitle = {16th IEEE International Symposium on Software Reliability Engineering},
  year = {2005},
  pages = {277--287},
  month = nov,
  abstract = {Developers often create common tests and special tests, which exercise
	common behaviors and special behaviors of the class under test,
	respectively. Although manually created tests are valuable, developers
	often overlook some special or even common tests. We have developed
	a new approach for automatically identifying special and common
	unit tests for a class without requiring any specification. Given
	a class, we automatically generate test inputs and identify common
	and special tests among the generated tests. Developers can inspect
	these identified tests and use them to augment existing tests. Our
	approach is based on statistical algebraic abstractions, program
	properties (in the form of algebraic specifications) dynamically
	inferred based on a set of predefined abstraction templates. We
	use statistical algebraic abstractions to characterize program behaviors
	and identify special and common tests. Our initial experience has
	shown that a relatively small number of common and special tests
	can be identified among a large number of generated tests and these
	identified tests expose common and special behaviors that deserve
	developers attention.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Automatically identifying special and common unit tests for oo programs.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://www-static.cc.gatech.edu/~csallnch/jcrasher/uses/xie05automatically-abstract.html},
}

@INPROCEEDINGS{Xu2004,
  author = {Guoqing Xu and Zongyuang Yang},
  title = {JMLAutoTest: A Novel Automated Testing Framework Based on JML and
	JUnit},
  booktitle = {Formal Approaches to Software Testing: Third International Workshop,
	FATES 2003, Montreal, Quebec, Canada, October 6th, 2003. Revised
	Papers},
  year = {2004},
  editor = {Alexandre Petrenko, Andreas Ulrich},
  volume = {2931 / 2004},
  pages = {70--85},
  publisher = {Springer-Verlag GmbH},
  abstract = {Writing specifications using Java Modeling Language has been accepted
	for a long time as a practical approach to increasing the correctness
	and quality of Java programs. However, the current JML testing system
	(the JML and JUnit framework) can only generate skeletons of test
	fixture and test case class. Writing codes for generating test cases,
	especially those with a complicated data structure is still a labor-intensive
	job in the test for programs annotated with JML specifications.
	This paper presents JMLAutoTest, a novel framework for automated
	testing of Java programs annotated with JML specifications. Firstly,
	given a method, three test classes (a skeleton of test client class,
	a JUnit test class and a test case class) can be generated. Secondly,
	JMLAutoTest can generate all nonisomorphic test cases that satisfy
	the requirements defined in the test client class. Thirdly, JMLAutoTest
	can avoid most meaningless cases by running the test in a double-phase
	way which saves much time of exploring meaningless cases in the
	test. This method can be adopted in the testing not only for Java
	programs, but also for programs written with other languages. Finally,
	JMLAutoTest executes the method and uses JML runtime assertion checker
	to decide whether its post-condition is violated. That is whether
	the method works correctly.},
  doi = {10.1007/b95400},
  pdf = {D:\Ilinca\Docs\Related work\JMLAutoTest.pdf},
  read = {yes},
  relevance = {4},
  url = {http://springerlink.metapress.com/app/home/contribution.asp?wasp=f3901fa4fa5e4a22a1bbd3b089ae8436&referrer=parent&backto=searcharticlesresults,1,1;},
}

@INPROCEEDINGS{Zhang1995,
  author = {Kaizhong Zhang and Jason Tsong-Li Wang and Dennis Shasha},
  title = {On the Editing Distance between Undirected Acyclic Graphs and Related
	Problems},
  booktitle = {Proceedings of the 6th Annual Symposium on Combinatorial Pattern
	Matching},
  year = {1995},
  pages = {395-407},
  publisher = {Springer Verlag, Berlin},
  owner = {Ilinca},
  timestamp = {2007.04.26},
}

@MISC{AsmLTest,
  title = {Parameter generation in the AsmL Test Generator tool},
  howpublished = {on web page},
  comment = {- They use ADF (Access Driven Filtering) to generate parameters. -
	The user has to write a IsInterestingParameter () predicate (it
	should also be the invariant, I think) - User has to do a lot of
	configuration},
  read = {yes},
  relevance = {5},
}

@MISC{EiffelBase,
  title = {The EiffelBase Library. Eiffel Software Inc. http://www.eiffel.com/},
  owner = {Ilinca},
  timestamp = {2007.01.24},
  url = {www.eiffel.com},
}

@MISC{Gobo,
  title = {The Gobo Eiffel Library. http://www.gobosoft.com/},
  howpublished = {web site},
  owner = {Ilinca},
  timestamp = {2007.01.24},
  url = {www.gobosoft.com},
}

@MISC{Jtest,
  title = {Jtest. Parasoft Corporation. http://www.parasoft.com/},
  owner = {Ilinca},
  timestamp = {2007.01.22},
  url = {http://www.parasoft.com/jsp/products/home.jsp?product=Jtest},
}

