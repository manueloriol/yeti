@inproceedings{ma:interclassmut:02,
author={{Y.-S.} Ma and {Y.-R.} Kwon and J. Offutt},
title={{Inter-Class Mutation Operators for Java}},
booktitle={Proc. ISSRE},
pages={352-366},
year={2002}
}


@INPROCEEDINGS{Bousquet2004,
  author = {Lydie du Bousquet and Yves Ledru and Oliver Maury and Catherine Oriat
    and Jean-Louis Lanet},
  title = {Case Study in JML-Based Software Validation.},
  booktitle = {ASE},
  year = {2004},
  pages = {294-297},
  abstract = {This paper reports on a testing case study applied to a small Java
    application, partially specified in JML. It illustrates that JML
    can easily be integrated with classical testing tools based on combinatorial
    techniques and random generation. It also reveals difficulties to
    reuse, in a testing context, JML annotations written for a proof
    process.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/duBousquet04_mycomments.doc},
  crossref = {DBLP:conf/kbse/2004},
  ee = {http://csdl.computer.org/comp/proceedings/ase/2004/2131/00/21310294abs.htm},
  pdf = {D:\Ilinca\Docs\Related work\Case Study in JML-Based Software Validation.pdf},
  read = {Yes},
  relevance = {5},
  special_observations = {Met Yves Ledru at TAROT summer school 2005 in Paris},
  url = {http://www-lsr.imag.fr/Les.Personnes/Yves.Ledru/PublicationsYL.html},
}

@ARTICLE{Goodenough1975,
  author = {John B. Goodenough and Susan L. Gerhart},
  title = {Toward a theory of test data selection},
  journal = {IEEE TSE},
  year = {1975},
  volume = {1(2)},
  pages = {156--173},
  abstract = {This paper examines the theoretical and

     practical role of testing in software development.

     We prove a fundamental theorem showing that

     properly structured tests are capable of demonstrating

     the absence of errors in a program. The

     theorem's proof hinges on our definition of test

     reliability and validity , but its practical utility

     hinges on being able to show when a test is actually

     reliable . We explain what makes tests

     unreliable (for example, we show by example

     why testing all program statements, predicates,

     or paths is not usually sufficient to insure test

     reliability) , and we outline a possible approach

     to developing reliable tests . We also show how

     the analysis required to define reliable tests can

     help in checking a program's design and specifications

     as well as in preventing and detecting

     implementation errors.},
  crossref = {testing, proofs of correctness},
  owner = {Ilinca},
  read = {yes (not all)},
  relevance = {5},
  timestamp = {2006.04.05},
}




@INPROCEEDINGS{Offutt1999a,
  author = {A. Jefferson Offutt and Yiwei Xiong and Shaoying Liu},
  title = {Criteria for Generating Specification-Based Tests},
  booktitle = {Proceedings of the Fifth IEEE International Conference on Engineering
    of Complex Computer Systems (ICECCS '99)},
  year = {1999},
  pages = {119-131},
  month = {October},
  abstract = {This paper presents general criteria for generating test inputs from
    state-based specifications. Software testing can only be formalized
    and quantified when a solid basis for test generation can be defined.
    Formal specifications of complex systems represent a significant
    opportunity for testing because they precisely describe what functions
    the software is supposed to provide in a form that can easily be
    manipulated. These techniques provide coverage criteria that are
    based on the specifications, and are made up of several parts, including
    test prefixes that contain inputs necessary to put the software
    into the appropriate state for the test values. The test generation
    process includes several steps for transforming specifications to
    tests. Empirical results from a comparative case study application
    of these criteria are presented.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Offutt99_mycomments.doc},
  crossref = {DBLP:conf/iceccs/1999},
  ee = {http://csdl.computer.org/comp/proceedings/iceccs/1999/0434/00/04340119abs.htm},
  pdf = {D:\Ilinca\Docs\Related work\Criteria for Generating Specification-based Tests.pdf},
  read = {yes},
  relevance = {5},
}

@INPROCEEDINGS{Petrenko2001,
  author = {Alexandre Petrenko},
  title = {Specification Based Testing: Towards Practice.},
  booktitle = {Ershov Memorial Conference},
  year = {2001},
  pages = {287-300},
  abstract = {Specification based testing facilities are gradually becoming software
    production aids. The paper shortly considers the current state of
    the art, original ISPRAS/RedVerst experience, and outlines the ways
    for further research and testing tool development. Both conceptual
    and technical problems of novel specification based testing technologies
    introduction are considered.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Petrenko01_mycomments.doc},
  crossref = {DBLP:conf/ershov/2001},
  ee = {http://link.springer.de/link/service/series/0558/bibs/2244/22440287.htm},
  pdf = {D:\Ilinca\Docs\Related work\SpecificationBasedTesting_TowardsPractice.pdf},
  read = {yes},
  relevance = {3},
  url = {http://www.springerlink.com/app/home/contribution.asp?wasp=d9cd04e55a35405ca80a5450d1b8feb6&referrer=parent&backto=issue,29,52;journal,1260,2055;linkingpublicationresults,1:105633,1},
}

@INPROCEEDINGS{dAmorim2006,
  author = {Marcelo d'Amorim and Carlos Pacheco and Darko Marinov and Tao Xie
    and Michael D. Ernst},
  title = {An empirical comparison of automated generation and classification
    techniques for object-oriented unit testing},
  booktitle = {Proc. ASE},
  year = {2006},
  pages = {59--68},
  abstract = {Testing involves two major activities: generating test inputs and
    determining whether they reveal faults. Automated test generation
    techniques include random generation and symbolic execution. Automated
    test classification techniques include ones based on uncaught exceptions
    and violations of operational models inferred from manually provided
    tests. Previous research on unit testing for object-oriented programs
    developed three pairs of these techniques: model-based random testing,
    exception-based random testing, and exception-based symbolic testing.
    We develop a novel pair, model-based symbolic testing. We also empirically
    compare all four pairs of these generation and classification techniques.
    The results show that the pairs are complementary (i.e., reveal
    faults differently), with their respective strengths and weaknesses.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\an_empirical_comparison_of_automated_generation_and_classification_techniques_for_OO_unit_testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.11.10},
}

@BOOK{581795,
  title = {Bug Patterns in Java},
  publisher = {APress L. P.},
  year = {2002},
  author = {E. Allen},
  isbn = {1590590619},
}

@TECHREPORT{Andrews2006,
  author = {J. H. Andrews and S. Haldar and Y. Lei and C. H. Li},
  title = {Randomized Unit Testing: Tool Support and Best Practices},
  institution = {Department of Computer Science, University of Western Ontario},
  year = {2006},
  number = {663},
  month = {January},
  abstract = {Randomization has long been used in testing, but it has not achieved
    widespread acceptance due to a lack of tool support and a failure
    to establish recognized best practices. In this paper, we describe
    RUTE-J, a Java package intended to provide tool support for randomized
    Java unit testing. We also discuss the best practices we have identified
    in our research on randomized unit testing. We report on case studies
    and an experiment in which we applied RUTE-J to various public-domain
    Java classes, finding failures even in mature software and supporting
    the claim that RUTE-J is an efficient, effective tool for unit testing.
    Finally, we compare the use of randomized unit testing to the use
    of other tools such as model checkers, and discuss the tradeoffs.
    We conclude that when best practices are followed, randomized unit
    testing with tool support is useful both as a preparation for full
    software model checking, and in its own right.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Randomized unit testing - tool support and best practices.pdf},
  read = {browsed through},
  relevance = {5},
  timestamp = {2006.04.09},
  url = {http://www.csd.uwo.ca/faculty/andrews/papers/},
}

@INPROCEEDINGS{Andrews2006a,
  author = {James H. Andrews and Susmita Haldar and Yong Lei and Felix Chun Hang
    Li},
  title = {Tool support for randomized unit testing},
  booktitle = {Proc. 1st Intl. Workshop on Random testing},
  year = {2006},
  pages = {36--45},
  abstract = {There are several problem areas that must be addressed when applying

     randomization to unit testing. As yet no general, fully automated

     solution that works for all units has been proposed. We

     therefore have developed RUTE-J, a Java package intended to help

     programmers do randomized unit testing in Java. In this paper, we

     describe RUTE-J and illustrate how it supports the development

     of per-unit solutions for the problems of randomized unit testing.

     We report on an experiment in which we applied RUTE-J to the

     standard Java TreeMap class, measuring the efficiency and effectiveness

     of the technique. We also illustrate the use of randomized

     testing in experimentation, by adapting RUTE-J so that it generates

     randomized minimal covering test suites, and measuring the

     effectiveness of the test suites generated.},
  doi = {http://doi.acm.org/10.1145/1145735.1145741},
  isbn = {1-59593-457-X},
  location = {Portland, Maine},
  pdf = {D:\Ilinca\Docs\Related work\tool_support_for_randomized_unit_testing.pdf},
  read = {no},
  relevance = {6},
}

@ARTICLE{Artho2003,
  author = {C. Artho and H. Barringer and A. Goldberg and K. Havelund

     and S. Khurshid and M. Lowry and C. Pasareanu and G. Ro\c{s}u and
    K. Sen

     and W. Visser and R. Washington},
  title = {Combining Test Case Generation with Run-time Verification},
  journal = {ASM issue of Theoretical Computer Science},
  year = {2003},
  note = {To appear},
  abstract = {Software testing is typically an ad-hoc process where human testers
    manually write test

     inputs and descriptions of expected test results, perhaps automating
    their execution in a

     regression suite. This process is cumbersome and costly. This paper
    reports results on a

     framework to further automate this process. The framework consists
    of combining automated

     test case generation based on systematically exploring the input
    domain of the program

     with runtime verification, where execution traces are monitored and
    verified against

     properties expressed in temporal logic. The input domain of the program
    is explored using

     a model checker extended with symbolic execution. Properties are
    formulated in an expressive

     temporal logic. A methodology is advocated that generates properties
    specific to each

     input instance rather than formulating properties uniformly true
    for all inputs. Capabilities

     for analyzis of concurrency errors are planned to be integrated with
    temporal logic monitoring.

     The paper describes an application of the technology to a planetary
    rover controller.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Artho03_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\Combining test case generation and runtime verification.pdf},
  publisher = {Elsevier},
  read = {yes (but not entirely)},
  relevance = {5},
  url = {http://research.nii.ac.jp/~cartho/papers/asm-tcs.pdf},
}

@INPROCEEDINGS{Artzi2006,
  author = {S. Artzi and M. Ernst and A, Kie{\.z}un and C. Pacheco and J. Perkins},
  title = {Finding the needles in the haystack: Generating legal test inputs
    for object-oriented programs},
  booktitle = {Proc. 1st Workshop on Model-Based Testing and Object-Oriented Systems},
  year = {2006},
  abstract = {A test input for an object-oriented program typically consists of
    a sequence of method calls that use the API defined by the program
    under test. Generating legal test inputs can be challenging because,
    for some programs, the set of legal method sequences is much smaller
    than the set of all possible sequences; without a formal specification
    of legal sequences, an input generator is bound to produce mostly
    illegal sequences.


     We propose a scalable technique that combines dynamic analysis with
    random testing to help an input generator create legal test inputs
    without a formal specification, even for programs in which most
    sequences are illegal. The technique uses an example execution of
    the program to infer a model of legal call sequences, and uses the
    model to guide a random input generator towards legal but behaviorally-diverse
    sequences.


     We have implemented our technique for Java, in a tool called Palulu,
    and evaluated its effectiveness in creating legal inputs for real
    programs. Our experimental results indicate that the technique is
    effective and scalable. Our preliminary evaluation indicates that
    the technique can quickly generate legal sequences for complex inputs:
    in a case study, Palulu created legal test inputs in seconds for
    a set of complex classes, for which it took an expert thirty minutes
    to generate a single legal input.},
  pdf = {D:\Ilinca\Docs\Related work\Generating_legal_test_inputs_for_OO_programs.pdf},
  relevance = {6},
}

@INPROCEEDINGS{1297897,
  author = {N. Ayewah and W. Pugh and D. Morgenthaler and J. Penix and Y. Zhou},
  title = {Using FindBugs on production software},
  booktitle = {OOPSLA companion},
  year = {2007},
  pages = {805--806},
  doi = {http://doi.acm.org/10.1145/1297846.1297897},
  isbn = {978-1-59593-865-7},
  location = {Montreal, Quebec, Canada},
}

@INPROCEEDINGS{Balcer1989,
  author = {M. Balcer and W. Hasling and T. Ostrand},
  title = {Automatic generation of test scripts from formal test specifications},
  booktitle = {TAV3: Proceedings of the ACM SIGSOFT '89 third symposium on Software
    testing, analysis, and verification},
  year = {1989},
  pages = {210--218},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {TSL is a language for writing formal test specifications of the functions
    of a software system. The test specifications are compiled into
    executable test scripts that establish test environments, assign
    values to input variables, perform necessary setup and cleanup operations,
    run the test cases, and check the correctness of test results. TSL
    is a working system that has been used to test commercial software
    in a production environment.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Balcer89_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/75308.75332},
  isbn = {0-89791-342-6},
  location = {Key West, Florida, United States},
  pdf = {D:\Ilinca\Docs\Related work\Automatic generation of test scripts from formal test specifications.pdf},
  read = {yes},
  relevance = {3},
  special_observations = {This paper is very similar to Ostrand88: it builds on the same ideas
    and doesn't bring any significant new information.},
}

@INPROCEEDINGS{Barnett2004,
  author = {Mike Barnett and K. Rustan M. Leino and Wolfram Schulte},
  title = {The Spec\# programming system: An overview},
  booktitle = {Proc. CASSIS},
  year = {2004},
  note = {Springer LNCS 3362},
  timestamp = {2008.02.01},
}

@ARTICLE{41117,
  author = {V. Basili and R. Selby},
  title = {Comparing the Effectiveness of Software Testing Strategies},
  journal = {IEEE TSE},
  year = {1987},
  volume = {13},
  pages = {1278--1296},
  number = {12},
  address = {Piscataway, NJ, USA},
  doi = {http://dx.doi.org/10.1109/TSE.1987.232881},
  issn = {0098-5589},
  publisher = {IEEE Press},
}

@BOOK{533100,
  title = {Software Testing Techniques},
  publisher = {John Wiley \& Sons, Inc.},
  year = {1990},
  author = {B. Beizer},
  address = {New York, NY, USA},
  isbn = {0442245920},
}

@ARTICLE{legeard:MBT-GSM:04,
  author = {E. Bernard and B. Legeard and X. Luck and F. Peureux},
  title = {{Generation of test sequences from formal specifications: GSM 11-11
    standard case study}},
  journal = {Software-Practice \& Experience},
  year = {2004},
  volume = {34},
  pages = {915--948},
  number = {10},
  month = {August},
}

@INPROCEEDINGS{Beyer2004,
  author = {Dirk Beyer and Adam J. Chlipala and Thomas A. Henzinger and Ranjit
    Jhala and Rupak Majumdar},
  title = {Generating Tests from Counterexamples},
  booktitle = {Proceedings of the 26th International Conference on Software Engineering
    (ICSE'04, Edinburgh)},
  year = {2004},
  pages = {326-335},
  publisher = {IEEE Computer Society Press},
  abstract = {We have extended the software model checker Blast to automatically
    generate test suites that guarantee full coverage with respect to
    a given predicate. More precisely, given a C program and a target
    predicate p, Blast determines the set L of program locations which
    program execution can reach with p true, and automatically generates
    a set of test vectors that exhibit the truth of p at all locations
    in L. We have used Blast to generate test suites and to detect dead
    code in C programs with up to 30K lines of code. The analysis and
    test-vector generation is fully automatic (no user intervention)
    and exact (no false positives).},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Beyer04_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\generating_tests_from_counterexamples.pdf},
  read = {yes},
  relevance = {5},
  url = {sherry.ifi.unizh.ch/beyer04generating.html},
}

@INPROCEEDINGS{Boshernitsan2006,
  author = {Marat Boshernitsan and Roongko Doong and Alberto Savoia},
  title = {From {Daikon} to {Agitator}: lessons and challenges in building a
    commercial tool for developer testing},
  booktitle = {ISSTA '06: Proceedings of the 2006 International Symposium on Software
    Testing and Analysis},
  year = {2006},
  pages = {169--180},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/1146238.1146258},
  isbn = {1-59593-263-1},
  location = {Portland, Maine, USA},
  pdf = {D:\Ilinca\Docs\Related work\From_Daikon_to_Agitator.pdf},
}

@INPROCEEDINGS{Boyapati2002,
  author = {Chandrasekhar Boyapati and Sarfraz Khurshid and Darko Marinov},
  title = {Korat: automated testing based on {Java} predicates},
  booktitle = {Proc. Intl. Symp. on Software Testing and Analysis},
  year = {2002},
  pages = {123-133},
  abstract = {This paper presents Korat, a novel framework for automated testing
    of Java programs. Given a formal specification for a method, Korat
    uses the method precondition to automatically generate all (nonisomorphic)
    test cases up to a given small size. Korat then executes the method
    on each test case, and uses the method postcondition as a test oracle
    to check the correctness of each output. To generate test cases
    for a method, Korat constructs a Java predicate (i.e., a method
    that returns a boolean) from the method's pre-condition. The heart
    of Korat is a technique for automatic test case generation: given
    a predicate and a bound on the size of its inputs, Korat generates
    all (nonisomorphic) inputs for which the predicate returns true.
    Korat exhaustively explores the bounded input space of the predicate
    but does so efficiently by monitoring the predicate's executions
    and pruning large portions of the search space. This paper illustrates
    the use of Korat for testing several data structures, including
    some from the Java Collections Framework. The experimental results
    show that it is feasible to generate test cases from Java predicates,
    even when the search space for inputs is very large. This paper
    also compares Korat with a testing framework based on declarative
    specifications. Contrary to our initial expectation, the experiments
    show that Korat generates test cases much faster than the declarative
    framework.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Boyapati02_mycomments.doc},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Korat - Automated Testing Based on Java Predicates.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.08.24},
}

@INPROCEEDINGS{Briand2002,
  author = {L. C. Briand and Y. Labiche and H. Sun},
  title = {Investigating the use of analysis contracts to support fault isolation
    in object oriented code},
  booktitle = {ISSTA '02: Proceedings of the 2002 ACM SIGSOFT international symposium
    on Software testing and analysis},
  year = {2002},
  pages = {70--80},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {A number of activities involved in testing software are known to be
    difficult and time consuming. Among them is the isolation of faults
    once failures have been detected. In this paper, we investigate
    how the instrumentation of contracts could address this issue. Contracts
    are known to be a useful technique to specify the precondition and
    postcondition of operations and class invariants, thus making the
    definition of object-oriented analysis or design elements more precise.
    Our aim in this paper is to reuse and instrument contracts to ease
    testing. A thorough case study is run where we define contracts,
    instrument them using a commercial tool, and assess the benefits
    and limitations of doing so to support the isolation of faults.
    We then draw practical conclusions regarding the applicability of
    the approach and its limitations.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Briand02_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/566172.566183},
  isbn = {1-58113-562-9},
  location = {Roma, Italy},
  pdf = {D:\Ilinca\Docs\Related work\Investigating the use of analysis contracts to support fault isolation in object oriented code.pdf},
  read = {yes},
  relevance = {6},
  url = {http://portal.acm.org/citation.cfm?doid=566172.566183},
}

@INPROCEEDINGS{Ceballos2005,
  author = {Rafael Ceballos and Rafael Martinez Gasca and Diana Borrego},
  title = {Constraint satisfaction techniques for diagnosing errors in design
    by contract software},
  booktitle = {SAVCBS '05: Proceedings of the 2005 conference on Specification and
    verification of component-based systems},
  year = {2005},
  pages = {11},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Design by Contract enables the development of more reliable and robust
    software applications. In this paper, a methodology that diagnoses
    errors in software is proposed. This is based on the combination
    of Design by Contract, Model-based Diagnosis and Constraint Programming.
    Contracts are specified by using assertions. These assertions together
    with an abstraction of the source code are transformed into constraints.
    The methodology detects if the contracts are consistent, and if
    there are incompatibilities between contracts and source code. The
    process is automatic and is based on constraint programming.},
  comments = {Not very understandable and does not go into any detail},
  doi = {http://doi.acm.org/10.1145/1123058.1123070},
  isbn = {1-59593-371-9},
  location = {Lisbon, Portugal},
  read = {yes},
  relevance = {5},
}

@INPROCEEDINGS{Chalin2006,
  author = {Patrice Chalin},
  title = {Are Practitioners Writing Contracts?},
  booktitle = {Springer LNCS 4157},
  year = {2006},
  pages = {100-113},
  abstract = {For decades now, modular design methodologies have helped software
    engineers cope with the size and complexity of modern-day industrial
    applications. To be truly effective though, it is essential that
    module interfaces be rigorously specified. Design by Contract (DBC)
    is an increasingly popular method of interface specification for
    object-oriented systems. Many researchers are actively adding support
    for DBC to various languages such as Ada, Java and C#. Are these
    research efforts justified? Does having support for DBC mean that
    developers will make use of it? We present the results of an empirical
    study measuring the proportion of assertion statements used in Eiffel
    contracts. The study results indicate that programmers using Eiffel
    (the only active language with built-in support for DBC) tend to
    write assertions in a proportion that is higher than for other languages.

     Keywords: design by contract, program assertions, empirical study,
    Eiffel.},
  doi = {10.1007/11916246},
  owner = {Ilinca},
  timestamp = {2007.10.11},
}

@ARTICLE{Chan1996,
  author = {F. T. Chan and T. Y. Chen and I. K. Mak and Y. T. Yu},
  title = {Proportional sampling strategy: guidelines for software testing practitioners},
  journal = {Information and Software Technology},
  year = {1996},
  volume = {38},
  pages = {775--782},
  number = {12},
  abstract = {Recently, several sufficient conditions have been developed that guarantee
    partition testing to have a higher probability of detecting at least
    one failure than random testing. One of these conditions is that
    the number of test cases selected from each partition is proportional
    to the size of the partition. We call such a method of allocating
    test cases the proportional sampling strategy. Although this condition
    is not the most general one, it is the most easily and practically
    applicable one. In this paper, we discuss how the proportional sampling
    strategy can be applied effectively in practice. Some practical
    issues that need to be attended are identified and guidelines to
    deal with these issues are suggested.},
  citeseerurl = {http://citeseer.ist.psu.edu/context/1179915/0},
  keywords = {Partition testing; Random testing; Selection of test data},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Proportional sampling strategy.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://www.ingentaconnect.com/content/els/09505849/1996/00000038/00000012/art01103;jsessionid=11wv0vigm8pia.alice},
}

@INPROCEEDINGS{Chan2002,
  author = {Kwok Ping Chan and Tsong Yueh Chen and Dave Towey},
  title = {Restricted Random Testing},
  booktitle = {Proceedings of the 7th International Conference on Software Quality},
  year = {2002},
  pages = {321 - 330},
  publisher = {Springer-Verlag, London, UK},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Restricted random testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.04.11},
}

@INPROCEEDINGS{Chen2004a,
  author = {T.Y. Chen and R. Merkel and P.K. Wong and G. Eddy},
  title = {Adaptive random testing through dynamic partitioning},
  booktitle = {Proceedings of the Fourth International Conference on Quality Software},
  year = {2004},
  volume = {00},
  pages = {79 - 86},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society},
  abstract = {Adaptive random testing (ART) describes a family of algorithms for
    generating random test cases that have been experimentally demonstrated
    to have greater fault-detection capacity than simple random testing.
    We outline and demonstrate two new ART algorithms, and demonstrate
    experimentally that they offer similar performance advantages, with
    considerably lower overhead than other ART algorithms.},
  doi = {http://doi.ieeecomputersociety.org/10.1109/QSIC.2004.1357947},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Adaptive Random Testing through dynamic partitioning.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.04.11},
  url = {http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=1357947},
}

@INPROCEEDINGS{Chen2003,
  author = {T. Y. Chen and F. C. Kuo and R. G. Merkel and S. P. Ng},
  title = {Mirror adaptive random testing},
  booktitle = {Proceedings of the Third International Conference on Quality Software},
  year = {2003},
  pages = {4 - 11},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society},
  doi = {10.1109/QSIC.2003.1319079},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Mirror adaptive random testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.04.11},
}

@INPROCEEDINGS{Chen2004,
  author = {T. Y. Chen and H. Leung and I. K. Mak},
  title = {Adaptive Random Testing},
  booktitle = {Proc. Asian Computing Science Conference},
  year = {2004},
  editor = {Michael J. Maher},
  abstract = {In this paper, we introduce an enhanced form of random testing called
    Adaptive Random Testing. Adaptive random testing seeks to distribute
    test cases more evenly within the input space. It is based on the
    intuition that for non-point types of failure patterns, an even
    spread of test cases is more likely to detect failures using fewer
    test cases than ordinary random testing. Experiments are performed
    using published programs. Results show that adaptive random testing
    does outperform ordinary random testing significantly (by up to
    as much as 50%) for the set of programs under study. These results
    are very encouraging, providing evidences that our intuition is
    likely to be useful in improving the effectiveness of random testing.},
  doi = {10.1007/b103476},
  pdf = {D:\Ilinca\Docs\Related work\Adaptive Random Testing.pdf},
  read = {yes},
  relevance = {6},
}

@INPROCEEDINGS{Chen2005,
  author = {Tsong Yueh Chen and Robert Merkel},
  title = {Quasi-random testing},
  booktitle = {ASE '05: Proceedings of the 20th IEEE/ACM international Conference
    on Automated software engineering},
  year = {2005},
  pages = {309--312},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Quasi-random sequences, also known as low-discrepancy or low-dispersion
    sequences, are sequences of points in an n-dimensional unit hypercube.
    These sequences have the property that points are spread more evenly
    throughout the cube than random point sequences, which result in
    regions where there are clusters of points and others that are sparsely
    populated. Based on the observation that program faults tend to
    lead to contiguous failure regions within a program's input domain,
    and that an even spread of random tests enhances the failure detection
    effectiveness for certain failure patterns, we examine the use of
    these sequences as a replacement for random sequences in automated
    testing.The limited number of quasi-random sequences available from
    the standard algorithms poses significant practical problems for
    use when testing real programs, and especially for evaluating its
    effectiveness. We examine the use of randomised quasi-random sequences,
    which are permuted in a nondeterministic fashion but still retain
    their low discrepancy properties, to overcome this problem, and
    show that testing using randomised quasi-random sequences is often
    significantly more effective than random testing.},
  doi = {http://doi.acm.org/10.1145/1101908.1101957},
  isbn = {1-59593-993-4},
  location = {Long Beach, CA, USA},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Quasi-random testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.05.19},
}

@ARTICLE{Chen1994,
  author = {T. Y. Chen and Y. T. Yu},
  title = {On the Relationship Between Partition and Random Testing},
  journal = {IEEE TSE},
  year = {1994},
  volume = {20},
  pages = {977--980},
  number = {12},
  abstract = {Weyuker and Jeng have investigated the conditions that affect the
    performance of partition testing and have compared analytically
    the fault-detecting ability of partition testing and random testing.
    This paper extends and generalizes some of their results. We give
    more general ways of characterizing the worst case for partition
    testing, along with a precise characterization of when this worst
    case is as good as random testing. We also find that partition testing
    is guaranteed to perform at least as well as random testing so long
    as the number of test cases selected is in proportion to the size
    of the subdomains.},
  address = {Piscataway, NJ, USA},
  doi = {http://dx.doi.org/10.1109/32.368132},
  issn = {0098-5589},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\On the relationship between partition and random testing.pdf},
  publisher = {IEEE Press},
  read = {yes},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://portal.acm.org/citation.cfm?id=203108&dl=GUIDE&coll=GUIDE#},
}

@ARTICLE{177364,
  author = {R. Chillarege and I.S. Bhandari and J.K. Chaar and M.J. Halliday
    and D.S. Moebus and B.K. Ray and M.-Y. Wong},
  title = {Orthogonal Defect Classification-A Concept for In-Process Measurements},
  journal = {IEEE TSE},
  year = {1992},
  volume = {18},
  pages = {943-956},
  number = {11},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/32.177364},
  issn = {0098-5589},
  publisher = {IEEE Computer Society},
}

@INPROCEEDINGS{Ciupa2007,
  author = {I. Ciupa and A. Leitner and M. Oriol and B. Meyer},
  title = {Experimental Assessment of Random Testing for Object-Oriented Software},
  booktitle = {Proc. ISSTA},
  year = {2007},
  pages = {84--94},
  owner = {Ilinca},
  timestamp = {2007.04.26},
}

@INPROCEEDINGS{Ciupa2006,
  author = {Ilinca Ciupa and Andreas Leitner and Manuel Oriol and Bertrand Meyer},
  title = {Object distance and its application to adaptive random testing of
    object-oriented programs},
  booktitle = {RT '06: Proceedings of the 1st international workshop on Random testing},
  year = {2006},
  pages = {55--63},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/1145735.1145744},
  isbn = {1-59593-457-X},
  location = {Portland, Maine},
}

@INPROCEEDINGS{Ciupa2005,
  author = {I. Ciupa and A. Leitner.},
  title = {Automatic Testing Based on Design by Contract},
  booktitle = {Proceedings of Net.ObjectDays 2005 (6th Annual International Conference
    on Object-Oriented and Internet-based Technologies, Concepts, and
    Applications for a Networked World)},
  year = {2005},
  pages = {545-557},
  month = {September 19-22},
  pdf = {D:\Ilinca\Docs\papers\SOQUA_05\Automatic_Testing_Based_on_Design_by_Contract.pdf},
}


@INPROCEEDINGS{Ciupa2008-classes,
  author = {Ilinca Ciupa and Bertrand Meyer and Manuel Oriol and Alexander Pretschner},
  title = {{Finding Faults:
Manual Testing vs. Random Testing+ vs. User Reports}},
  year = {2008},
note={Submitted to ISSRE}
}



@INPROCEEDINGS{Ciupa2008,
  author = {Ilinca Ciupa and Alexander Pretschner and Andreas Leitner and Manuel
    Oriol and Bertrand Meyer},
  title = {On the Predictability of Random Tests for Object-Oriented Software},
  booktitle = {Proceedings of the First International Conference on Software Testing,
    Verification and Validation (ICST'08)},
  year = {2008},
  pages={72--81},
  month = {April}
}


@ARTICLE{Claessen2000,
  author = {Koen Claessen and John Hughes},
  title = {{QuickCheck}: a lightweight tool for random testing of {Haskell}
    programs},
  journal = {ACM SIGPLAN Notices},
  year = {2000},
  volume = {35},
  pages = {268--279},
  number = {9},
  pdf = {D:\Ilinca\Docs\Related work\QuickCheck.pdf},
  read = {no},
  relevance = {4},
}

@INPROCEEDINGS{Cousot2000,
  author = {P. Cousot and R. Cousot},
  title = {Abstract Interpretation Based Program Testing},
  booktitle = {Proceedings of the SSGRR 2000 Computer \& eBusiness International
    Conference},
  year = {2000},
  address = {Compact disk paper 248 and electronic proceedings \url{http://www.ssgrr.it/en/ssgrr2000/proceedings.htm},
    L'Aquila, Italy},
  month = {July 31 -- August 6},
  publisher = {Scuola Superiore G{.} Reiss Romoli},
  abstract = {Every one can daily experiment that programs are bugged. Software
    bugs can have severe if not catastrophic consequences in computer-based
    safety critical applications. This impels the development of formal
    methods, whether manual, computer-assisted or automatic, for verifying
    that a program satisfies a specification. Among the automatic formal
    methods, program static analysis can be used to check for the absence
    of run-time errors. In this case the specification is provided by
    the semantics of the programming language in which the program is
    written. Abstract interpretation provides a formal theory for approximating
    this semantics, which leads to completely automated tools where
    run-time bugs can be statically and safely classified as unreachable,
    certain, impossible or potential. We discuss the extension of these
    techniques to abstract testing where specifications are provided
    by the programmers. Abstract testing is compared to program debugging
    and model-checking.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Cousot00_mycomments.doc},
  isbn = {88-85280-52-8},
  pdf = {D:\Ilinca\Docs\Related work\Abstract interpretation-based program testing.pdf},
  read = {yes},
  relevance = {5},
  special_observations = {Abstract interpretation was formalized by Patrick Cousot (with this
    paper?)},
  url = {http://www.di.ens.fr/~cousot/COUSOTpapers/SSGRRP-00-PC-RC.shtml},
}

@INPROCEEDINGS{Csallner2006,
  author = {Christoph Csallner and Yannis Smaragdakis},
  title = {{DSD-Crasher: A Hybrid Analysis Tool for Bug Finding}},
  booktitle = {Proc. Intl. Symp. on Software Testing and Analysis},
  year = {2006},
  pages = {245--254},
  month = jul,
  abstract = {DSD-Crasher is a bug finding tool that follows a three-step approach
    to program analysis: D. Capture the program's intended execution
    behavior with dynamic invariant detection. The derived invariants
    exclude many unwanted values from the program's input domain. S.
    Statically analyze the program within the restricted input domain
    to explore many paths. D. Automatically generate test cases that
    focus on verifying the results of the static analysis. Thereby confirmed
    results are never false positives, as opposed to the high false
    positive rate inherent in conservative static analysis. This three-step
    approach yields benefits compared to past two-step combinations
    in the literature. In our evaluation with third-party applications,
    we demonstrate higher precision over tools that lack a dynamic step
    and higher efficiency over tools that lack a static step.},
  pdf = {D:\Ilinca\Docs\Related work\dsd_crasher.pdf},
}

@INPROCEEDINGS{Csallner2005,
  author = {Christoph Csallner and Yannis Smaragdakis},
  title = {CnC: Combining Static Checking and Testing},
  booktitle = {International Conference on Software Engineering (ICSE)},
  year = {2005},
  pages = {422--431},
  month = may,
}

@ARTICLE{Csallner2004,
  author = {Christoph Csallner and Yannis Smaragdakis},
  title = {{JCrasher}: an automatic robustness tester for {Java}},
  journal = {Software: Practice and Experience},
  year = {2004},
  volume = {34},
  pages = {1025--1050},
  number = {11},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1002/spe.602},
  issn = {0038-0644},
  pdf = {D:\Ilinca\Docs\Related work\JCrasher_An_automatic_robustness_tester_for_Java.pdf},
  publisher = {John Wiley \& Sons, Inc.},
}

@INPROCEEDINGS{Demers1995,
  author = {F.-N. Demers and J. Malenfant},
  title = {Reflection in logic, functional and object-oriented programming:
    a short comparative study},
  booktitle = {Proc. IJCAI'95, Workshop on Reflection and Metalevel Architectures
    and their Applications in AI},
  year = {1995},
  pages = {29-38},
  abstract = {Reflection is a wide-ranging concept that has been studied independently
    in many different areas of science in general, and computer science
    in particular. Even in the sub-area of programming languages, it
    has been applied to different paradigms, especially the logic, functional
    and objectoriented ones. Partly because of different past influences,
    but also because researchers in these communities scarcely talk
    to each others, concepts have evolved separately, sometimes to the
    point where it is...},
  citeseerurl = {http://citeseer.ist.psu.edu/106401.html},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\reflection-in-logic-functional.pdf},
  read = {yes},
  timestamp = {2006.02.20},
  url = {http://citeseer.ist.psu.edu/106401.html},
}

@INPROCEEDINGS{leveson:heteMCDC:00,
  author = {A. Dupuy and N. Leveson},
  title = {{An empirical evaluation of the MC/DC coverage criterion on theHETE-2
    satellite software}},
  booktitle = {Proc. 19th Digital Avionics Systems Conferences},
  year = {2000},
  pages = {1B6/1--1B6/7},
}


@ARTICLE{duran:random:84,
  author = {J. Duran and S. Ntafos},
  title = {{An Evaluation of Random Testing}},
  journal = {IEEE TSE},
  year = {1984},
  volume = {SE-10},
  pages = {438--444},
  number = {4},
  month = {July},
}

@PHDTHESIS{Ernst2000,
  author = {Michael D. Ernst},
  title = {Dynamically Discovering Likely Program Invariants},
  school = {University of Washington Department of Computer Science and Engineering},
  year = {2000},
  month = {August},
  owner = {Ilinca},
  relevance = {6},
  timestamp = {2008.04.16},
}

@INPROCEEDINGS{838485,
  author = {E. Farchi and Y. Nir and S. Ur},
  title = {Concurrent Bug Patterns and How to Test Them},
  booktitle = {Proc 17th Intl. Symp. on Parallel and Distributed Processing},
  year = {2003},
  pages = {286.2},
  isbn = {0-7695-1926-1},
}

@INPROCEEDINGS{Forrester2000,
  author = {J. E. Forrester and B. P. Miller},
  title = {{An empirical study of the robustness of Windows NT applications
    using random testing}},
  booktitle = {4th USENIX Windows Systems Symposium},
  year = {2000},
  address = {Seattle},
  month = {August},
  abstract = {We report on the third in a series of studies on the reliability of
    application programs in the face of random input. In 1990 and 1995,
    we studied the reliability of UNIX application programs, both command
    line and X-Window based (GUI). In this study, we apply our testing
    techniques to applications running on the Windows NT operating system.
    Our testing is simple black-box random input testing; by any measure,
    it is a crude technique, but it seems to be effective at locating
    bugs in real programs.


     We tested over 30 GUI-based applications by subjecting them to two
    kinds of random input: (1) streams of valid keyboard and mouse events
    and (2) streams of random Win32 messages. We have built a tool that
    helps automate the testing of Windows NT applications. With a few
    simple parameters, any application can be tested.


     Using our random testing techniques, our previous UNIX-based studies
    showed that we could crash a wide variety of command-line and X-window
    based applications on several UNIX platforms. The test results are
    similar for NT-based applications. When subjected to random valid
    input that could be produced by using the mouse and keyboard, we
    crashed 21% of applications that we tested and hung an additional
    24% of applications. When subjected to raw random Win32 messages,
    we crashed or hung all the applications that we tested. We report
    which applications failed under which tests, and provide some analysis
    of the failures.},
  owner = {Ilinca},
  timestamp = {2007.09.20},
}

@INPROCEEDINGS{frankl:condcov:98,
  author = {P. Frankl and O. Iakounenko},
  title = {{Further Empirical Studies of Test Effectiveness}},
  booktitle = {Proc. FSE},
  year = {1998},
  pages = {153--162},
}

@ARTICLE{frankl:expCoverage:93,
  author = {P. Frankl and S. Weiss},
  title = {{An Experimental Comparison of the Effectiveness of Branch Testing
    and Data Flow Testing}},
  journal = {IEEE TSE},
  year = {1993},
  volume = {19},
  pages = {774--787},
  number = {8},
}

@MISC{frankl94alluses,
  author = {P. Frankl and S. Weiss and C. Hu},
  title = {All-uses versus mutation testing: An experimental comparison of effectiveness},
  year = {1994},
  text = {P. G. Frankl, S. N. Weiss, and C. Hu. All-uses versus mutation testing:


     An experimental comparison of effectiveness. Technical report PUCS-100-94,


     Department of Computer Science, Polytechnic University, Brooklyn,
    NY, February


     1994. Submitted for publication.},
  url = {citeseer.ist.psu.edu/frankl96alluses.html},
}

@ARTICLE{franklWeyuker:dataFlowTest:88,
  author = {P. Frankl and E. Weyuker},
  title = {{An Applicable Family of Data Flow Testing Criteria}},
  journal = {IEEE TSE},
  year = {1998},
  volume = {14},
  pages = {1483-1498},
  number = {10},
}

@INPROCEEDINGS{Gall2007,
  author = {Pascale Le Gall and Nicolas Rapin and Assia Touil},
  title = {Symbolic execution techniques for refinement testing},
  booktitle = {Proceedings of the International Conference on Tests and Proofs (TAP)
    2007},
  year = {2007},
  volume = {LNCS},
  publisher = {Springer-Verlag},
  abstract = {We propose an approach to test whether an abstract specification

     is refined or not by a more concrete one. The specifications are

     input / output symbolic transition systems (IOSTS). The refinement
    relation

     requires that all traces of the abstract system are also traces of
    the

     concrete system, up to some signature inclusion. Our work takes inspiration

     from the conformance testing area. Symbolic execution techniques

     allow us to select traces of the abstract system and to submit them
    on

     the concrete specification. Each trace execution leads to a verdict
    Fail,

     Pass or Warning. The verdict Pass is provided with a formula which

     has to be verified by the values only manipulated at the level of
    the

     concrete specification in order to ensure the refinement relation.
    The

     verdict Warning reports that the concrete specification has not been

     sufficiently explored to give a reliable verdict. This is thus a
    partial verification

     process, related to the quality of the set of selected traces and

     of the exploration of the concrete specification. Our approach has
    been

     implemented and is demonstrated on a simple example.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\symbolic_execution_techniques_for_refinement_testing.pdf},
  read = {yes},
  relevance = {4},
  review = {They propose a method for testing if a specification refines another
    one. They target IOSTS (inout-output symbolic transition systems).},
  timestamp = {2007.02.16},
}

@INPROCEEDINGS{girgis:coverageeffectiveness:86,
  author = {M. Girgis and M. Woodward},
  title = {An experimental comparison of the error exposing ability of program
    testing criteria},
  booktitle = {Proc. IEEE/ACM workshop on software testing},
  year = {1986},
  pages = {64--73},
  month = {July},
}

@INPROCEEDINGS{Godefroid2005,
  author = {Patrice Godefroid and Nils Klarlund and Koushik Sen},
  title = {{DART: directed automated random testing}},
  booktitle = {{Proc. ACM SIGPLAN Conference on Programming Language Design and
    Implementation}},
  year = {2005},
  pages = {213--223},
  abstract = {We present a new tool, named DART, for automatically testing software
    that combines three main techniques: (1) automated extraction of
    the interface of a program with its external environment using static
    source-code parsing; (2) automatic generation of a test driver for
    this interface that performs random testing to simulate the most
    general environment the program can operate in; and (3) dynamic
    analysis of how the program behaves under random testing and automatic
    generation of new test inputs to direct systematically the execution
    along alternative program paths. Together, these three techniques
    constitute Directed Automated Random Testing, or DART for short.
    The main strength of DART is thus that testing can be performed
    completely automatically on any program that compiles -- there is
    no need to write any test driver or harness code. During testing,
    DART detects standard errors such as program crashes, assertion
    violations, and non-termination. Preliminary experiments to unit
    test several examples of C programs are very encouraging.},
  doi = {http://doi.acm.org/10.1145/1065010.1065036},
  isbn = {1-59593-056-6},
  location = {Chicago, IL, USA},
  pdf = {D:\Ilinca\Docs\Related work\dart.pdf},
  read = {no},
  relevance = {5},
}

@INPROCEEDINGS{gray86why,
  author = {J. Gray},
  title = {Why Do Computers Stop and What Can Be Done About It?},
  booktitle = {Symposium on Reliability in Distributed Software and Database Systems},
  year = {1986},
  pages = {3-12},
}

@INPROCEEDINGS{Guo2006,
  author = {Philip J. Guo and Jeff H. Perkins and Stephen McCamant and Michael
    D. Ernst},
  title = {Dynamic inference of abstract types},
  booktitle = {ISSTA '06: Proceedings of the 2006 International Symposium on Software
    Testing and Analysis},
  year = {2006},
  pages = {255--265},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {An abstract type groups variables that are used for related purposes

     in a program. We describe a dynamic unification-based analysis

     for inferring abstract types. Initially, each run-time value gets
    a

     unique abstract type. A run-time interaction among values indicates

     that they have the same abstract type, so their abstract types

     are unified. Also at run time, abstract types for variables are accumulated

     from abstract types for values. The notion of interaction

     may be customized, permitting the analysis to compute finer

     or coarser abstract types; these different notions of abstract type

     are useful for different tasks. We have implemented the analysis

     for compiled x86 binaries and for Java bytecodes. Our experiments

     indicate that the inferred abstract types are useful for program
    comprehension,

     improve both the results and the run time of a follow-on

     program analysis, and are more precise than the output of a comparable

     static analysis, without suffering from overfitting.},
  doi = {http://doi.acm.org/10.1145/1146238.1146268},
  isbn = {1-59593-263-1},
  location = {Portland, Maine, USA},
  pdf = {D:\Ilinca\Docs\Related work\dynamic_inference_of_abstract_types.pdf},
}


@ARTICLE{gutjahr:random:99,
  author = {W. Gutjahr},
  title = {{Partition testing versus random testing: the influence of uncertainty}},
  journal = {IEEE TSE},
  year = {1999},
  volume = {25},
  pages = {661--674},
  number = {5},
}

@INPROCEEDINGS{Hamlet2006,
  author = {Dick Hamlet},
  title = {When only random testing will do},
  booktitle = {Proc. 1st Intl. Workshop on Random Testing},
  year = {2006},
  pages = {1--9},
  doi = {http://doi.acm.org/10.1145/1145735.1145737},
  isbn = {1-59593-457-X},
}

@ARTICLE{hamlet:random:90,
  author = {D. Hamlet and R. Taylor},
  title = {{Partition Testing Does Not Inspire Confidence}},
  journal = {IEEE TSE},
  year = {1990},
  volume = {16},
  pages = {1402--1411},
  number = {12},
  month = dec,
}

@INCOLLECTION{Hamlet1994,
  author = {R. Hamlet},
  title = {Random testing},
  booktitle = {Encyclopedia of Software Engineering},
  publisher = {Wiley},
  year = {1994},
  editor = {J. Marciniak},
  pages = {970--978},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Random Testing.pdf},
  read = {no},
  relevance = {6},
  timestamp = {2006.04.14},
}

@INPROCEEDINGS{Hartman2004,
  author = {A. Hartman and K. Nagin},
  title = {The AGEDIS tools for model based testing},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
    on Software testing and analysis},
  year = {2004},
  pages = {129--132},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {We describe the tools and interfaces created by the AGEDIS project,
    a European Commission sponsored project for the creation of a methodology
    and tools for automated model driven test generation and execution
    for distributed systems. The project includes an integrated environment
    for modeling, test generation, test execution, and other test related
    activities. The tools support a model based testing methodology
    that features a large degree of automation and also includes a feedback
    loop integrating coverage and defect analysis tools with the test
    generator and execution framework. Prototypes of the tools have
    been tried in industrial settings providing important feedback for
    the creation of the next generation of tools in this area.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Hartman04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007529},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
  pdf = {D:\Ilinca\Docs\Related work\The AGEDIS tools for model based testing.pdf},
  read = {yes},
  relevance = {4},
  url = {http://portal.acm.org/citation.cfm?doid=1007529#},
}

@INPROCEEDINGS{heimdahlgeorge:testsuitereduction:04,
  author = {M. Heimdahl and D. George},
  title = {{Test Suite Reduction for Model Based Tests: Effects on Test Quality
    and Implications for testing}},
  booktitle = {Proc. ASE},
  year = {2004},
  pages = {176--185},
}

@INPROCEEDINGS{heimdahl:coverageMC:04,
  author = {M. Heimdahl and D. George and R. Weber},
  title = {{Specification Test Coverage Adequacy Criteria = Specification Test
    Generation Inadequacy Criteria?}},
  booktitle = {Proc. 8th IEEE High Assurance in Systems Engineering Workshop},
  year = {2004},
  month = {February},
}

@INPROCEEDINGS{1021569,
  author = {K. Henningsson and C. Wohlin},
  title = {Assuring Fault Classification Agreement -- An Empirical Evaluation},
  booktitle = {Proc. Intl. Symp. on Empirical Software Engineering},
  year = {2004},
  pages = {95--104},
  doi = {http://dx.doi.org/10.1109/ISESE.2004.13},
  isbn = {0-7695-2165-7},
}

@ARTICLE{1052895,
  author = {D. Hovemeyer and W. Pugh},
  title = {Finding bugs is easy},
  journal = {SIGPLAN Not.},
  year = {2004},
  volume = {39},
  pages = {92--106},
  number = {12},
  doi = {http://doi.acm.org/10.1145/1052883.1052895},
  issn = {0362-1340},
}

@INPROCEEDINGS{257766,
  author = {M. Hutchins and H. Foster and T. Goradia and T. Ostrand},
  title = {Experiments of the effectiveness of dataflow- and controlflow-based
    test adequacy criteria},
  booktitle = {ICSE '94: Proceedings of the 16th international conference on Software
    engineering},
  year = {1994},
  pages = {191--200},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society Press},
  isbn = {0-8186-5855-X},
  location = {Sorrento, Italy},
}

@INPROCEEDINGS{hutchins:structCovEffective:94,
  author = {M. Hutchins and H. Foster and T. Goradia and T. Ostrand},
  title = {Experiments of the effectiveness of dataflow- and controlflow-based
    test adequacy criteria},
  booktitle = {Proc. ICSE},
  year = {1994},
  pages = {191--200},
}

@ARTICLE{296778,
  author = {M. Hutchins and H. Foster Goradia and T. Ostrand},
  title = {Experiments on the effectiveness of dataflow- and control-flow-based
    test adequacy criteria},
  journal = {Software Engineering, 1994. Proceedings. ICSE-16., 16th International
    Conference on},
  year = {16-21 May 1994},
  pages = {191-200},
  doi = {10.1109/ICSE.1994.296778},
  keywords = {program testing, software engineeringcontrol-flow-based test adequacy
    criteria, coverage criteria, dataflow-based test adequacy criteria,
    faulty program versions, test sets},
}

@ARTICLE{Jezequel2001,
  author = {Jean-Marc Jezequel and Daniel Deveaux and Yves Le Traon},
  title = {Reliable Objects: a Lightweight Approach Applied to Java},
  journal = {IEEE Software},
  year = {2001},
  volume = {18(4)},
  pages = {76--83},
  month = {July/August},
  abstract = {Small scale software developments need specific low cost and low overhead
    methods and tools to deliver quality products within tight time
    and budget constraints. This is particularly true of testing, because
    of its cost and impact on final product reliability. We propose
    a lightweight approach to embed tests into components, making them
    self testable. We also propose a method to evaluate testing efficiency,
    based on mutation techniques, which ultimately provides an estimation
    of a component's quality. This allows the software developer to
    consciously trade reliability for resources. Our methodology has
    been implemented in the Eiffel, Java, C++ and Perl languages. The
    Java implementation, built on top of iContract, is outlined here.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Jezequel01_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\reliable objects - a lightweight approach applied to Java.pdf},
  read = {yes},
  relevance = {6},
}

@ARTICLE{1022544,
  author = {N. Juristo and A. M. Moreno and S. Vegas},
  title = {Towards building a solid empirical body of knowledge in testing techniques},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2004},
  volume = {29},
  pages = {1--4},
  number = {5},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/1022494.1022544},
  issn = {0163-5948},
  publisher = {ACM},
}

@INPROCEEDINGS{651507,
  author = {E. Kamsties and C. Lott},
  title = {An Empirical Evaluation of Three Defect-Detection Techniques},
  booktitle = {Proc. ESEC},
  year = {1995},
  pages = {362--383},
  isbn = {3-540-60406-5},
}

@INPROCEEDINGS{Khurshid2003,
  author = {Sarfraz Khurshid and Corina S. Pasareanu and Willem Visser},
  title = {Generalized Symbolic Execution for Model Checking and Testing.},
  booktitle = {Proc. Intl. Conf. on Tools and Algorithms for the Construction and
    Analysis of Systems},
  year = {2003},
  volume = {Springer LNCS 2619},
  pages = {553-568},
  abstract = {Modern software systems, which often are concurrent and manipulate
    complex data structures must be extremely reliable. We present a
    novel framework based on symbolic execution, for automated checking
    of such systems. We provide a two-fold generalization of traditional
    symbolic execution based approaches. First, we define a source to
    source translation to instrument a program, which enables standard
    model checkers to perform symbolic execution of the program. Second,
    we give a novel symbolic execution algorithm that handles dynamically
    allocated structures (e.g., lists and trees), method preconditions
    (e.g., acyclicity), data (e.g., integers and strings) and concurrency.
    The program instrumentation enables a model checker to automatically
    explore different program heap configurations and manipulate logical
    formulae on program data (using a decision procedure). We illustrate
    two applications of our framework: checking correctness of multi-threaded
    programs that take inputs from unbounded domains with complex structure
    and generation of non-isomorphic test inputs that satisfy a testing
    criterion. Our implementation for Java uses the Java PathFinder
    model checker.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Khurshid03_mycomments.doc},
  ee = {http://link.springer.de/link/service/series/0558/bibs/2619/26190553.htm},
  pdf = {D:\Ilinca\Docs\Related work\Generalized symbolic execution for model checking and testing.pdf},
  read = {yes},
  relevance = {5},
}

@ARTICLE{66416,
  author = {D. E. Knuth},
  title = {The errors of TEX},
  journal = {Softw. Pract. Exper.},
  year = {1989},
  volume = {19},
  pages = {607--685},
  number = {7},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1002/spe.4380190702},
  issn = {0038-0644},
  publisher = {John Wiley \& Sons, Inc.},
}

@ARTICLE{Le2006,
  author = {Yves {Le Traon} and Benoit Baudry and {Jean-Marc} J\'ez\'equel},
  title = {{Design by Contract to Improve Software Vigilance}},
  journal = {IEEE TSE},
  year = {2006},
  volume = {32},
  pages = {571--586},
  number = {8},
}

@MISC{Leavens2005,
  author = {Gary T. Leavens and Yoonsik Cheon},
  title = {Design by Contract with JML},
  howpublished = {on web page},
  month = {January},
  year = {2005},
  note = {documentation on JML, not a conference paper},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Leavens05_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\jmldbc.pdf},
  read = {yes},
  relevance = {5},
  url = {www.cs.caltech.edu/~cs141/resources/JML/docs/jmltutorial/jmldbc.pdf},
}

@MISC{Leitner2005-2007,
  author = {A. Leitner and I. Ciupa},
  title = {{AutoTest}},
  howpublished = {\url{se.inf.ethz.ch/people/leitner/auto_test/}},
  year = {2005 - 2007},
  owner = {Ilinca},
  timestamp = {2006.04.14},
}

@INPROCEEDINGS{Leitner2007,
  author = {A. Leitner and I. Ciupa and B. Meyer and M. Howard},
  title = {Reconciling Manual and Automated Testing: the AutoTest Experience},
  booktitle = {Proc. HICCS},
  year = {2007},
}

@INPROCEEDINGS{Leitner2007b,
  author = {Andreas Leitner and Patrick Eugster and Manuel Oriol and Ilinca Ciupa},
  title = {Reflecting on an Existing Programming Language},
  booktitle = {Proceedings of TOOLS EUROPE 2007 - Objects, Models, Components, Patterns,},
  year = {2007},
  month = {July},
}

@INPROCEEDINGS{leitner:AutoTestMinimization:07,
  author = {A. Leitner and M. Oriol and I. Ciupa and A. Zeller and B. Meyer},
  title = {{Efficient Unit Test Case Minimization}},
  booktitle = {Proc. ASE},
  year = {2007},
}

@INPROCEEDINGS{Leitner2007a,
  author = {A. Leitner and M. Oriol and A. Zeller and I. Ciupa and B. Meyer},
  title = {Efficient Unit Test Case Minimization},
  booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated
    Software Engineering (ASE 2007)},
  year = {2007},
  month = {November},
}

@ARTICLE{Levenshtein1965,
  author = {Vladimir I. Levenshtein},
  title = {Binary codes capable of correcting deletions, insertions, and reversals},
  journal = {Doklady Akademii Nauk SSSR},
  year = {1965},
  volume = {163},
  pages = {845-848},
  number = {4},
  owner = {Ilinca},
  timestamp = {2006.04.20},
}

@INPROCEEDINGS{Liu2007,
  author = {Lisa (Ling) Liu and Bertrand Meyer and Bernd Schoeller},
  title = {Using Contracts and Boolean Queries to Improve the Quality of Automatic
    Test Generation},
  booktitle = {Proceedings of the International Conference on Tests and Proofs (TAP)
    2007},
  year = {2007},
  volume = {LNCS},
  publisher = {Springer-Verlag},
  abstract = {Since test cases cannot be exhaustive, any effective test case generation
    strategy must identify the execution states most likely to uncover
    bugs. The key issue is to define criteria for selecting such interesting
    states.


     If the units being tested are classes in object-oriented programming,
    it seems attractive to rely on the boolean queries present in each
    class, which indeed define criteria on the states of the corresponding
    objects, and  in contract-equipped O-O software  figure prominently
    in preconditions, postconditions and invariants. As these queries
    are part of the class specification and hence relevant to its clients,
    one may conjecture that the resulting partition of the state space
    is also relevant for tests.


     We explore this conjecture by examining whether relying on the boolean
    queries of a class to extract abstract states improves the results
    of black-box testing. The approach uses proof techniques to generate
    objects that satisfy the class invariants, then performs testing
    by relying on postconditions as test oracles.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\using_contracts_and_boolean_queries_to_improve_the_quality_of_automatic_test_generation.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2007.02.16},
}

@INPROCEEDINGS{lutz93targeting,
  author = {R. Lutz},
  title = {{Targeting Safety-Related Errors During Software Requirements Analysis}},
  booktitle = {Proc. {ACM} {SIGSOFT} FSE},
  year = {1993},
  pages = {99--106},
  url = {citeseer.ist.psu.edu/article/lutz96targeting.html},
}

@ARTICLE{Mankefors2003,
  author = {S. Mankefors and R. Torkar and A. Boklund},
  title = {New Quality Estimations in Random Testing},
  journal = {Proceedings of the 14th International Symposium on Software Reliability
    Engineering (ISSRE 2003)},
  year = {2003},
  volume = {00},
  pages = {468-478},
  abstract = {By reformulating the issue of random testing into an equivalent problem
    we are able to introduce a new kind of quality estimations based
    on Monte Carlo integration and the central limit theorem. This method
    also provides a limited but working "success theory" in the case
    of no detected failures. In an empirical evaluation using hundreds
    of billions of simulated tests we furthermore find a very good match
    between the quality estimations presented in this article and the
    true failure frequencies. Both simple modulus defects as well as
    seeded defects in two extensively employed numerical routines were
    subject to investigation in the empirical work.},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ISSRE.2003.1251067},
  issn = {1071-9458},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\New quality estimations in random testing.pdf},
  publisher = {IEEE Computer Society},
  read = {only beginning},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=1251067},
}

@TECHREPORT{mathur93comparing,
  author = {A. Mathur and W. Wong},
  title = {Comparing the fault detection effectiveness of mutation and data
    flow testing:


     An empirical study},
  institution = {Software Engineering Research Center, Purdue University},
  year = {1993},
  number = {SERC-TR-146-P},
  month = {March},
}

@INPROCEEDINGS{Mayer2005,
  author = {Johannes Mayer},
  title = {Lattice-Based Adaptive Random Testing},
  booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated
    Software Engineering (ASE 2005)},
  year = {2005},
  series = {ACM},
  pages = {333-336},
  publisher = {ACM Press, New York, NY, USA},
  abstract = {Adaptive Random Testing (ART) denotes a family of testing algorithms
    that have a better performance compared to pure random testing with
    respect to the number of test cases necessary to detect the first
    failure. Many of these algorithms, however, are not very efficient
    regarding runtime. A new ART algorithm is presented that has a better
    performance than all other ART methods for the block failure pattern.
    Its runtime is linear in the number of test cases selected, which
    is nearly as efficient as pure random testing, as opposed to most
    other ART methods. This new ART algorithm selects the test cases
    based on a lattice.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Lattice-based adaptive random testing.pdf},
  read = {yes (not all)},
  relevance = {5},
  timestamp = {2006.04.14},
}

@INPROCEEDINGS{Mayer2005a,
  author = {Johannes Mayer},
  title = {Adaptive Random Testing by Bisection with Restriction},
  booktitle = {Proceedings of the Seventh International Conference on Formal Engineering
    Methods (ICFEM 2005)},
  year = {2005},
  series = {LNCS 3785},
  pages = {251-263},
  publisher = {Springer-Verlag, Berlin},
  abstract = {Random Testing is a strategy to select test cases based on pure randomness.
    Adaptive Random Testing (ART), a family of algorithms, improves
    pure Random Testing by taking common failure pattern into account.
    The bestin terms of the number of test cases necessary to detect
    the first failureART algorithms, however, are too runtime inefficient.
    Therefore, a modification of a fast, but not so good ART algorithm,
    namely ART by Bisection, is presented. This modification requires
    much less test cases than the original method while retaining its
    computational efficiency.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Adaptive Random Testing by Bisection with Restriction.pdf},
  read = {no},
  relevance = {5},
  timestamp = {2006.04.14},
}

@INPROCEEDINGS{Meinke2004,
  author = {Karl Meinke},
  title = {Automated black-box testing of functional correctness using function
    approximation},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
    on Software testing and analysis},
  year = {2004},
  pages = {143--153},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {We consider black-box testing of functional correctness as a special
    case of a satisfiability or constraint solving problem. We introduce
    a general method for solving this problem based on function approximation.
    We then describe some practical results obtained for an automated
    testing algorithm using approximation by piecewise polynomial functions.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Meinke04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007532},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
  pdf = {D:\Ilinca\Docs\Related work\Automated blckbox testing of functional correctness using function approximation.pdf},
  read = {yes},
  relevance = {4},
  url = {http://portal.acm.org/citation.cfm?id=1007532},
}

@ARTICLE{Menzies2000,
  author = {Tim Menzies and Bojan Cukic},
  title = {When to Test Less},
  journal = {IEEE Software},
  year = {2000},
  volume = {17},
  pages = {107-112},
  number = {5},
  month = {September - October},
  abstract = {Small-scale software projects usually cant afford to implement time-consuming
    and expensive tests. However, the authors show that in a surprisingly
    large number of cases, a small number of randomly selected tests
    will adequately probe the software.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\When to test less.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2006.04.10},
}

@INPROCEEDINGS{Meyer2005,
  author = {Bertrand Meyer},
  title = {Attached Types and their Application to Three Open Problems of Object-Oriented
    Programming},
  booktitle = {Proc. ECOOP},
  year = {2005},
  pages = {1-32},
  note = {Springer LNCS 3586},
}

@BOOK{Meyer1997,
  title = {Object-Oriented Software Construction, 2nd edition},
  publisher = {Prentice Hall},
  year = {1997},
  author = {B. Meyer},
  owner = {Ilinca},
  timestamp = {2007.01.23},
}

@INPROCEEDINGS{Meyer2007,
  author = {Bertrand Meyer and Ilinca Ciupa and Andreas Leitner and Lisa (Ling)
    Liu},
  title = {Automatic Testing of Object-Oriented Software},
  booktitle = {Proceedings of SOFSEM 2007 (Current Trends in Theory and Practice
    of Computer Science)},
  year = {2007},
  editor = {Jan van Leeuwen},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer-Verlag},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\papers\sofsem07\sofsem07.pdf},
  timestamp = {2007.02.08},
}

@ARTICLE{Miller1990,
  author = {Barton P. Miller and Louis Fredriksen and Bryan So},
  title = {An empirical study of the reliability of UNIX utilities},
  journal = {Commun. ACM},
  year = {1990},
  volume = {33},
  pages = {32--44},
  number = {12},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/96267.96279},
  issn = {0001-0782},
  publisher = {ACM Press},
}

@BOOK{Myers1979,
  title = {The Art of Software Testing},
  publisher = {John Wiley \& Sons},
  year = {1979},
  author = {Glenford J. Myers},
  owner = {Ilinca},
  read = {no},
  relevance = {5},
  timestamp = {2006.04.09},
}

@ARTICLE{ntafos:ctrlFlowTest:88,
  author = {D. Ntafos},
  title = {{A Comparison of Some Structural Testing Strategies}},
  journal = {IEEE TSE},
  year = {1988},
  volume = {14},
  pages = {868-874},
  number = {6},
  month = {June},
}

@INPROCEEDINGS{Ntafos1998,
  author = {Simeon Ntafos},
  title = {On random and partition testing},
  booktitle = {Proc. Intl. Symp. on Software Testing and Analysis},
  year = {1998},
  pages = {42--48},
  abstract = {There have been many comparisons of random and partition testing.
    Proportional partition testing has been suggested as the optimum
    way to perform partition testing. In this paper we show that this
    might not be so and discuss some of the problems with previous studies.
    We look at the expected cost of failures as a way to evaluate the
    effectiveness of testing strategies and use it to compare random
    testing, uniform partition testing and proportional partition testing.
    Also, we introduce partition testing strategies that try to take
    the cost of failures into account and present some results on their
    effectiveness.},
  doi = {http://doi.acm.org/10.1145/271771.271785},
  isbn = {0-89791-971-8},
  location = {Clearwater Beach, Florida, United States},
  pdf = {D:\Ilinca\Docs\Related work\on_random_and_partition_testing.pdf},
  read = {no},
  relevance = {5},
}

@INPROCEEDINGS{ntafos:coverageEffectiveness:84,
  author = {S. Ntafos},
  title = {An evaluation of required element testing strategies},
  booktitle = {Proc. ICSE},
  year = {1984},
  pages = {250--256},
}

@INPROCEEDINGS{Offutt1996,
  author = {A. Jefferson Offutt and J. Huffman Hayes},
  title = {A semantic model of program faults},
  booktitle = {ISSTA '96: Proceedings of the 1996 ACM SIGSOFT international symposium
    on Software testing and analysis},
  year = {1996},
  pages = {195--200},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Program faults are artifacts that are widely studied, but there are
    many aspects of faults that we still do not understand. In addition
    to the simple fact that one important goal during testing is to
    cause failures and thereby detect faults, a full understanding of
    the characteristics of faults is crucial to several research areas
    in testing. These include fault-based testing, testability, mutation
    testing, and the comparative evaluation of testing strategies. In
    this workshop paper, we explore the fundamental nature of faults
    by looking at the differences between a syntactic and semantic characterization
    of faults. We offer definitions of these characteristics and explore
    the differentiation. Specifically, we discuss the concept of "size"
    of program faults --- the measurement of size provides interesting
    and useful distinctions between the syntactic and semantic characterization
    of faults. We use the fault size observations to make several predictions
    about testing and present preliminary data that supports this model.
    We also use the model to offer explanations about several questions
    that have intrigued testing researchers.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Offutt96_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/229000.226317},
  isbn = {0-89791-787-1},
  location = {San Diego, California, United States},
  pdf = {D:\Ilinca\Docs\Related work\A semantic model of program faults.pdf},
  read = {yes},
  relevance = {5},
  url = {http://portal.acm.org/citation.cfm?id=226317#},
}

@ARTICLE{Offutt1999,
  author = {A. Jefferson Offutt and Zhenyi Jin and Jie Pan},
  title = {The dynamic domain reduction procedure for test data generation},
  journal = {Softw. Pract. Exper.},
  year = {1999},
  volume = {29},
  pages = {167--193},
  number = {2},
  abstract = {(Summary) Test data generation is one of the most technically challenging
    steps of testing software, but most commercial systems currently
    incorporate very little automation for this step. This paper presents
    results from a project that is trying to find ways to incorporate
    test data generation into practical test processes. The results
    include a new procedure for automatically generating test data that
    incorporates ideas from symbolic evaluation, constraint-based testing,
    and dynamic test data generation. It takes an initial set of values
    for each input, and dynamically pushes the values through the
    control-flow graph of the program, modifying the sets of values
    as branches in the program are taken. The result is usually a set
    of values for each input parameter that has the property that any
    choice from the sets will cause the path to be traversed. This procedure
    uses new analysis techniques, offers improvements over previous
    research results in constraint based testing, and combines several
    steps into one coherent process. The dynamic nature of this procedure
    yields several benefits. Moving through the control flow graph dynamically
    allows path constraints to be resolved immediately, which is more
    efficient both in space and time, and more often successful than
    constraint-based testing. This new procedure also incorporates an
    intelligent search technique based on bisection. The dynamic nature
    of this procedure also allows certain improvements to be made in
    the handling of arrays, loops, and expressions; language features
    that are traditionally difficult to handle in test data generation
    systems. The paper presents the test data generation procedure,
    examples to explain the working of the procedure, and results from
    a proof-of-concept implementation. Copyright 1999 John Wiley & Sons,
    Ltd.},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1002/(SICI)1097-024X(199902)29:2<167::AID-SPE225>3.3.CO;2-M},
  issn = {0038-0644},
  pdf = {D:\Ilinca\Docs\Related work\The dynamic domain reduction procedure for test data generation.pdf},
  publisher = {John Wiley \& Sons, Inc.},
  read = {no},
  relevance = {4},
  url = {http://portal.acm.org/citation.cfm?id=309101#},
}

@TECHREPORT{Oriat2004,
  author = {Catherine Oriat},
  title = {Jartege: a Tool for Random Generation of Unit Tests for Java Classes},
  institution = {CNRS, Universite Joseph Fourier Grenoble I},
  year = {2004},
  number = {RR-1069-I},
  month = {June},
  abstract = {This report presents Jartege, a tool which allows random generation
    of unit tests for Java classes specified in JML. JML (Java Modeling
    Language) is a specification language for Java which allows one
    to write invariants for classes, and pre- and postconditions for
    operations. As in the JML-JUnit tool, we use JML specifications
    on the one hand to eliminate irrelevant test cases, and on the other
    hand as a test oracle. Jartege randomly generates test cases, which
    consist of a sequence of constructor and method calls for the classes
    under test. The random aspect of the tool can be parameterized by
    associating weights to classes and operations, and by controlling
    the number of instances which are created for each class under test.
    The practical use of Jartege is illustrated by a small case study.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Oriat04_mycomments.doc},
  keywords = {Testing, unit testing, random generation of test cases, Java, JML},
  pdf = {D:\Ilinca\Docs\Related work\Jartege_A_tool_for_random_generation_of_unit_tests_for_Java_classes.pdf},
  read = {yes},
  relevance = {6},
}

@ARTICLE{Ostrand1988,
  author = {T. J. Ostrand and M. J. Balcer},
  title = {The category-partition method for specifying and generating fuctional
    tests},
  journal = {Commun. ACM},
  year = {1988},
  volume = {31},
  pages = {676--686},
  number = {6},
  abstract = {A method for creating functional test suites has been developed in
    which a test engineer analyzes the system specification, writes
    a series of formal test specifications, and then uses a generator
    tool to produce test descriptions from which test scripts are written.
    The advantages of this method are that the tester can easily modify
    the test specification when necessary, and can control the complexity
    and number of the tests by annotating the tests specification with
    constraints.},
  address = {New York, NY, USA},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Ostrand88_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/62959.62964},
  issn = {0001-0782},
  pdf = {D:\Ilinca\Docs\Related work\The category-partition method.pdf},
  publisher = {ACM Press},
  read = {yes},
  relevance = {5},
  url = {http://portal.acm.org/citation.cfm?id=62964},
}

@INPROCEEDINGS{Pacheco2005,
  author = {Carlos Pacheco and Michael D. Ernst},
  title = {Eclat: Automatic generation and classification of test inputs},
  booktitle = {Proc. ECOOP},
  year = {2005},
  pages = {504--527},
  abstract = {This paper describes a technique that selects, from a large set of
    test inputs, a small subset likely to reveal faults in the software
    under test. The technique takes a program or software component,
    plus a set of correct executions  say, from observations of the
    software running properly, or from an existing test suite that a
    user wishes to enhance. The technique first infers an operational
    model of the software's operation. Then, inputs whose operational
    pattern of execution differs from the model in specific ways are
    suggestive of faults. These inputs are further reduced by selecting
    only one input per operational pattern. The result is a small portion
    of the original inputs, deemed by the technique as most likely to
    reveal faults. Thus, the technique can also be seen as an error-detection
    technique. The paper describes two additional techniques that complement
    test input selection. One is a technique for automatically producing
    an oracle (a set of assertions) for a test input from the operational
    model, thus transforming the test input into a test case. The other
    is a classification-guided test input generation technique that
    also makes use of operational models and patterns. When generating
    inputs, it filters out code sequences that are unlikely to contribute
    to legal inputs, improving the efficiency of its search for fault-revealing
    inputs. We have implemented these techniques in the Eclat tool,
    which generates unit tests for Java classes. Eclat's input is a
    set of classes to test and an example program execution  say, a
    passing test suite. Eclat's output is a set of JUnit test cases,
    each containing a potentially fault-revealing input and a set of
    assertions at least one of which fails. In our experiments, Eclat
    successfully generated inputs that exposed fault-revealing behavior;
    we have used Eclat to reveal real errors in programs. The inputs
    it selects as fault-revealing are an order of magnitude as likely
    to reveal a fault as all generated inputs.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Pacheco05_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\Eclat Automatic Generation and Classification of Test Inputs.pdf},
  read = {yes},
  relevance = {4},
}

@INPROCEEDINGS{Pacheco2007,
  author = {C. Pacheco and S. K. Lahiri and M. D. Ernst and T. Ball},
  title = {Feedback-directed random test generation},
  booktitle = {Proc. ICSE},
  year = {2007},
  pages = {75--84},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Feedback-directed random test generation.pdf},
  relevance = {6},
  timestamp = {2007.09.12},
}

@ARTICLE{1317478,
  author = {J. Ploski and M. Rohr and P. Schwenkenberg and W. Hasselbring},
  title = {Research issues in software fault categorization},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2007},
  volume = {32},
  pages = {6},
  number = {6},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/1317471.1317478},
  issn = {0163-5948},
  publisher = {ACM},
}

@INPROCEEDINGS{pretschner:mbt:05,
  author = {A. Pretschner and W. Prenninger and S. Wagner and C. K\"{u}hnel and
    M. Baumgartner and B. Sostawa and R. Z\"{o}lch and T. Stauner},
  title = {One evaluation of model-based testing and its automation},
  booktitle = {Proc. ICSE},
  year = {2005},
  pages = {392--401},
}

@INPROCEEDINGS{Richardson1989,
  author = {D. Richardson and O. O'Malley and C. Tittle},
  title = {Approaches to specification-based testing},
  booktitle = {TAV3: Proceedings of the ACM SIGSOFT '89 third symposium on Software
    testing, analysis, and verification},
  year = {1989},
  pages = {86--96},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Current software testing practices focus, almost exclusively, on the
    implementation, despite widely acknowledged benefits of testing
    based on software specifications. We propose approaches to specification-based
    testing by extending a wide variety of implementation-based testing
    techniques to be applicable to formal specification languages. We
    demonstrate these approaches for the Anna and Larch specification
    languages.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Richardson89_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/75308.75319},
  isbn = {0-89791-342-6},
  location = {Key West, Florida, United States},
  pdf = {D:\Ilinca\Docs\Related work\Approaches to specification-based testing.pdf},
  read = {yes},
  relevance = {5},
  special_observations = {this is the original paper formalizing specification-based testing},
  url = {http://portal.acm.org/citation.cfm?id=75319},
}

@ARTICLE{Rosenblum1995,
  author = {David S. Rosenblum},
  title = {A Practical Approach to Programming With Assertions},
  journal = {IEEE TSE},
  year = {1995},
  volume = {21},
  pages = {19--31},
  number = {1},
  abstract = {Embedded assertions have been recognized as a potentially powerful
    tool for automatic runtime detection of software faults during debugging,
    testing, maintenance and even production versions of software systems.
    Yet despite the richness of the notations and the maturity of the
    techniques and tools that have been developed for programming with
    assertions, assertions are a development tool that has seen little
    widespread use in practice. The main reasons seem to be that (1)
    previous assertion processing tools did not integrate easily with
    existing programming environments, and (2) it is not well understood
    what kinds of assertions are most effective at detecting software
    faults. This paper describes experience using an assertion processing
    tool that was built to address the concerns of ease-of-use and effectiveness.
    The tool is called APP, an Annotation PreProcessor for C programs
    developed in UNIX-based development environments. APP has been used
    in the development of a variety of software systems over the past
    five years. Based on this experience, the paper presents a classification
    of the assertions that were most effective at detecting faults.
    While the assertions that are described guard against many common
    kinds of faults and errors, the very commonness of such faults demonstrates
    the need for an explicit, high-level, automatically checkable specification
    of required behavior. It is hoped that the classification presented
    in this paper will prove to be a useful first step in developing
    a method of programming with assertions.},
  address = {Piscataway, NJ, USA},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Rosenblum95_mycomments.doc},
  doi = {http://dx.doi.org/10.1109/32.341844},
  issn = {0098-5589},
  pdf = {D:\Ilinca\Docs\Related work\A practical approach to programming with assertions.pdf},
  publisher = {IEEE Press},
  read = {yes},
  relevance = {5},
  special_observations = {This paper received the ICSE most influential paper award in 2002.},
  url = {http://portal.acm.org/citation.cfm?id=203111&coll=GUIDE&dl=GUIDE&CFID=46749401&CFTOKEN=25541718},
}

@INPROCEEDINGS{Satpathy2007,
  author = {Manoranjan Satpathy and Michael Butler and Michael Leuschel and S.
    Ramesh},
  title = {Automatic Testing from Formal Specifications},
  booktitle = {Proceedings of the International Conference on Tests and Proofs (TAP)
    2007},
  year = {2007},
  volume = {LNCS},
  publisher = {Springer-Verlag},
  abstract = {In this article, we consider model oriented formal specifica-

     tion languages. We generate test cases by performing symbolic execution

     over a model, and from the test cases obtain a Java program. This
    Java

     program acts as a test driver and when it is run in conjunction with
    the

     implementation then testing is performed in an automatic manner.
    Our

     approach makes the testing cycle fully automatic. The main contribution

     of our work is that we perform automatic testing even when the models

     are non-deterministic.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\automatic_testing_from_formal_specifications.pdf},
  read = {yes},
  relevance = {4},
  timestamp = {2007.02.16},
}

@INPROCEEDINGS{Shukla2004,
  author = {R. Shukla and P. Strooper and D. Carrington},
  title = {A Framework for Reliability Assessment of Software Components},
  booktitle = {Component-Based Software Engineering: 7th International Symposium,
    CBSE 2004, Edinburgh, UK, May 24-25, 2004. Proceedings},
  year = {2004},
  editor = {Ivica Crnkovic, Judith A. Stafford, Heinz W. Schmidt, et al.},
  volume = {3054 / 2004},
  pages = {272--279},
  publisher = {Springer-Verlag GmbH},
  abstract = {This paper proposes a conceptual framework for the reliability assessment
    of software components that incorporates test case execution and
    output evaluation. Determining an operational profile and test output
    evaluation are two difficult and important problems that must be
    addressed in such a framework. Determining an operational profile
    is difficult, because it requires anticipating the future use of
    the component. An expected result is needed for each test case to
    evaluate the test result and a test oracle is used to generate these
    expected results. The framework combines statistical testing and
    test oracles implemented as self-checking versions of the implementations.
    The framework is illustrated using two examples that were chosen
    to identify the issues that must be addressed to provide tool support
    for the framework.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Shukla04_mycomments.doc},
  doi = {10.1007/b97813},
  pdf = {D:\Ilinca\Docs\Related work\A Framework for Reliability Assessment of Software Components.pdf},
  read = {yes},
  relevance = {4},
  url = {http://springerlink.metapress.com/app/home/contribution.asp?wasp=ff1c3b88abb7441aa86174e572d4c35e&referrer=parent&backto=searcharticlesresults,1,1;},
}

@ARTICLE{Tillmann2006,
  author = {Nikolai Tillmann and Wolfram Schulte},
  title = {Unit Tests Reloaded: Parameterized Unit Testing with Symbolic Execution},
  journal = {IEEE Software},
  year = {2006},
  volume = {23},
  pages = {38--47},
  number = {4},
  abstract = {Unit tests are becoming popular. Are there ways to automate the generation
    of good unit tests? Parameterized unit tests are unit tests that
    depend on inputs. PUTs describe behavior more concisely than traditional
    unit tests. We use symbolic execution techniques and constraint
    solving to find inputs for PUTs that achieve high code coverage,
    to turn existing unit tests into PUTs, and to generate entirely
    new PUTs that describe an existing implementation's behavior. Traditional
    testing benefits from these techniques because test inputs - including
    the behavior of entire classes - can often be generated automatically
    from compact PUTs.},
  address = {Los Alamitos, CA, USA},
  doi = {http://dx.doi.org/10.1109/MS.2006.117},
  issn = {0740-7459},
  pdf = {D:\Ilinca\Docs\Related work\not read yet\Unit_tests_reloaded_parameterized_unit_testing_with_symbolic_execution.pdf},
  publisher = {IEEE Computer Society Press},
  read = {no},
  relevance = {6},
  url = {http://portal.acm.org/citation.cfm?id=1159169.1159389&coll=&dl=acm&CFID=15151515&CFTOKEN=6184618#},
}

@INPROCEEDINGS{Tonella2004,
  author = {Paolo Tonella},
  title = {Evolutionary testing of classes},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
    on Software testing and analysis},
  year = {2004},
  pages = {119--128},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Object oriented programming promotes reuse of classes in multiple
    contexts. Thus, a class is designed and implemented with several
    usage scenarios in mind, some of which possibly open and generic.
    Correspondingly, the unit testing of classes cannot make too strict
    assumptions on the actual method invocation sequences, since these
    vary from application to application. In this paper, a genetic algorithm
    is exploited to automatically produce test cases for the unit testing
    of classes in a generic usage scenario. Test cases are described
    by chromosomes, which include information on which objects to create,
    which methods to invoke and which values to use as inputs. The proposed
    algorithm mutates them with the aim of maximizing a given coverage
    measure. The implementation of the algorithm and its application
    to classes from the Java standard library are described.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Tonella04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007528},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
  pdf = {D:\Ilinca\Docs\Related work\Evolutionary Testing of Classes.pdf},
  read = {yes},
  relevance = {6},
  url = {http://portal.acm.org/citation.cfm?id=1007528},
}

@TECHREPORT{Torshizi2006,
  author = {F. Torshizi and J. Ostroff},
  title = {ESpec -- a Tool for Agile Development via Early Testable Specifications},
  institution = {York University, Canada},
  year = {2006},
  number = {CS-2006-04},
  owner = {Ilinca},
  timestamp = {2008.01.21},
}

@TECHREPORT{upl:MBT:06,
  author = {M. Utting and A. Pretschner and B. Legeard},
  title = {A taxonomy of model-based testing},
  institution = {Department of Computer Science, The University of Waikato (New Zealand)},
  year = {2006},
  number = {04/2006},
  month = {April},
}

@TECHREPORT{Venolia2005,
  author = {Gina D. Venolia and Robert DeLine and Thomas LaToza},
  title = {Software Development at Microsoft Observed},
  institution = {Microsoft Research},
  year = {2005},
  number = {MSR-TR-2005-140},
  month = {October},
  abstract = {To understand Microsoft developers typical tools and work habits
    and their level of satisfaction with these, we performed two surveys
    and eleven interviews with developers across all business divisions.
    This report provides a summary of the resulting data. From the set
    of potential problems we gave them, the top three that Microsoft
    developers agree they have are: understanding the rationale behind
    a piece of code (66%); having to switch tasks often because of requests
    from teammates or managers (62%); and being aware of changes to
    code elsewhere that impact their own code (61%). The most notable
    take-away from the data is that developers go to great lengths to
    create and maintain rich mental models of code and dont rely on
    external representations. The mental nature of these models requires
    frequent, disruptive, face-to-face meetings to keep individuals
    models in sync, which greatly slow the rate at which a newcomer
    to a team can become productive. These interruptions also burden
    more senior development team members, as they have to recover what
    they were doing in the code following the interruption from team
    members.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Software_development_at_microsoft_observed.pdf},
  read = {no},
  relevance = {5},
  timestamp = {2006.11.10},
  url = {http://research.microsoft.com/research/pubs/view.aspx?type=Technical%20Report&id=994},
}

@INPROCEEDINGS{Visser2004,
  author = {Willem Visser and Corina S. Pasareanu and Sarfraz Khurshid},
  title = {{Test input generation with Java PathFinder}},
  booktitle = {Proc. Intl. Symp. on Software testing and analysis},
  year = {2004},
  pages = {97--107},
  abstract = {We show how model checking and symbolic execution can be used to generate
    test inputs to achieve structural coverage of code that manipulates
    complex data structures. We focus on obtaining branch-coverage during
    unit testing of some of the core methods of the red-black tree implementation
    in the Java TreeMap library, using the Java PathFinder model checker.
    Three different test generation techniques will be introduced and
    compared, namely, straight model checking of the code, model checking
    used in a black-box fashion to generate all inputs up to a fixed
    size, and lastly, model checking used during white-box test input
    generation. The main contribution of this work is to show how efficient
    white-box test input generation can be done for code manipulating
    complex data, taking into account complex method preconditions.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Visser04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007526},
  isbn = {1-58113-820-2},
  pdf = {D:\Ilinca\Docs\Related work\Test input generation with Java PathFinder.pdf},
  read = {yes},
  relevance = {6},
}

@INPROCEEDINGS{Werner2004,
  author = {Edith Werner and Helmut Neukirchen and Jens Grabowski},
  title = {{Self-adaptive Functional Testing of Services Working in Continuously
    Changing Contexts}},
  booktitle = {Proceedings of the 3rd Workshop on System Testing and Validation
    (SV04), December 2004, Fraunhofer Book Series},
  year = {2004},
  month = dec,
  abstract = {Web Services and ad-hoc networks are examples for systems that work
    in continuously changing contexts. The services provided by such
    systems can be tested in a laboratory environment using the standard
    conformance testing methodology. Testing the inter-working of services
    with other services in all contexts is an impossible task, because
    the possible usages of services may not be known or may increase
    due to the development of new services. Thus, errors may occur and
    have to be detected and handled at runtime. Our approach to this
    problem is self-adaptive functional testing, which is a combination
    of monitoring and active testing at runtime. In order to locate
    an error which has been detected during monitoring, existing test
    cases are adapted automatically to the specific situation and applied
    afterwards at runtime.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Werner04_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\Self_Adaptive_Functional_Testing_of_Services.pdf},
  read = {yes},
  relevance = {2},
  special_observations = {Met Edith Werner and Helmut Neukirchen in TAROT summer school in Paris
    (July 2005).},
}


@ARTICLE{weyukerJeng:RT:91,
  author = {E. Weyuker and B. Jeng},
  title = {{Analyzing Partition Testing Strategies}},
  journal = {IEEE TSE},
  year = {1991},
  volume = {17},
  pages = {703--711},
  number = {7},
}

@ARTICLE{267915,
  author = {M. Wood and M. Roper and A. Brooks and J. Miller},
  title = {Comparing and combining software defect detection techniques: a replicated
    empirical study},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {1997},
  volume = {22},
  pages = {262--277},
  number = {6},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/267896.267915},
  issn = {0163-5948},
  publisher = {ACM},
}

@INPROCEEDINGS{Xie2005,
  author = {Tao Xie and Darko Marinov and Wolfram Schulte and David Notkin},
  title = {{Symstra: A Framework for Generating Object-Oriented Unit Tests using
    Symbolic Execution}},
  booktitle = {Proc. Intl. Conf. on Tools and Algorithms for the Construction and
    Analysis of Systems},
  year = {2005},
  pages = {365--381},
  month = {April},
  abstract = {Object-oriented unit tests consist of sequences of method invocations.
    Behavior of an invocation depends on the methods arguments and
    the state of the receiver at the beginning of the invocation. Correspondingly,
    generating unit tests involves two tasks: generating method sequences
    that build relevant receiver object states and generating relevant
    method arguments. This paper proposes Symstra, a framework that
    achieves both test generation tasks using symbolic execution of
    method sequences with symbolic arguments. The paper defines symbolic
    states of object-oriented programs and novel comparisons of states.
    Given a set of methods from the class under test and a bound on
    the length of sequences, Symstra systematically explores the object-state
    space of the class and prunes this exploration based on the state
    comparisons. Experimental results show that Symstra generates unit
    tests that achieve higher branch coverage faster than the existing
    test-generation techniques based on concrete method arguments.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Xie05_mycomments.doc},
  location = {Edinburgh, UK},
  pdf = {D:\Ilinca\Docs\Related work\Symstra.pdf},
  read = {yes},
  relevance = {6},
}

@INPROCEEDINGS{Xie2005a,
  author = {Tao Xie and David Notkin},
  title = {Automatically Identifying Special and Common Unit Tests for Object-Oriented
    Programs},
  booktitle = {16th IEEE International Symposium on Software Reliability Engineering},
  year = {2005},
  pages = {277--287},
  month = nov,
  abstract = {Developers often create common tests and special tests, which exercise
    common behaviors and special behaviors of the class under test,
    respectively. Although manually created tests are valuable, developers
    often overlook some special or even common tests. We have developed
    a new approach for automatically identifying special and common
    unit tests for a class without requiring any specification. Given
    a class, we automatically generate test inputs and identify common
    and special tests among the generated tests. Developers can inspect
    these identified tests and use them to augment existing tests. Our
    approach is based on statistical algebraic abstractions, program
    properties (in the form of algebraic specifications) dynamically
    inferred based on a set of predefined abstraction templates. We
    use statistical algebraic abstractions to characterize program behaviors
    and identify special and common tests. Our initial experience has
    shown that a relatively small number of common and special tests
    can be identified among a large number of generated tests and these
    identified tests expose common and special behaviors that deserve
    developers attention.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Automatically identifying special and common unit tests for oo programs.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://www-static.cc.gatech.edu/~csallnch/jcrasher/uses/xie05automatically-abstract.html},
}

@INPROCEEDINGS{Xu2004,
  author = {Guoqing Xu and Zongyuang Yang},
  title = {JMLAutoTest: A Novel Automated Testing Framework Based on JML and
    JUnit},
  booktitle = {Formal Approaches to Software Testing: Third International Workshop,
    FATES 2003, Montreal, Quebec, Canada, October 6th, 2003. Revised
    Papers},
  year = {2004},
  editor = {Alexandre Petrenko, Andreas Ulrich},
  volume = {2931 / 2004},
  pages = {70--85},
  publisher = {Springer-Verlag GmbH},
  abstract = {Writing specifications using Java Modeling Language has been accepted
    for a long time as a practical approach to increasing the correctness
    and quality of Java programs. However, the current JML testing system
    (the JML and JUnit framework) can only generate skeletons of test
    fixture and test case class. Writing codes for generating test cases,
    especially those with a complicated data structure is still a labor-intensive
    job in the test for programs annotated with JML specifications.
    This paper presents JMLAutoTest, a novel framework for automated
    testing of Java programs annotated with JML specifications. Firstly,
    given a method, three test classes (a skeleton of test client class,
    a JUnit test class and a test case class) can be generated. Secondly,
    JMLAutoTest can generate all nonisomorphic test cases that satisfy
    the requirements defined in the test client class. Thirdly, JMLAutoTest
    can avoid most meaningless cases by running the test in a double-phase
    way which saves much time of exploring meaningless cases in the
    test. This method can be adopted in the testing not only for Java
    programs, but also for programs written with other languages. Finally,
    JMLAutoTest executes the method and uses JML runtime assertion checker
    to decide whether its post-condition is violated. That is whether
    the method works correctly.},
  doi = {10.1007/b95400},
  pdf = {D:\Ilinca\Docs\Related work\JMLAutoTest.pdf},
  read = {yes},
  relevance = {4},
  url = {http://springerlink.metapress.com/app/home/contribution.asp?wasp=f3901fa4fa5e4a22a1bbd3b089ae8436&referrer=parent&backto=searcharticlesresults,1,1;},
}

@ARTICLE{zeller:deltaDebugging:02,
  author = {A. Zeller and R. Hildebrandt},
  title = {Simplifying and isolating failure-inducing input},
  journal = {IEEE TSE},
  year = {2002},
  volume = {SE-28},
  pages = {183--202},
  number = {2},
  month = {February},
}

@INPROCEEDINGS{Zhang1995,
  author = {Kaizhong Zhang and Jason Tsong-Li Wang and Dennis Shasha},
  title = {On the Editing Distance between Undirected Acyclic Graphs and Related
    Problems},
  booktitle = {Proceedings of the 6th Annual Symposium on Combinatorial Pattern
    Matching},
  year = {1995},
  pages = {395-407},
  publisher = {Springer Verlag, Berlin},
  owner = {Ilinca},
  timestamp = {2007.04.26},
}

@MISC{AsmLTest,
  title = {Parameter generation in the AsmL Test Generator tool},
  howpublished = {on web page},
  comment = {- They use ADF (Access Driven Filtering) to generate parameters. -
    The user has to write a IsInterestingParameter () predicate (it
    should also be the invariant, I think) - User has to do a lot of
    configuration},
  read = {yes},
  relevance = {5},
}

@MISC{EiffelBase,
  title = {The EiffelBase Library. Eiffel Software Inc. http://www.eiffel.com/},
  owner = {Ilinca},
  timestamp = {2007.01.24},
  url = {www.eiffel.com},
}

@MISC{Gobo,
  title = {The Gobo Eiffel Library. http://www.gobosoft.com/},
  howpublished = {web site},
  owner = {Ilinca},
  timestamp = {2007.01.24},
  url = {www.gobosoft.com},
}

@MISC{Jtest,
  title = {{Jtest}. {Parasoft Corporation}. http://www.parasoft.com/},
  owner = {Ilinca},
  timestamp = {2007.01.22},
  url = {http://www.parasoft.com/jsp/products/home.jsp?product=Jtest},
}

@ARTICLE{392549,
  title = {{IEEE standard classification for software anomalies}},
  journal = {{IEEE Std 1044-1993}},
  year = {2 Jun 1994},
}


@article{267590,
 author = {H. Zhu and P. Hall and J. May},
 title = {Software unit test coverage and adequacy},
 journal = {ACM Comput. Surv.},
 volume = {29},
 number = {4},
 year = {1997},
 issn = {0360-0300},
 pages = {366--427},
 doi = {http://doi.acm.org/10.1145/267580.267590},
 publisher = {ACM},
 address = {New York, NY, USA},
 }

@INPROCEEDINGS{Morasca2004,
  author = {Sandro Morasca and Stefano Serra-Capizzano},
  title = {On the analytical comparison of testing techniques},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
	on Software testing and analysis},
  year = {2004},
  pages = {154--164},
  address = {New York, NY, USA},
  publisher = {ACM},
  doi = {http://doi.acm.org/10.1145/1007512.1007533},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
}
@inproceedings{ma:interclassmut:02,
author={{Y.-S.} Ma and {Y.-R.} Kwon and J. Offutt},
title={{Inter-Class Mutation Operators for Java}},
booktitle={Proc. ISSRE},
pages={352-366},
year={2002}
}


@INPROCEEDINGS{Bousquet2004,
  author = {Lydie du Bousquet and Yves Ledru and Oliver Maury and Catherine Oriat
    and Jean-Louis Lanet},
  title = {Case Study in JML-Based Software Validation.},
  booktitle = {ASE},
  year = {2004},
  pages = {294-297},
  abstract = {This paper reports on a testing case study applied to a small Java
    application, partially specified in JML. It illustrates that JML
    can easily be integrated with classical testing tools based on combinatorial
    techniques and random generation. It also reveals difficulties to
    reuse, in a testing context, JML annotations written for a proof
    process.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/duBousquet04_mycomments.doc},
  crossref = {DBLP:conf/kbse/2004},
  ee = {http://csdl.computer.org/comp/proceedings/ase/2004/2131/00/21310294abs.htm},
  pdf = {D:\Ilinca\Docs\Related work\Case Study in JML-Based Software Validation.pdf},
  read = {Yes},
  relevance = {5},
  special_observations = {Met Yves Ledru at TAROT summer school 2005 in Paris},
  url = {http://www-lsr.imag.fr/Les.Personnes/Yves.Ledru/PublicationsYL.html},
}

@ARTICLE{Goodenough1975,
  author = {John B. Goodenough and Susan L. Gerhart},
  title = {Toward a theory of test data selection},
  journal = {IEEE TSE},
  year = {1975},
  volume = {1(2)},
  pages = {156--173},
  abstract = {This paper examines the theoretical and

     practical role of testing in software development.

     We prove a fundamental theorem showing that

     properly structured tests are capable of demonstrating

     the absence of errors in a program. The

     theorem's proof hinges on our definition of test

     reliability and validity , but its practical utility

     hinges on being able to show when a test is actually

     reliable . We explain what makes tests

     unreliable (for example, we show by example

     why testing all program statements, predicates,

     or paths is not usually sufficient to insure test

     reliability) , and we outline a possible approach

     to developing reliable tests . We also show how

     the analysis required to define reliable tests can

     help in checking a program's design and specifications

     as well as in preventing and detecting

     implementation errors.},
  crossref = {testing, proofs of correctness},
  owner = {Ilinca},
  read = {yes (not all)},
  relevance = {5},
  timestamp = {2006.04.05},
}




@INPROCEEDINGS{Offutt1999a,
  author = {A. Jefferson Offutt and Yiwei Xiong and Shaoying Liu},
  title = {Criteria for Generating Specification-Based Tests},
  booktitle = {Proceedings of the Fifth IEEE International Conference on Engineering
    of Complex Computer Systems (ICECCS '99)},
  year = {1999},
  pages = {119-131},
  month = {October},
  abstract = {This paper presents general criteria for generating test inputs from
    state-based specifications. Software testing can only be formalized
    and quantified when a solid basis for test generation can be defined.
    Formal specifications of complex systems represent a significant
    opportunity for testing because they precisely describe what functions
    the software is supposed to provide in a form that can easily be
    manipulated. These techniques provide coverage criteria that are
    based on the specifications, and are made up of several parts, including
    test prefixes that contain inputs necessary to put the software
    into the appropriate state for the test values. The test generation
    process includes several steps for transforming specifications to
    tests. Empirical results from a comparative case study application
    of these criteria are presented.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Offutt99_mycomments.doc},
  crossref = {DBLP:conf/iceccs/1999},
  ee = {http://csdl.computer.org/comp/proceedings/iceccs/1999/0434/00/04340119abs.htm},
  pdf = {D:\Ilinca\Docs\Related work\Criteria for Generating Specification-based Tests.pdf},
  read = {yes},
  relevance = {5},
}

@INPROCEEDINGS{Petrenko2001,
  author = {Alexandre Petrenko},
  title = {Specification Based Testing: Towards Practice.},
  booktitle = {Ershov Memorial Conference},
  year = {2001},
  pages = {287-300},
  abstract = {Specification based testing facilities are gradually becoming software
    production aids. The paper shortly considers the current state of
    the art, original ISPRAS/RedVerst experience, and outlines the ways
    for further research and testing tool development. Both conceptual
    and technical problems of novel specification based testing technologies
    introduction are considered.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Petrenko01_mycomments.doc},
  crossref = {DBLP:conf/ershov/2001},
  ee = {http://link.springer.de/link/service/series/0558/bibs/2244/22440287.htm},
  pdf = {D:\Ilinca\Docs\Related work\SpecificationBasedTesting_TowardsPractice.pdf},
  read = {yes},
  relevance = {3},
  url = {http://www.springerlink.com/app/home/contribution.asp?wasp=d9cd04e55a35405ca80a5450d1b8feb6&referrer=parent&backto=issue,29,52;journal,1260,2055;linkingpublicationresults,1:105633,1},
}

@INPROCEEDINGS{dAmorim2006,
  author = {Marcelo d'Amorim and Carlos Pacheco and Darko Marinov and Tao Xie
    and Michael D. Ernst},
  title = {An empirical comparison of automated generation and classification
    techniques for object-oriented unit testing},
  booktitle = {Proc. ASE},
  year = {2006},
  pages = {59--68},
  abstract = {Testing involves two major activities: generating test inputs and
    determining whether they reveal faults. Automated test generation
    techniques include random generation and symbolic execution. Automated
    test classification techniques include ones based on uncaught exceptions
    and violations of operational models inferred from manually provided
    tests. Previous research on unit testing for object-oriented programs
    developed three pairs of these techniques: model-based random testing,
    exception-based random testing, and exception-based symbolic testing.
    We develop a novel pair, model-based symbolic testing. We also empirically
    compare all four pairs of these generation and classification techniques.
    The results show that the pairs are complementary (i.e., reveal
    faults differently), with their respective strengths and weaknesses.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\an_empirical_comparison_of_automated_generation_and_classification_techniques_for_OO_unit_testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.11.10},
}

@BOOK{581795,
  title = {Bug Patterns in Java},
  publisher = {APress L. P.},
  year = {2002},
  author = {E. Allen},
  isbn = {1590590619},
}

@TECHREPORT{Andrews2006,
  author = {J. H. Andrews and S. Haldar and Y. Lei and C. H. Li},
  title = {Randomized Unit Testing: Tool Support and Best Practices},
  institution = {Department of Computer Science, University of Western Ontario},
  year = {2006},
  number = {663},
  month = {January},
  abstract = {Randomization has long been used in testing, but it has not achieved
    widespread acceptance due to a lack of tool support and a failure
    to establish recognized best practices. In this paper, we describe
    RUTE-J, a Java package intended to provide tool support for randomized
    Java unit testing. We also discuss the best practices we have identified
    in our research on randomized unit testing. We report on case studies
    and an experiment in which we applied RUTE-J to various public-domain
    Java classes, finding failures even in mature software and supporting
    the claim that RUTE-J is an efficient, effective tool for unit testing.
    Finally, we compare the use of randomized unit testing to the use
    of other tools such as model checkers, and discuss the tradeoffs.
    We conclude that when best practices are followed, randomized unit
    testing with tool support is useful both as a preparation for full
    software model checking, and in its own right.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Randomized unit testing - tool support and best practices.pdf},
  read = {browsed through},
  relevance = {5},
  timestamp = {2006.04.09},
  url = {http://www.csd.uwo.ca/faculty/andrews/papers/},
}

@INPROCEEDINGS{Andrews2006a,
  author = {James H. Andrews and Susmita Haldar and Yong Lei and Felix Chun Hang
    Li},
  title = {Tool support for randomized unit testing},
  booktitle = {Proc. 1st Intl. Workshop on Random testing},
  year = {2006},
  pages = {36--45},
  abstract = {There are several problem areas that must be addressed when applying

     randomization to unit testing. As yet no general, fully automated

     solution that works for all units has been proposed. We

     therefore have developed RUTE-J, a Java package intended to help

     programmers do randomized unit testing in Java. In this paper, we

     describe RUTE-J and illustrate how it supports the development

     of per-unit solutions for the problems of randomized unit testing.

     We report on an experiment in which we applied RUTE-J to the

     standard Java TreeMap class, measuring the efficiency and effectiveness

     of the technique. We also illustrate the use of randomized

     testing in experimentation, by adapting RUTE-J so that it generates

     randomized minimal covering test suites, and measuring the

     effectiveness of the test suites generated.},
  doi = {http://doi.acm.org/10.1145/1145735.1145741},
  isbn = {1-59593-457-X},
  location = {Portland, Maine},
  pdf = {D:\Ilinca\Docs\Related work\tool_support_for_randomized_unit_testing.pdf},
  read = {no},
  relevance = {6},
}

@ARTICLE{Artho2003,
  author = {C. Artho and H. Barringer and A. Goldberg and K. Havelund

     and S. Khurshid and M. Lowry and C. Pasareanu and G. Ro\c{s}u and
    K. Sen

     and W. Visser and R. Washington},
  title = {Combining Test Case Generation with Run-time Verification},
  journal = {ASM issue of Theoretical Computer Science},
  year = {2003},
  note = {To appear},
  abstract = {Software testing is typically an ad-hoc process where human testers
    manually write test

     inputs and descriptions of expected test results, perhaps automating
    their execution in a

     regression suite. This process is cumbersome and costly. This paper
    reports results on a

     framework to further automate this process. The framework consists
    of combining automated

     test case generation based on systematically exploring the input
    domain of the program

     with runtime verification, where execution traces are monitored and
    verified against

     properties expressed in temporal logic. The input domain of the program
    is explored using

     a model checker extended with symbolic execution. Properties are
    formulated in an expressive

     temporal logic. A methodology is advocated that generates properties
    specific to each

     input instance rather than formulating properties uniformly true
    for all inputs. Capabilities

     for analyzis of concurrency errors are planned to be integrated with
    temporal logic monitoring.

     The paper describes an application of the technology to a planetary
    rover controller.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Artho03_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\Combining test case generation and runtime verification.pdf},
  publisher = {Elsevier},
  read = {yes (but not entirely)},
  relevance = {5},
  url = {http://research.nii.ac.jp/~cartho/papers/asm-tcs.pdf},
}

@INPROCEEDINGS{Artzi2006,
  author = {S. Artzi and M. Ernst and A, Kie{\.z}un and C. Pacheco and J. Perkins},
  title = {Finding the needles in the haystack: Generating legal test inputs
    for object-oriented programs},
  booktitle = {Proc. 1st Workshop on Model-Based Testing and Object-Oriented Systems},
  year = {2006},
  abstract = {A test input for an object-oriented program typically consists of
    a sequence of method calls that use the API defined by the program
    under test. Generating legal test inputs can be challenging because,
    for some programs, the set of legal method sequences is much smaller
    than the set of all possible sequences; without a formal specification
    of legal sequences, an input generator is bound to produce mostly
    illegal sequences.


     We propose a scalable technique that combines dynamic analysis with
    random testing to help an input generator create legal test inputs
    without a formal specification, even for programs in which most
    sequences are illegal. The technique uses an example execution of
    the program to infer a model of legal call sequences, and uses the
    model to guide a random input generator towards legal but behaviorally-diverse
    sequences.


     We have implemented our technique for Java, in a tool called Palulu,
    and evaluated its effectiveness in creating legal inputs for real
    programs. Our experimental results indicate that the technique is
    effective and scalable. Our preliminary evaluation indicates that
    the technique can quickly generate legal sequences for complex inputs:
    in a case study, Palulu created legal test inputs in seconds for
    a set of complex classes, for which it took an expert thirty minutes
    to generate a single legal input.},
  pdf = {D:\Ilinca\Docs\Related work\Generating_legal_test_inputs_for_OO_programs.pdf},
  relevance = {6},
}

@INPROCEEDINGS{1297897,
  author = {N. Ayewah and W. Pugh and D. Morgenthaler and J. Penix and Y. Zhou},
  title = {Using FindBugs on production software},
  booktitle = {OOPSLA companion},
  year = {2007},
  pages = {805--806},
  doi = {http://doi.acm.org/10.1145/1297846.1297897},
  isbn = {978-1-59593-865-7},
  location = {Montreal, Quebec, Canada},
}

@INPROCEEDINGS{Balcer1989,
  author = {M. Balcer and W. Hasling and T. Ostrand},
  title = {Automatic generation of test scripts from formal test specifications},
  booktitle = {TAV3: Proceedings of the ACM SIGSOFT '89 third symposium on Software
    testing, analysis, and verification},
  year = {1989},
  pages = {210--218},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {TSL is a language for writing formal test specifications of the functions
    of a software system. The test specifications are compiled into
    executable test scripts that establish test environments, assign
    values to input variables, perform necessary setup and cleanup operations,
    run the test cases, and check the correctness of test results. TSL
    is a working system that has been used to test commercial software
    in a production environment.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Balcer89_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/75308.75332},
  isbn = {0-89791-342-6},
  location = {Key West, Florida, United States},
  pdf = {D:\Ilinca\Docs\Related work\Automatic generation of test scripts from formal test specifications.pdf},
  read = {yes},
  relevance = {3},
  special_observations = {This paper is very similar to Ostrand88: it builds on the same ideas
    and doesn't bring any significant new information.},
}

@INPROCEEDINGS{Barnett2004,
  author = {Mike Barnett and K. Rustan M. Leino and Wolfram Schulte},
  title = {The Spec\# programming system: An overview},
  booktitle = {Proc. CASSIS},
  year = {2004},
  note = {Springer LNCS 3362},
  timestamp = {2008.02.01},
}

@ARTICLE{41117,
  author = {V. Basili and R. Selby},
  title = {Comparing the Effectiveness of Software Testing Strategies},
  journal = {IEEE TSE},
  year = {1987},
  volume = {13},
  pages = {1278--1296},
  number = {12},
  address = {Piscataway, NJ, USA},
  doi = {http://dx.doi.org/10.1109/TSE.1987.232881},
  issn = {0098-5589},
  publisher = {IEEE Press},
}

@BOOK{533100,
  title = {Software Testing Techniques},
  publisher = {John Wiley \& Sons, Inc.},
  year = {1990},
  author = {B. Beizer},
  address = {New York, NY, USA},
  isbn = {0442245920},
}

@ARTICLE{legeard:MBT-GSM:04,
  author = {E. Bernard and B. Legeard and X. Luck and F. Peureux},
  title = {{Generation of test sequences from formal specifications: GSM 11-11
    standard case study}},
  journal = {Software-Practice \& Experience},
  year = {2004},
  volume = {34},
  pages = {915--948},
  number = {10},
  month = {August},
}

@INPROCEEDINGS{Beyer2004,
  author = {Dirk Beyer and Adam J. Chlipala and Thomas A. Henzinger and Ranjit
    Jhala and Rupak Majumdar},
  title = {Generating Tests from Counterexamples},
  booktitle = {Proceedings of the 26th International Conference on Software Engineering
    (ICSE'04, Edinburgh)},
  year = {2004},
  pages = {326-335},
  publisher = {IEEE Computer Society Press},
  abstract = {We have extended the software model checker Blast to automatically
    generate test suites that guarantee full coverage with respect to
    a given predicate. More precisely, given a C program and a target
    predicate p, Blast determines the set L of program locations which
    program execution can reach with p true, and automatically generates
    a set of test vectors that exhibit the truth of p at all locations
    in L. We have used Blast to generate test suites and to detect dead
    code in C programs with up to 30K lines of code. The analysis and
    test-vector generation is fully automatic (no user intervention)
    and exact (no false positives).},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Beyer04_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\generating_tests_from_counterexamples.pdf},
  read = {yes},
  relevance = {5},
  url = {sherry.ifi.unizh.ch/beyer04generating.html},
}

@INPROCEEDINGS{Boshernitsan2006,
  author = {Marat Boshernitsan and Roongko Doong and Alberto Savoia},
  title = {From {Daikon} to {Agitator}: lessons and challenges in building a
    commercial tool for developer testing},
  booktitle = {ISSTA '06: Proceedings of the 2006 International Symposium on Software
    Testing and Analysis},
  year = {2006},
  pages = {169--180},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/1146238.1146258},
  isbn = {1-59593-263-1},
  location = {Portland, Maine, USA},
  pdf = {D:\Ilinca\Docs\Related work\From_Daikon_to_Agitator.pdf},
}

@INPROCEEDINGS{Boyapati2002,
  author = {Chandrasekhar Boyapati and Sarfraz Khurshid and Darko Marinov},
  title = {Korat: automated testing based on {Java} predicates},
  booktitle = {Proc. Intl. Symp. on Software Testing and Analysis},
  year = {2002},
  pages = {123-133},
  abstract = {This paper presents Korat, a novel framework for automated testing
    of Java programs. Given a formal specification for a method, Korat
    uses the method precondition to automatically generate all (nonisomorphic)
    test cases up to a given small size. Korat then executes the method
    on each test case, and uses the method postcondition as a test oracle
    to check the correctness of each output. To generate test cases
    for a method, Korat constructs a Java predicate (i.e., a method
    that returns a boolean) from the method's pre-condition. The heart
    of Korat is a technique for automatic test case generation: given
    a predicate and a bound on the size of its inputs, Korat generates
    all (nonisomorphic) inputs for which the predicate returns true.
    Korat exhaustively explores the bounded input space of the predicate
    but does so efficiently by monitoring the predicate's executions
    and pruning large portions of the search space. This paper illustrates
    the use of Korat for testing several data structures, including
    some from the Java Collections Framework. The experimental results
    show that it is feasible to generate test cases from Java predicates,
    even when the search space for inputs is very large. This paper
    also compares Korat with a testing framework based on declarative
    specifications. Contrary to our initial expectation, the experiments
    show that Korat generates test cases much faster than the declarative
    framework.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Boyapati02_mycomments.doc},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Korat - Automated Testing Based on Java Predicates.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.08.24},
}

@INPROCEEDINGS{Briand2002,
  author = {L. C. Briand and Y. Labiche and H. Sun},
  title = {Investigating the use of analysis contracts to support fault isolation
    in object oriented code},
  booktitle = {ISSTA '02: Proceedings of the 2002 ACM SIGSOFT international symposium
    on Software testing and analysis},
  year = {2002},
  pages = {70--80},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {A number of activities involved in testing software are known to be
    difficult and time consuming. Among them is the isolation of faults
    once failures have been detected. In this paper, we investigate
    how the instrumentation of contracts could address this issue. Contracts
    are known to be a useful technique to specify the precondition and
    postcondition of operations and class invariants, thus making the
    definition of object-oriented analysis or design elements more precise.
    Our aim in this paper is to reuse and instrument contracts to ease
    testing. A thorough case study is run where we define contracts,
    instrument them using a commercial tool, and assess the benefits
    and limitations of doing so to support the isolation of faults.
    We then draw practical conclusions regarding the applicability of
    the approach and its limitations.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Briand02_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/566172.566183},
  isbn = {1-58113-562-9},
  location = {Roma, Italy},
  pdf = {D:\Ilinca\Docs\Related work\Investigating the use of analysis contracts to support fault isolation in object oriented code.pdf},
  read = {yes},
  relevance = {6},
  url = {http://portal.acm.org/citation.cfm?doid=566172.566183},
}

@INPROCEEDINGS{Ceballos2005,
  author = {Rafael Ceballos and Rafael Martinez Gasca and Diana Borrego},
  title = {Constraint satisfaction techniques for diagnosing errors in design
    by contract software},
  booktitle = {SAVCBS '05: Proceedings of the 2005 conference on Specification and
    verification of component-based systems},
  year = {2005},
  pages = {11},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Design by Contract enables the development of more reliable and robust
    software applications. In this paper, a methodology that diagnoses
    errors in software is proposed. This is based on the combination
    of Design by Contract, Model-based Diagnosis and Constraint Programming.
    Contracts are specified by using assertions. These assertions together
    with an abstraction of the source code are transformed into constraints.
    The methodology detects if the contracts are consistent, and if
    there are incompatibilities between contracts and source code. The
    process is automatic and is based on constraint programming.},
  comments = {Not very understandable and does not go into any detail},
  doi = {http://doi.acm.org/10.1145/1123058.1123070},
  isbn = {1-59593-371-9},
  location = {Lisbon, Portugal},
  read = {yes},
  relevance = {5},
}

@INPROCEEDINGS{Chalin2006,
  author = {Patrice Chalin},
  title = {Are Practitioners Writing Contracts?},
  booktitle = {Springer LNCS 4157},
  year = {2006},
  pages = {100-113},
  abstract = {For decades now, modular design methodologies have helped software
    engineers cope with the size and complexity of modern-day industrial
    applications. To be truly effective though, it is essential that
    module interfaces be rigorously specified. Design by Contract (DBC)
    is an increasingly popular method of interface specification for
    object-oriented systems. Many researchers are actively adding support
    for DBC to various languages such as Ada, Java and C#. Are these
    research efforts justified? Does having support for DBC mean that
    developers will make use of it? We present the results of an empirical
    study measuring the proportion of assertion statements used in Eiffel
    contracts. The study results indicate that programmers using Eiffel
    (the only active language with built-in support for DBC) tend to
    write assertions in a proportion that is higher than for other languages.

     Keywords: design by contract, program assertions, empirical study,
    Eiffel.},
  doi = {10.1007/11916246},
  owner = {Ilinca},
  timestamp = {2007.10.11},
}

@ARTICLE{Chan1996,
  author = {F. T. Chan and T. Y. Chen and I. K. Mak and Y. T. Yu},
  title = {Proportional sampling strategy: guidelines for software testing practitioners},
  journal = {Information and Software Technology},
  year = {1996},
  volume = {38},
  pages = {775--782},
  number = {12},
  abstract = {Recently, several sufficient conditions have been developed that guarantee
    partition testing to have a higher probability of detecting at least
    one failure than random testing. One of these conditions is that
    the number of test cases selected from each partition is proportional
    to the size of the partition. We call such a method of allocating
    test cases the proportional sampling strategy. Although this condition
    is not the most general one, it is the most easily and practically
    applicable one. In this paper, we discuss how the proportional sampling
    strategy can be applied effectively in practice. Some practical
    issues that need to be attended are identified and guidelines to
    deal with these issues are suggested.},
  citeseerurl = {http://citeseer.ist.psu.edu/context/1179915/0},
  keywords = {Partition testing; Random testing; Selection of test data},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Proportional sampling strategy.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://www.ingentaconnect.com/content/els/09505849/1996/00000038/00000012/art01103;jsessionid=11wv0vigm8pia.alice},
}

@INPROCEEDINGS{Chan2002,
  author = {Kwok Ping Chan and Tsong Yueh Chen and Dave Towey},
  title = {Restricted Random Testing},
  booktitle = {Proceedings of the 7th International Conference on Software Quality},
  year = {2002},
  pages = {321 - 330},
  publisher = {Springer-Verlag, London, UK},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Restricted random testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.04.11},
}

@INPROCEEDINGS{Chen2004a,
  author = {T.Y. Chen and R. Merkel and P.K. Wong and G. Eddy},
  title = {Adaptive random testing through dynamic partitioning},
  booktitle = {Proceedings of the Fourth International Conference on Quality Software},
  year = {2004},
  volume = {00},
  pages = {79 - 86},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society},
  abstract = {Adaptive random testing (ART) describes a family of algorithms for
    generating random test cases that have been experimentally demonstrated
    to have greater fault-detection capacity than simple random testing.
    We outline and demonstrate two new ART algorithms, and demonstrate
    experimentally that they offer similar performance advantages, with
    considerably lower overhead than other ART algorithms.},
  doi = {http://doi.ieeecomputersociety.org/10.1109/QSIC.2004.1357947},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Adaptive Random Testing through dynamic partitioning.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.04.11},
  url = {http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=1357947},
}

@INPROCEEDINGS{Chen2003,
  author = {T. Y. Chen and F. C. Kuo and R. G. Merkel and S. P. Ng},
  title = {Mirror adaptive random testing},
  booktitle = {Proceedings of the Third International Conference on Quality Software},
  year = {2003},
  pages = {4 - 11},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society},
  doi = {10.1109/QSIC.2003.1319079},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Mirror adaptive random testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.04.11},
}

@INPROCEEDINGS{Chen2004,
  author = {T. Y. Chen and H. Leung and I. K. Mak},
  title = {Adaptive Random Testing},
  booktitle = {Proc. Asian Computing Science Conference},
  year = {2004},
  editor = {Michael J. Maher},
  abstract = {In this paper, we introduce an enhanced form of random testing called
    Adaptive Random Testing. Adaptive random testing seeks to distribute
    test cases more evenly within the input space. It is based on the
    intuition that for non-point types of failure patterns, an even
    spread of test cases is more likely to detect failures using fewer
    test cases than ordinary random testing. Experiments are performed
    using published programs. Results show that adaptive random testing
    does outperform ordinary random testing significantly (by up to
    as much as 50%) for the set of programs under study. These results
    are very encouraging, providing evidences that our intuition is
    likely to be useful in improving the effectiveness of random testing.},
  doi = {10.1007/b103476},
  pdf = {D:\Ilinca\Docs\Related work\Adaptive Random Testing.pdf},
  read = {yes},
  relevance = {6},
}

@INPROCEEDINGS{Chen2005,
  author = {Tsong Yueh Chen and Robert Merkel},
  title = {Quasi-random testing},
  booktitle = {ASE '05: Proceedings of the 20th IEEE/ACM international Conference
    on Automated software engineering},
  year = {2005},
  pages = {309--312},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Quasi-random sequences, also known as low-discrepancy or low-dispersion
    sequences, are sequences of points in an n-dimensional unit hypercube.
    These sequences have the property that points are spread more evenly
    throughout the cube than random point sequences, which result in
    regions where there are clusters of points and others that are sparsely
    populated. Based on the observation that program faults tend to
    lead to contiguous failure regions within a program's input domain,
    and that an even spread of random tests enhances the failure detection
    effectiveness for certain failure patterns, we examine the use of
    these sequences as a replacement for random sequences in automated
    testing.The limited number of quasi-random sequences available from
    the standard algorithms poses significant practical problems for
    use when testing real programs, and especially for evaluating its
    effectiveness. We examine the use of randomised quasi-random sequences,
    which are permuted in a nondeterministic fashion but still retain
    their low discrepancy properties, to overcome this problem, and
    show that testing using randomised quasi-random sequences is often
    significantly more effective than random testing.},
  doi = {http://doi.acm.org/10.1145/1101908.1101957},
  isbn = {1-59593-993-4},
  location = {Long Beach, CA, USA},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Quasi-random testing.pdf},
  read = {yes},
  relevance = {6},
  timestamp = {2006.05.19},
}

@ARTICLE{Chen1994,
  author = {T. Y. Chen and Y. T. Yu},
  title = {On the Relationship Between Partition and Random Testing},
  journal = {IEEE TSE},
  year = {1994},
  volume = {20},
  pages = {977--980},
  number = {12},
  abstract = {Weyuker and Jeng have investigated the conditions that affect the
    performance of partition testing and have compared analytically
    the fault-detecting ability of partition testing and random testing.
    This paper extends and generalizes some of their results. We give
    more general ways of characterizing the worst case for partition
    testing, along with a precise characterization of when this worst
    case is as good as random testing. We also find that partition testing
    is guaranteed to perform at least as well as random testing so long
    as the number of test cases selected is in proportion to the size
    of the subdomains.},
  address = {Piscataway, NJ, USA},
  doi = {http://dx.doi.org/10.1109/32.368132},
  issn = {0098-5589},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\On the relationship between partition and random testing.pdf},
  publisher = {IEEE Press},
  read = {yes},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://portal.acm.org/citation.cfm?id=203108&dl=GUIDE&coll=GUIDE#},
}

@ARTICLE{177364,
  author = {R. Chillarege and I.S. Bhandari and J.K. Chaar and M.J. Halliday
    and D.S. Moebus and B.K. Ray and M.-Y. Wong},
  title = {Orthogonal Defect Classification-A Concept for In-Process Measurements},
  journal = {IEEE TSE},
  year = {1992},
  volume = {18},
  pages = {943-956},
  number = {11},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/32.177364},
  issn = {0098-5589},
  publisher = {IEEE Computer Society},
}

@INPROCEEDINGS{Ciupa2007,
  author = {I. Ciupa and A. Leitner and M. Oriol and B. Meyer},
  title = {Experimental Assessment of Random Testing for Object-Oriented Software},
  booktitle = {Proc. ISSTA},
  year = {2007},
  pages = {84--94},
  owner = {Ilinca},
  timestamp = {2007.04.26},
}

@INPROCEEDINGS{Ciupa2006,
  author = {Ilinca Ciupa and Andreas Leitner and Manuel Oriol and Bertrand Meyer},
  title = {Object distance and its application to adaptive random testing of
    object-oriented programs},
  booktitle = {RT '06: Proceedings of the 1st international workshop on Random testing},
  year = {2006},
  pages = {55--63},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  doi = {http://doi.acm.org/10.1145/1145735.1145744},
  isbn = {1-59593-457-X},
  location = {Portland, Maine},
}

@INPROCEEDINGS{Ciupa2005,
  author = {I. Ciupa and A. Leitner.},
  title = {Automatic Testing Based on Design by Contract},
  booktitle = {Proceedings of Net.ObjectDays 2005 (6th Annual International Conference
    on Object-Oriented and Internet-based Technologies, Concepts, and
    Applications for a Networked World)},
  year = {2005},
  pages = {545-557},
  month = {September 19-22},
  pdf = {D:\Ilinca\Docs\papers\SOQUA_05\Automatic_Testing_Based_on_Design_by_Contract.pdf},
}


@INPROCEEDINGS{Ciupa2008-classes,
  author = {Ilinca Ciupa and Bertrand Meyer and Manuel Oriol and Alexander Pretschner},
  title = {{Finding Faults:
Manual Testing vs. Random Testing+ vs. User Reports}},
  year = {2008},
note={Submitted to ISSRE}
}



@INPROCEEDINGS{Ciupa2008,
  author = {Ilinca Ciupa and Alexander Pretschner and Andreas Leitner and Manuel
    Oriol and Bertrand Meyer},
  title = {On the Predictability of Random Tests for Object-Oriented Software},
  booktitle = {Proceedings of the First International Conference on Software Testing,
    Verification and Validation (ICST'08)},
  year = {2008},
  pages={72--81},
  month = {April}
}


@ARTICLE{Claessen2000,
  author = {Koen Claessen and John Hughes},
  title = {{QuickCheck}: a lightweight tool for random testing of {Haskell}
    programs},
  journal = {ACM SIGPLAN Notices},
  year = {2000},
  volume = {35},
  pages = {268--279},
  number = {9},
  pdf = {D:\Ilinca\Docs\Related work\QuickCheck.pdf},
  read = {no},
  relevance = {4},
}

@INPROCEEDINGS{Cousot2000,
  author = {P. Cousot and R. Cousot},
  title = {Abstract Interpretation Based Program Testing},
  booktitle = {Proceedings of the SSGRR 2000 Computer \& eBusiness International
    Conference},
  year = {2000},
  address = {Compact disk paper 248 and electronic proceedings \url{http://www.ssgrr.it/en/ssgrr2000/proceedings.htm},
    L'Aquila, Italy},
  month = {July 31 -- August 6},
  publisher = {Scuola Superiore G{.} Reiss Romoli},
  abstract = {Every one can daily experiment that programs are bugged. Software
    bugs can have severe if not catastrophic consequences in computer-based
    safety critical applications. This impels the development of formal
    methods, whether manual, computer-assisted or automatic, for verifying
    that a program satisfies a specification. Among the automatic formal
    methods, program static analysis can be used to check for the absence
    of run-time errors. In this case the specification is provided by
    the semantics of the programming language in which the program is
    written. Abstract interpretation provides a formal theory for approximating
    this semantics, which leads to completely automated tools where
    run-time bugs can be statically and safely classified as unreachable,
    certain, impossible or potential. We discuss the extension of these
    techniques to abstract testing where specifications are provided
    by the programmers. Abstract testing is compared to program debugging
    and model-checking.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Cousot00_mycomments.doc},
  isbn = {88-85280-52-8},
  pdf = {D:\Ilinca\Docs\Related work\Abstract interpretation-based program testing.pdf},
  read = {yes},
  relevance = {5},
  special_observations = {Abstract interpretation was formalized by Patrick Cousot (with this
    paper?)},
  url = {http://www.di.ens.fr/~cousot/COUSOTpapers/SSGRRP-00-PC-RC.shtml},
}

@INPROCEEDINGS{Csallner2006,
  author = {Christoph Csallner and Yannis Smaragdakis},
  title = {{DSD-Crasher: A Hybrid Analysis Tool for Bug Finding}},
  booktitle = {Proc. Intl. Symp. on Software Testing and Analysis},
  year = {2006},
  pages = {245--254},
  month = jul,
  abstract = {DSD-Crasher is a bug finding tool that follows a three-step approach
    to program analysis: D. Capture the program's intended execution
    behavior with dynamic invariant detection. The derived invariants
    exclude many unwanted values from the program's input domain. S.
    Statically analyze the program within the restricted input domain
    to explore many paths. D. Automatically generate test cases that
    focus on verifying the results of the static analysis. Thereby confirmed
    results are never false positives, as opposed to the high false
    positive rate inherent in conservative static analysis. This three-step
    approach yields benefits compared to past two-step combinations
    in the literature. In our evaluation with third-party applications,
    we demonstrate higher precision over tools that lack a dynamic step
    and higher efficiency over tools that lack a static step.},
  pdf = {D:\Ilinca\Docs\Related work\dsd_crasher.pdf},
}

@INPROCEEDINGS{Csallner2005,
  author = {Christoph Csallner and Yannis Smaragdakis},
  title = {CnC: Combining Static Checking and Testing},
  booktitle = {International Conference on Software Engineering (ICSE)},
  year = {2005},
  pages = {422--431},
  month = may,
}

@ARTICLE{Csallner2004,
  author = {Christoph Csallner and Yannis Smaragdakis},
  title = {{JCrasher}: an automatic robustness tester for {Java}},
  journal = {Software: Practice and Experience},
  year = {2004},
  volume = {34},
  pages = {1025--1050},
  number = {11},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1002/spe.602},
  issn = {0038-0644},
  pdf = {D:\Ilinca\Docs\Related work\JCrasher_An_automatic_robustness_tester_for_Java.pdf},
  publisher = {John Wiley \& Sons, Inc.},
}

@INPROCEEDINGS{Demers1995,
  author = {F.-N. Demers and J. Malenfant},
  title = {Reflection in logic, functional and object-oriented programming:
    a short comparative study},
  booktitle = {Proc. IJCAI'95, Workshop on Reflection and Metalevel Architectures
    and their Applications in AI},
  year = {1995},
  pages = {29-38},
  abstract = {Reflection is a wide-ranging concept that has been studied independently
    in many different areas of science in general, and computer science
    in particular. Even in the sub-area of programming languages, it
    has been applied to different paradigms, especially the logic, functional
    and objectoriented ones. Partly because of different past influences,
    but also because researchers in these communities scarcely talk
    to each others, concepts have evolved separately, sometimes to the
    point where it is...},
  citeseerurl = {http://citeseer.ist.psu.edu/106401.html},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\reflection-in-logic-functional.pdf},
  read = {yes},
  timestamp = {2006.02.20},
  url = {http://citeseer.ist.psu.edu/106401.html},
}

@INPROCEEDINGS{leveson:heteMCDC:00,
  author = {A. Dupuy and N. Leveson},
  title = {{An empirical evaluation of the MC/DC coverage criterion on theHETE-2
    satellite software}},
  booktitle = {Proc. 19th Digital Avionics Systems Conferences},
  year = {2000},
  pages = {1B6/1--1B6/7},
}


@ARTICLE{duran:random:84,
  author = {J. Duran and S. Ntafos},
  title = {{An Evaluation of Random Testing}},
  journal = {IEEE TSE},
  year = {1984},
  volume = {SE-10},
  pages = {438--444},
  number = {4},
  month = {July},
}

@PHDTHESIS{Ernst2000,
  author = {Michael D. Ernst},
  title = {Dynamically Discovering Likely Program Invariants},
  school = {University of Washington Department of Computer Science and Engineering},
  year = {2000},
  month = {August},
  owner = {Ilinca},
  relevance = {6},
  timestamp = {2008.04.16},
}

@INPROCEEDINGS{838485,
  author = {E. Farchi and Y. Nir and S. Ur},
  title = {Concurrent Bug Patterns and How to Test Them},
  booktitle = {Proc 17th Intl. Symp. on Parallel and Distributed Processing},
  year = {2003},
  pages = {286.2},
  isbn = {0-7695-1926-1},
}

@INPROCEEDINGS{Forrester2000,
  author = {J. E. Forrester and B. P. Miller},
  title = {{An empirical study of the robustness of Windows NT applications
    using random testing}},
  booktitle = {4th USENIX Windows Systems Symposium},
  year = {2000},
  address = {Seattle},
  month = {August},
  abstract = {We report on the third in a series of studies on the reliability of
    application programs in the face of random input. In 1990 and 1995,
    we studied the reliability of UNIX application programs, both command
    line and X-Window based (GUI). In this study, we apply our testing
    techniques to applications running on the Windows NT operating system.
    Our testing is simple black-box random input testing; by any measure,
    it is a crude technique, but it seems to be effective at locating
    bugs in real programs.


     We tested over 30 GUI-based applications by subjecting them to two
    kinds of random input: (1) streams of valid keyboard and mouse events
    and (2) streams of random Win32 messages. We have built a tool that
    helps automate the testing of Windows NT applications. With a few
    simple parameters, any application can be tested.


     Using our random testing techniques, our previous UNIX-based studies
    showed that we could crash a wide variety of command-line and X-window
    based applications on several UNIX platforms. The test results are
    similar for NT-based applications. When subjected to random valid
    input that could be produced by using the mouse and keyboard, we
    crashed 21% of applications that we tested and hung an additional
    24% of applications. When subjected to raw random Win32 messages,
    we crashed or hung all the applications that we tested. We report
    which applications failed under which tests, and provide some analysis
    of the failures.},
  owner = {Ilinca},
  timestamp = {2007.09.20},
}

@INPROCEEDINGS{frankl:condcov:98,
  author = {P. Frankl and O. Iakounenko},
  title = {{Further Empirical Studies of Test Effectiveness}},
  booktitle = {Proc. FSE},
  year = {1998},
  pages = {153--162},
}

@ARTICLE{frankl:expCoverage:93,
  author = {P. Frankl and S. Weiss},
  title = {{An Experimental Comparison of the Effectiveness of Branch Testing
    and Data Flow Testing}},
  journal = {IEEE TSE},
  year = {1993},
  volume = {19},
  pages = {774--787},
  number = {8},
}

@MISC{frankl94alluses,
  author = {P. Frankl and S. Weiss and C. Hu},
  title = {All-uses versus mutation testing: An experimental comparison of effectiveness},
  year = {1994},
  text = {P. G. Frankl, S. N. Weiss, and C. Hu. All-uses versus mutation testing:


     An experimental comparison of effectiveness. Technical report PUCS-100-94,


     Department of Computer Science, Polytechnic University, Brooklyn,
    NY, February


     1994. Submitted for publication.},
  url = {citeseer.ist.psu.edu/frankl96alluses.html},
}

@ARTICLE{franklWeyuker:dataFlowTest:88,
  author = {P. Frankl and E. Weyuker},
  title = {{An Applicable Family of Data Flow Testing Criteria}},
  journal = {IEEE TSE},
  year = {1998},
  volume = {14},
  pages = {1483-1498},
  number = {10},
}

@INPROCEEDINGS{Gall2007,
  author = {Pascale Le Gall and Nicolas Rapin and Assia Touil},
  title = {Symbolic execution techniques for refinement testing},
  booktitle = {Proceedings of the International Conference on Tests and Proofs (TAP)
    2007},
  year = {2007},
  volume = {LNCS},
  publisher = {Springer-Verlag},
  abstract = {We propose an approach to test whether an abstract specification

     is refined or not by a more concrete one. The specifications are

     input / output symbolic transition systems (IOSTS). The refinement
    relation

     requires that all traces of the abstract system are also traces of
    the

     concrete system, up to some signature inclusion. Our work takes inspiration

     from the conformance testing area. Symbolic execution techniques

     allow us to select traces of the abstract system and to submit them
    on

     the concrete specification. Each trace execution leads to a verdict
    Fail,

     Pass or Warning. The verdict Pass is provided with a formula which

     has to be verified by the values only manipulated at the level of
    the

     concrete specification in order to ensure the refinement relation.
    The

     verdict Warning reports that the concrete specification has not been

     sufficiently explored to give a reliable verdict. This is thus a
    partial verification

     process, related to the quality of the set of selected traces and

     of the exploration of the concrete specification. Our approach has
    been

     implemented and is demonstrated on a simple example.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\symbolic_execution_techniques_for_refinement_testing.pdf},
  read = {yes},
  relevance = {4},
  review = {They propose a method for testing if a specification refines another
    one. They target IOSTS (inout-output symbolic transition systems).},
  timestamp = {2007.02.16},
}

@INPROCEEDINGS{girgis:coverageeffectiveness:86,
  author = {M. Girgis and M. Woodward},
  title = {An experimental comparison of the error exposing ability of program
    testing criteria},
  booktitle = {Proc. IEEE/ACM workshop on software testing},
  year = {1986},
  pages = {64--73},
  month = {July},
}

@INPROCEEDINGS{Godefroid2005,
  author = {Patrice Godefroid and Nils Klarlund and Koushik Sen},
  title = {{DART: directed automated random testing}},
  booktitle = {{Proc. ACM SIGPLAN Conference on Programming Language Design and
    Implementation}},
  year = {2005},
  pages = {213--223},
  abstract = {We present a new tool, named DART, for automatically testing software
    that combines three main techniques: (1) automated extraction of
    the interface of a program with its external environment using static
    source-code parsing; (2) automatic generation of a test driver for
    this interface that performs random testing to simulate the most
    general environment the program can operate in; and (3) dynamic
    analysis of how the program behaves under random testing and automatic
    generation of new test inputs to direct systematically the execution
    along alternative program paths. Together, these three techniques
    constitute Directed Automated Random Testing, or DART for short.
    The main strength of DART is thus that testing can be performed
    completely automatically on any program that compiles -- there is
    no need to write any test driver or harness code. During testing,
    DART detects standard errors such as program crashes, assertion
    violations, and non-termination. Preliminary experiments to unit
    test several examples of C programs are very encouraging.},
  doi = {http://doi.acm.org/10.1145/1065010.1065036},
  isbn = {1-59593-056-6},
  location = {Chicago, IL, USA},
  pdf = {D:\Ilinca\Docs\Related work\dart.pdf},
  read = {no},
  relevance = {5},
}

@INPROCEEDINGS{gray86why,
  author = {J. Gray},
  title = {Why Do Computers Stop and What Can Be Done About It?},
  booktitle = {Symposium on Reliability in Distributed Software and Database Systems},
  year = {1986},
  pages = {3-12},
}

@INPROCEEDINGS{Guo2006,
  author = {Philip J. Guo and Jeff H. Perkins and Stephen McCamant and Michael
    D. Ernst},
  title = {Dynamic inference of abstract types},
  booktitle = {ISSTA '06: Proceedings of the 2006 International Symposium on Software
    Testing and Analysis},
  year = {2006},
  pages = {255--265},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {An abstract type groups variables that are used for related purposes

     in a program. We describe a dynamic unification-based analysis

     for inferring abstract types. Initially, each run-time value gets
    a

     unique abstract type. A run-time interaction among values indicates

     that they have the same abstract type, so their abstract types

     are unified. Also at run time, abstract types for variables are accumulated

     from abstract types for values. The notion of interaction

     may be customized, permitting the analysis to compute finer

     or coarser abstract types; these different notions of abstract type

     are useful for different tasks. We have implemented the analysis

     for compiled x86 binaries and for Java bytecodes. Our experiments

     indicate that the inferred abstract types are useful for program
    comprehension,

     improve both the results and the run time of a follow-on

     program analysis, and are more precise than the output of a comparable

     static analysis, without suffering from overfitting.},
  doi = {http://doi.acm.org/10.1145/1146238.1146268},
  isbn = {1-59593-263-1},
  location = {Portland, Maine, USA},
  pdf = {D:\Ilinca\Docs\Related work\dynamic_inference_of_abstract_types.pdf},
}


@ARTICLE{gutjahr:random:99,
  author = {W. Gutjahr},
  title = {{Partition testing versus random testing: the influence of uncertainty}},
  journal = {IEEE TSE},
  year = {1999},
  volume = {25},
  pages = {661--674},
  number = {5},
}

@INPROCEEDINGS{Hamlet2006,
  author = {Dick Hamlet},
  title = {When only random testing will do},
  booktitle = {Proc. 1st Intl. Workshop on Random Testing},
  year = {2006},
  pages = {1--9},
  doi = {http://doi.acm.org/10.1145/1145735.1145737},
  isbn = {1-59593-457-X},
}

@ARTICLE{hamlet:random:90,
  author = {D. Hamlet and R. Taylor},
  title = {{Partition Testing Does Not Inspire Confidence}},
  journal = {IEEE TSE},
  year = {1990},
  volume = {16},
  pages = {1402--1411},
  number = {12},
  month = dec,
}

@INCOLLECTION{Hamlet1994,
  author = {R. Hamlet},
  title = {Random testing},
  booktitle = {Encyclopedia of Software Engineering},
  publisher = {Wiley},
  year = {1994},
  editor = {J. Marciniak},
  pages = {970--978},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Random Testing.pdf},
  read = {no},
  relevance = {6},
  timestamp = {2006.04.14},
}

@INPROCEEDINGS{Hartman2004,
  author = {A. Hartman and K. Nagin},
  title = {The AGEDIS tools for model based testing},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
    on Software testing and analysis},
  year = {2004},
  pages = {129--132},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {We describe the tools and interfaces created by the AGEDIS project,
    a European Commission sponsored project for the creation of a methodology
    and tools for automated model driven test generation and execution
    for distributed systems. The project includes an integrated environment
    for modeling, test generation, test execution, and other test related
    activities. The tools support a model based testing methodology
    that features a large degree of automation and also includes a feedback
    loop integrating coverage and defect analysis tools with the test
    generator and execution framework. Prototypes of the tools have
    been tried in industrial settings providing important feedback for
    the creation of the next generation of tools in this area.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Hartman04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007529},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
  pdf = {D:\Ilinca\Docs\Related work\The AGEDIS tools for model based testing.pdf},
  read = {yes},
  relevance = {4},
  url = {http://portal.acm.org/citation.cfm?doid=1007529#},
}

@INPROCEEDINGS{heimdahlgeorge:testsuitereduction:04,
  author = {M. Heimdahl and D. George},
  title = {{Test Suite Reduction for Model Based Tests: Effects on Test Quality
    and Implications for testing}},
  booktitle = {Proc. ASE},
  year = {2004},
  pages = {176--185},
}

@INPROCEEDINGS{heimdahl:coverageMC:04,
  author = {M. Heimdahl and D. George and R. Weber},
  title = {{Specification Test Coverage Adequacy Criteria = Specification Test
    Generation Inadequacy Criteria?}},
  booktitle = {Proc. 8th IEEE High Assurance in Systems Engineering Workshop},
  year = {2004},
  month = {February},
}

@INPROCEEDINGS{1021569,
  author = {K. Henningsson and C. Wohlin},
  title = {Assuring Fault Classification Agreement -- An Empirical Evaluation},
  booktitle = {Proc. Intl. Symp. on Empirical Software Engineering},
  year = {2004},
  pages = {95--104},
  doi = {http://dx.doi.org/10.1109/ISESE.2004.13},
  isbn = {0-7695-2165-7},
}

@ARTICLE{1052895,
  author = {D. Hovemeyer and W. Pugh},
  title = {Finding bugs is easy},
  journal = {SIGPLAN Not.},
  year = {2004},
  volume = {39},
  pages = {92--106},
  number = {12},
  doi = {http://doi.acm.org/10.1145/1052883.1052895},
  issn = {0362-1340},
}

@INPROCEEDINGS{257766,
  author = {M. Hutchins and H. Foster and T. Goradia and T. Ostrand},
  title = {Experiments of the effectiveness of dataflow- and controlflow-based
    test adequacy criteria},
  booktitle = {ICSE '94: Proceedings of the 16th international conference on Software
    engineering},
  year = {1994},
  pages = {191--200},
  address = {Los Alamitos, CA, USA},
  publisher = {IEEE Computer Society Press},
  isbn = {0-8186-5855-X},
  location = {Sorrento, Italy},
}

@INPROCEEDINGS{hutchins:structCovEffective:94,
  author = {M. Hutchins and H. Foster and T. Goradia and T. Ostrand},
  title = {Experiments of the effectiveness of dataflow- and controlflow-based
    test adequacy criteria},
  booktitle = {Proc. ICSE},
  year = {1994},
  pages = {191--200},
}

@ARTICLE{296778,
  author = {M. Hutchins and H. Foster Goradia and T. Ostrand},
  title = {Experiments on the effectiveness of dataflow- and control-flow-based
    test adequacy criteria},
  journal = {Software Engineering, 1994. Proceedings. ICSE-16., 16th International
    Conference on},
  year = {16-21 May 1994},
  pages = {191-200},
  doi = {10.1109/ICSE.1994.296778},
  keywords = {program testing, software engineeringcontrol-flow-based test adequacy
    criteria, coverage criteria, dataflow-based test adequacy criteria,
    faulty program versions, test sets},
}

@ARTICLE{Jezequel2001,
  author = {Jean-Marc Jezequel and Daniel Deveaux and Yves Le Traon},
  title = {Reliable Objects: a Lightweight Approach Applied to Java},
  journal = {IEEE Software},
  year = {2001},
  volume = {18(4)},
  pages = {76--83},
  month = {July/August},
  abstract = {Small scale software developments need specific low cost and low overhead
    methods and tools to deliver quality products within tight time
    and budget constraints. This is particularly true of testing, because
    of its cost and impact on final product reliability. We propose
    a lightweight approach to embed tests into components, making them
    self testable. We also propose a method to evaluate testing efficiency,
    based on mutation techniques, which ultimately provides an estimation
    of a component's quality. This allows the software developer to
    consciously trade reliability for resources. Our methodology has
    been implemented in the Eiffel, Java, C++ and Perl languages. The
    Java implementation, built on top of iContract, is outlined here.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Jezequel01_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\reliable objects - a lightweight approach applied to Java.pdf},
  read = {yes},
  relevance = {6},
}

@ARTICLE{1022544,
  author = {N. Juristo and A. M. Moreno and S. Vegas},
  title = {Towards building a solid empirical body of knowledge in testing techniques},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2004},
  volume = {29},
  pages = {1--4},
  number = {5},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/1022494.1022544},
  issn = {0163-5948},
  publisher = {ACM},
}

@INPROCEEDINGS{651507,
  author = {E. Kamsties and C. Lott},
  title = {An Empirical Evaluation of Three Defect-Detection Techniques},
  booktitle = {Proc. ESEC},
  year = {1995},
  pages = {362--383},
  isbn = {3-540-60406-5},
}

@INPROCEEDINGS{Khurshid2003,
  author = {Sarfraz Khurshid and Corina S. Pasareanu and Willem Visser},
  title = {Generalized Symbolic Execution for Model Checking and Testing.},
  booktitle = {Proc. Intl. Conf. on Tools and Algorithms for the Construction and
    Analysis of Systems},
  year = {2003},
  volume = {Springer LNCS 2619},
  pages = {553-568},
  abstract = {Modern software systems, which often are concurrent and manipulate
    complex data structures must be extremely reliable. We present a
    novel framework based on symbolic execution, for automated checking
    of such systems. We provide a two-fold generalization of traditional
    symbolic execution based approaches. First, we define a source to
    source translation to instrument a program, which enables standard
    model checkers to perform symbolic execution of the program. Second,
    we give a novel symbolic execution algorithm that handles dynamically
    allocated structures (e.g., lists and trees), method preconditions
    (e.g., acyclicity), data (e.g., integers and strings) and concurrency.
    The program instrumentation enables a model checker to automatically
    explore different program heap configurations and manipulate logical
    formulae on program data (using a decision procedure). We illustrate
    two applications of our framework: checking correctness of multi-threaded
    programs that take inputs from unbounded domains with complex structure
    and generation of non-isomorphic test inputs that satisfy a testing
    criterion. Our implementation for Java uses the Java PathFinder
    model checker.},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Khurshid03_mycomments.doc},
  ee = {http://link.springer.de/link/service/series/0558/bibs/2619/26190553.htm},
  pdf = {D:\Ilinca\Docs\Related work\Generalized symbolic execution for model checking and testing.pdf},
  read = {yes},
  relevance = {5},
}

@ARTICLE{66416,
  author = {D. E. Knuth},
  title = {The errors of TEX},
  journal = {Softw. Pract. Exper.},
  year = {1989},
  volume = {19},
  pages = {607--685},
  number = {7},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1002/spe.4380190702},
  issn = {0038-0644},
  publisher = {John Wiley \& Sons, Inc.},
}

@ARTICLE{Le2006,
  author = {Yves {Le Traon} and Benoit Baudry and {Jean-Marc} J\'ez\'equel},
  title = {{Design by Contract to Improve Software Vigilance}},
  journal = {IEEE TSE},
  year = {2006},
  volume = {32},
  pages = {571--586},
  number = {8},
}

@MISC{Leavens2005,
  author = {Gary T. Leavens and Yoonsik Cheon},
  title = {Design by Contract with JML},
  howpublished = {on web page},
  month = {January},
  year = {2005},
  note = {documentation on JML, not a conference paper},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Leavens05_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\jmldbc.pdf},
  read = {yes},
  relevance = {5},
  url = {www.cs.caltech.edu/~cs141/resources/JML/docs/jmltutorial/jmldbc.pdf},
}

@MISC{Leitner2005-2007,
  author = {A. Leitner and I. Ciupa},
  title = {{AutoTest}},
  howpublished = {\url{se.inf.ethz.ch/people/leitner/auto_test/}},
  year = {2005 - 2007},
  owner = {Ilinca},
  timestamp = {2006.04.14},
}

@INPROCEEDINGS{Leitner2007,
  author = {A. Leitner and I. Ciupa and B. Meyer and M. Howard},
  title = {Reconciling Manual and Automated Testing: the AutoTest Experience},
  booktitle = {Proc. HICCS},
  year = {2007},
}

@INPROCEEDINGS{Leitner2007b,
  author = {Andreas Leitner and Patrick Eugster and Manuel Oriol and Ilinca Ciupa},
  title = {Reflecting on an Existing Programming Language},
  booktitle = {Proceedings of TOOLS EUROPE 2007 - Objects, Models, Components, Patterns,},
  year = {2007},
  month = {July},
}

@INPROCEEDINGS{leitner:AutoTestMinimization:07,
  author = {A. Leitner and M. Oriol and I. Ciupa and A. Zeller and B. Meyer},
  title = {{Efficient Unit Test Case Minimization}},
  booktitle = {Proc. ASE},
  year = {2007},
}

@INPROCEEDINGS{Leitner2007a,
  author = {A. Leitner and M. Oriol and A. Zeller and I. Ciupa and B. Meyer},
  title = {Efficient Unit Test Case Minimization},
  booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated
    Software Engineering (ASE 2007)},
  year = {2007},
  month = {November},
}

@ARTICLE{Levenshtein1965,
  author = {Vladimir I. Levenshtein},
  title = {Binary codes capable of correcting deletions, insertions, and reversals},
  journal = {Doklady Akademii Nauk SSSR},
  year = {1965},
  volume = {163},
  pages = {845-848},
  number = {4},
  owner = {Ilinca},
  timestamp = {2006.04.20},
}

@INPROCEEDINGS{Liu2007,
  author = {Lisa (Ling) Liu and Bertrand Meyer and Bernd Schoeller},
  title = {Using Contracts and Boolean Queries to Improve the Quality of Automatic
    Test Generation},
  booktitle = {Proceedings of the International Conference on Tests and Proofs (TAP)
    2007},
  year = {2007},
  volume = {LNCS},
  publisher = {Springer-Verlag},
  abstract = {Since test cases cannot be exhaustive, any effective test case generation
    strategy must identify the execution states most likely to uncover
    bugs. The key issue is to define criteria for selecting such interesting
    states.


     If the units being tested are classes in object-oriented programming,
    it seems attractive to rely on the boolean queries present in each
    class, which indeed define criteria on the states of the corresponding
    objects, and  in contract-equipped O-O software  figure prominently
    in preconditions, postconditions and invariants. As these queries
    are part of the class specification and hence relevant to its clients,
    one may conjecture that the resulting partition of the state space
    is also relevant for tests.


     We explore this conjecture by examining whether relying on the boolean
    queries of a class to extract abstract states improves the results
    of black-box testing. The approach uses proof techniques to generate
    objects that satisfy the class invariants, then performs testing
    by relying on postconditions as test oracles.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\using_contracts_and_boolean_queries_to_improve_the_quality_of_automatic_test_generation.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2007.02.16},
}

@INPROCEEDINGS{lutz93targeting,
  author = {R. Lutz},
  title = {{Targeting Safety-Related Errors During Software Requirements Analysis}},
  booktitle = {Proc. {ACM} {SIGSOFT} FSE},
  year = {1993},
  pages = {99--106},
  url = {citeseer.ist.psu.edu/article/lutz96targeting.html},
}

@ARTICLE{Mankefors2003,
  author = {S. Mankefors and R. Torkar and A. Boklund},
  title = {New Quality Estimations in Random Testing},
  journal = {Proceedings of the 14th International Symposium on Software Reliability
    Engineering (ISSRE 2003)},
  year = {2003},
  volume = {00},
  pages = {468-478},
  abstract = {By reformulating the issue of random testing into an equivalent problem
    we are able to introduce a new kind of quality estimations based
    on Monte Carlo integration and the central limit theorem. This method
    also provides a limited but working "success theory" in the case
    of no detected failures. In an empirical evaluation using hundreds
    of billions of simulated tests we furthermore find a very good match
    between the quality estimations presented in this article and the
    true failure frequencies. Both simple modulus defects as well as
    seeded defects in two extensively employed numerical routines were
    subject to investigation in the empirical work.},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/ISSRE.2003.1251067},
  issn = {1071-9458},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\New quality estimations in random testing.pdf},
  publisher = {IEEE Computer Society},
  read = {only beginning},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=1251067},
}

@TECHREPORT{mathur93comparing,
  author = {A. Mathur and W. Wong},
  title = {Comparing the fault detection effectiveness of mutation and data
    flow testing:


     An empirical study},
  institution = {Software Engineering Research Center, Purdue University},
  year = {1993},
  number = {SERC-TR-146-P},
  month = {March},
}

@INPROCEEDINGS{Mayer2005,
  author = {Johannes Mayer},
  title = {Lattice-Based Adaptive Random Testing},
  booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated
    Software Engineering (ASE 2005)},
  year = {2005},
  series = {ACM},
  pages = {333-336},
  publisher = {ACM Press, New York, NY, USA},
  abstract = {Adaptive Random Testing (ART) denotes a family of testing algorithms
    that have a better performance compared to pure random testing with
    respect to the number of test cases necessary to detect the first
    failure. Many of these algorithms, however, are not very efficient
    regarding runtime. A new ART algorithm is presented that has a better
    performance than all other ART methods for the block failure pattern.
    Its runtime is linear in the number of test cases selected, which
    is nearly as efficient as pure random testing, as opposed to most
    other ART methods. This new ART algorithm selects the test cases
    based on a lattice.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Lattice-based adaptive random testing.pdf},
  read = {yes (not all)},
  relevance = {5},
  timestamp = {2006.04.14},
}

@INPROCEEDINGS{Mayer2005a,
  author = {Johannes Mayer},
  title = {Adaptive Random Testing by Bisection with Restriction},
  booktitle = {Proceedings of the Seventh International Conference on Formal Engineering
    Methods (ICFEM 2005)},
  year = {2005},
  series = {LNCS 3785},
  pages = {251-263},
  publisher = {Springer-Verlag, Berlin},
  abstract = {Random Testing is a strategy to select test cases based on pure randomness.
    Adaptive Random Testing (ART), a family of algorithms, improves
    pure Random Testing by taking common failure pattern into account.
    The bestin terms of the number of test cases necessary to detect
    the first failureART algorithms, however, are too runtime inefficient.
    Therefore, a modification of a fast, but not so good ART algorithm,
    namely ART by Bisection, is presented. This modification requires
    much less test cases than the original method while retaining its
    computational efficiency.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Adaptive Random Testing by Bisection with Restriction.pdf},
  read = {no},
  relevance = {5},
  timestamp = {2006.04.14},
}

@INPROCEEDINGS{Meinke2004,
  author = {Karl Meinke},
  title = {Automated black-box testing of functional correctness using function
    approximation},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
    on Software testing and analysis},
  year = {2004},
  pages = {143--153},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {We consider black-box testing of functional correctness as a special
    case of a satisfiability or constraint solving problem. We introduce
    a general method for solving this problem based on function approximation.
    We then describe some practical results obtained for an automated
    testing algorithm using approximation by piecewise polynomial functions.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Meinke04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007532},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
  pdf = {D:\Ilinca\Docs\Related work\Automated blckbox testing of functional correctness using function approximation.pdf},
  read = {yes},
  relevance = {4},
  url = {http://portal.acm.org/citation.cfm?id=1007532},
}

@ARTICLE{Menzies2000,
  author = {Tim Menzies and Bojan Cukic},
  title = {When to Test Less},
  journal = {IEEE Software},
  year = {2000},
  volume = {17},
  pages = {107-112},
  number = {5},
  month = {September - October},
  abstract = {Small-scale software projects usually cant afford to implement time-consuming
    and expensive tests. However, the authors show that in a surprisingly
    large number of cases, a small number of randomly selected tests
    will adequately probe the software.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\When to test less.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2006.04.10},
}

@INPROCEEDINGS{Meyer2005,
  author = {Bertrand Meyer},
  title = {Attached Types and their Application to Three Open Problems of Object-Oriented
    Programming},
  booktitle = {Proc. ECOOP},
  year = {2005},
  pages = {1-32},
  note = {Springer LNCS 3586},
}

@BOOK{Meyer1997,
  title = {Object-Oriented Software Construction, 2nd edition},
  publisher = {Prentice Hall},
  year = {1997},
  author = {B. Meyer},
  owner = {Ilinca},
  timestamp = {2007.01.23},
}

@INPROCEEDINGS{Meyer2007,
  author = {Bertrand Meyer and Ilinca Ciupa and Andreas Leitner and Lisa (Ling)
    Liu},
  title = {Automatic Testing of Object-Oriented Software},
  booktitle = {Proceedings of SOFSEM 2007 (Current Trends in Theory and Practice
    of Computer Science)},
  year = {2007},
  editor = {Jan van Leeuwen},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer-Verlag},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\papers\sofsem07\sofsem07.pdf},
  timestamp = {2007.02.08},
}

@ARTICLE{Miller1990,
  author = {Barton P. Miller and Louis Fredriksen and Bryan So},
  title = {An empirical study of the reliability of UNIX utilities},
  journal = {Commun. ACM},
  year = {1990},
  volume = {33},
  pages = {32--44},
  number = {12},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/96267.96279},
  issn = {0001-0782},
  publisher = {ACM Press},
}

@BOOK{Myers1979,
  title = {The Art of Software Testing},
  publisher = {John Wiley \& Sons},
  year = {1979},
  author = {Glenford J. Myers},
  owner = {Ilinca},
  read = {no},
  relevance = {5},
  timestamp = {2006.04.09},
}

@ARTICLE{ntafos:ctrlFlowTest:88,
  author = {D. Ntafos},
  title = {{A Comparison of Some Structural Testing Strategies}},
  journal = {IEEE TSE},
  year = {1988},
  volume = {14},
  pages = {868-874},
  number = {6},
  month = {June},
}

@INPROCEEDINGS{Ntafos1998,
  author = {Simeon Ntafos},
  title = {On random and partition testing},
  booktitle = {Proc. Intl. Symp. on Software Testing and Analysis},
  year = {1998},
  pages = {42--48},
  abstract = {There have been many comparisons of random and partition testing.
    Proportional partition testing has been suggested as the optimum
    way to perform partition testing. In this paper we show that this
    might not be so and discuss some of the problems with previous studies.
    We look at the expected cost of failures as a way to evaluate the
    effectiveness of testing strategies and use it to compare random
    testing, uniform partition testing and proportional partition testing.
    Also, we introduce partition testing strategies that try to take
    the cost of failures into account and present some results on their
    effectiveness.},
  doi = {http://doi.acm.org/10.1145/271771.271785},
  isbn = {0-89791-971-8},
  location = {Clearwater Beach, Florida, United States},
  pdf = {D:\Ilinca\Docs\Related work\on_random_and_partition_testing.pdf},
  read = {no},
  relevance = {5},
}

@INPROCEEDINGS{ntafos:coverageEffectiveness:84,
  author = {S. Ntafos},
  title = {An evaluation of required element testing strategies},
  booktitle = {Proc. ICSE},
  year = {1984},
  pages = {250--256},
}

@INPROCEEDINGS{Offutt1996,
  author = {A. Jefferson Offutt and J. Huffman Hayes},
  title = {A semantic model of program faults},
  booktitle = {ISSTA '96: Proceedings of the 1996 ACM SIGSOFT international symposium
    on Software testing and analysis},
  year = {1996},
  pages = {195--200},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Program faults are artifacts that are widely studied, but there are
    many aspects of faults that we still do not understand. In addition
    to the simple fact that one important goal during testing is to
    cause failures and thereby detect faults, a full understanding of
    the characteristics of faults is crucial to several research areas
    in testing. These include fault-based testing, testability, mutation
    testing, and the comparative evaluation of testing strategies. In
    this workshop paper, we explore the fundamental nature of faults
    by looking at the differences between a syntactic and semantic characterization
    of faults. We offer definitions of these characteristics and explore
    the differentiation. Specifically, we discuss the concept of "size"
    of program faults --- the measurement of size provides interesting
    and useful distinctions between the syntactic and semantic characterization
    of faults. We use the fault size observations to make several predictions
    about testing and present preliminary data that supports this model.
    We also use the model to offer explanations about several questions
    that have intrigued testing researchers.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Offutt96_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/229000.226317},
  isbn = {0-89791-787-1},
  location = {San Diego, California, United States},
  pdf = {D:\Ilinca\Docs\Related work\A semantic model of program faults.pdf},
  read = {yes},
  relevance = {5},
  url = {http://portal.acm.org/citation.cfm?id=226317#},
}

@ARTICLE{Offutt1999,
  author = {A. Jefferson Offutt and Zhenyi Jin and Jie Pan},
  title = {The dynamic domain reduction procedure for test data generation},
  journal = {Softw. Pract. Exper.},
  year = {1999},
  volume = {29},
  pages = {167--193},
  number = {2},
  abstract = {(Summary) Test data generation is one of the most technically challenging
    steps of testing software, but most commercial systems currently
    incorporate very little automation for this step. This paper presents
    results from a project that is trying to find ways to incorporate
    test data generation into practical test processes. The results
    include a new procedure for automatically generating test data that
    incorporates ideas from symbolic evaluation, constraint-based testing,
    and dynamic test data generation. It takes an initial set of values
    for each input, and dynamically pushes the values through the
    control-flow graph of the program, modifying the sets of values
    as branches in the program are taken. The result is usually a set
    of values for each input parameter that has the property that any
    choice from the sets will cause the path to be traversed. This procedure
    uses new analysis techniques, offers improvements over previous
    research results in constraint based testing, and combines several
    steps into one coherent process. The dynamic nature of this procedure
    yields several benefits. Moving through the control flow graph dynamically
    allows path constraints to be resolved immediately, which is more
    efficient both in space and time, and more often successful than
    constraint-based testing. This new procedure also incorporates an
    intelligent search technique based on bisection. The dynamic nature
    of this procedure also allows certain improvements to be made in
    the handling of arrays, loops, and expressions; language features
    that are traditionally difficult to handle in test data generation
    systems. The paper presents the test data generation procedure,
    examples to explain the working of the procedure, and results from
    a proof-of-concept implementation. Copyright 1999 John Wiley & Sons,
    Ltd.},
  address = {New York, NY, USA},
  doi = {http://dx.doi.org/10.1002/(SICI)1097-024X(199902)29:2<167::AID-SPE225>3.3.CO;2-M},
  issn = {0038-0644},
  pdf = {D:\Ilinca\Docs\Related work\The dynamic domain reduction procedure for test data generation.pdf},
  publisher = {John Wiley \& Sons, Inc.},
  read = {no},
  relevance = {4},
  url = {http://portal.acm.org/citation.cfm?id=309101#},
}

@TECHREPORT{Oriat2004,
  author = {Catherine Oriat},
  title = {Jartege: a Tool for Random Generation of Unit Tests for Java Classes},
  institution = {CNRS, Universite Joseph Fourier Grenoble I},
  year = {2004},
  number = {RR-1069-I},
  month = {June},
  abstract = {This report presents Jartege, a tool which allows random generation
    of unit tests for Java classes specified in JML. JML (Java Modeling
    Language) is a specification language for Java which allows one
    to write invariants for classes, and pre- and postconditions for
    operations. As in the JML-JUnit tool, we use JML specifications
    on the one hand to eliminate irrelevant test cases, and on the other
    hand as a test oracle. Jartege randomly generates test cases, which
    consist of a sequence of constructor and method calls for the classes
    under test. The random aspect of the tool can be parameterized by
    associating weights to classes and operations, and by controlling
    the number of instances which are created for each class under test.
    The practical use of Jartege is illustrated by a small case study.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Oriat04_mycomments.doc},
  keywords = {Testing, unit testing, random generation of test cases, Java, JML},
  pdf = {D:\Ilinca\Docs\Related work\Jartege_A_tool_for_random_generation_of_unit_tests_for_Java_classes.pdf},
  read = {yes},
  relevance = {6},
}

@ARTICLE{Ostrand1988,
  author = {T. J. Ostrand and M. J. Balcer},
  title = {The category-partition method for specifying and generating fuctional
    tests},
  journal = {Commun. ACM},
  year = {1988},
  volume = {31},
  pages = {676--686},
  number = {6},
  abstract = {A method for creating functional test suites has been developed in
    which a test engineer analyzes the system specification, writes
    a series of formal test specifications, and then uses a generator
    tool to produce test descriptions from which test scripts are written.
    The advantages of this method are that the tester can easily modify
    the test specification when necessary, and can control the complexity
    and number of the tests by annotating the tests specification with
    constraints.},
  address = {New York, NY, USA},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Ostrand88_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/62959.62964},
  issn = {0001-0782},
  pdf = {D:\Ilinca\Docs\Related work\The category-partition method.pdf},
  publisher = {ACM Press},
  read = {yes},
  relevance = {5},
  url = {http://portal.acm.org/citation.cfm?id=62964},
}

@INPROCEEDINGS{Pacheco2005,
  author = {Carlos Pacheco and Michael D. Ernst},
  title = {Eclat: Automatic generation and classification of test inputs},
  booktitle = {Proc. ECOOP},
  year = {2005},
  pages = {504--527},
  abstract = {This paper describes a technique that selects, from a large set of
    test inputs, a small subset likely to reveal faults in the software
    under test. The technique takes a program or software component,
    plus a set of correct executions  say, from observations of the
    software running properly, or from an existing test suite that a
    user wishes to enhance. The technique first infers an operational
    model of the software's operation. Then, inputs whose operational
    pattern of execution differs from the model in specific ways are
    suggestive of faults. These inputs are further reduced by selecting
    only one input per operational pattern. The result is a small portion
    of the original inputs, deemed by the technique as most likely to
    reveal faults. Thus, the technique can also be seen as an error-detection
    technique. The paper describes two additional techniques that complement
    test input selection. One is a technique for automatically producing
    an oracle (a set of assertions) for a test input from the operational
    model, thus transforming the test input into a test case. The other
    is a classification-guided test input generation technique that
    also makes use of operational models and patterns. When generating
    inputs, it filters out code sequences that are unlikely to contribute
    to legal inputs, improving the efficiency of its search for fault-revealing
    inputs. We have implemented these techniques in the Eclat tool,
    which generates unit tests for Java classes. Eclat's input is a
    set of classes to test and an example program execution  say, a
    passing test suite. Eclat's output is a set of JUnit test cases,
    each containing a potentially fault-revealing input and a set of
    assertions at least one of which fails. In our experiments, Eclat
    successfully generated inputs that exposed fault-revealing behavior;
    we have used Eclat to reveal real errors in programs. The inputs
    it selects as fault-revealing are an order of magnitude as likely
    to reveal a fault as all generated inputs.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Pacheco05_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\Eclat Automatic Generation and Classification of Test Inputs.pdf},
  read = {yes},
  relevance = {4},
}

@INPROCEEDINGS{Pacheco2007,
  author = {C. Pacheco and S. K. Lahiri and M. D. Ernst and T. Ball},
  title = {Feedback-directed random test generation},
  booktitle = {Proc. ICSE},
  year = {2007},
  pages = {75--84},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Feedback-directed random test generation.pdf},
  relevance = {6},
  timestamp = {2007.09.12},
}

@ARTICLE{1317478,
  author = {J. Ploski and M. Rohr and P. Schwenkenberg and W. Hasselbring},
  title = {Research issues in software fault categorization},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2007},
  volume = {32},
  pages = {6},
  number = {6},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/1317471.1317478},
  issn = {0163-5948},
  publisher = {ACM},
}

@INPROCEEDINGS{pretschner:mbt:05,
  author = {A. Pretschner and W. Prenninger and S. Wagner and C. K\"{u}hnel and
    M. Baumgartner and B. Sostawa and R. Z\"{o}lch and T. Stauner},
  title = {One evaluation of model-based testing and its automation},
  booktitle = {Proc. ICSE},
  year = {2005},
  pages = {392--401},
}

@INPROCEEDINGS{Richardson1989,
  author = {D. Richardson and O. O'Malley and C. Tittle},
  title = {Approaches to specification-based testing},
  booktitle = {TAV3: Proceedings of the ACM SIGSOFT '89 third symposium on Software
    testing, analysis, and verification},
  year = {1989},
  pages = {86--96},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Current software testing practices focus, almost exclusively, on the
    implementation, despite widely acknowledged benefits of testing
    based on software specifications. We propose approaches to specification-based
    testing by extending a wide variety of implementation-based testing
    techniques to be applicable to formal specification languages. We
    demonstrate these approaches for the Anna and Larch specification
    languages.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Richardson89_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/75308.75319},
  isbn = {0-89791-342-6},
  location = {Key West, Florida, United States},
  pdf = {D:\Ilinca\Docs\Related work\Approaches to specification-based testing.pdf},
  read = {yes},
  relevance = {5},
  special_observations = {this is the original paper formalizing specification-based testing},
  url = {http://portal.acm.org/citation.cfm?id=75319},
}

@ARTICLE{Rosenblum1995,
  author = {David S. Rosenblum},
  title = {A Practical Approach to Programming With Assertions},
  journal = {IEEE TSE},
  year = {1995},
  volume = {21},
  pages = {19--31},
  number = {1},
  abstract = {Embedded assertions have been recognized as a potentially powerful
    tool for automatic runtime detection of software faults during debugging,
    testing, maintenance and even production versions of software systems.
    Yet despite the richness of the notations and the maturity of the
    techniques and tools that have been developed for programming with
    assertions, assertions are a development tool that has seen little
    widespread use in practice. The main reasons seem to be that (1)
    previous assertion processing tools did not integrate easily with
    existing programming environments, and (2) it is not well understood
    what kinds of assertions are most effective at detecting software
    faults. This paper describes experience using an assertion processing
    tool that was built to address the concerns of ease-of-use and effectiveness.
    The tool is called APP, an Annotation PreProcessor for C programs
    developed in UNIX-based development environments. APP has been used
    in the development of a variety of software systems over the past
    five years. Based on this experience, the paper presents a classification
    of the assertions that were most effective at detecting faults.
    While the assertions that are described guard against many common
    kinds of faults and errors, the very commonness of such faults demonstrates
    the need for an explicit, high-level, automatically checkable specification
    of required behavior. It is hoped that the classification presented
    in this paper will prove to be a useful first step in developing
    a method of programming with assertions.},
  address = {Piscataway, NJ, USA},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Rosenblum95_mycomments.doc},
  doi = {http://dx.doi.org/10.1109/32.341844},
  issn = {0098-5589},
  pdf = {D:\Ilinca\Docs\Related work\A practical approach to programming with assertions.pdf},
  publisher = {IEEE Press},
  read = {yes},
  relevance = {5},
  special_observations = {This paper received the ICSE most influential paper award in 2002.},
  url = {http://portal.acm.org/citation.cfm?id=203111&coll=GUIDE&dl=GUIDE&CFID=46749401&CFTOKEN=25541718},
}

@INPROCEEDINGS{Satpathy2007,
  author = {Manoranjan Satpathy and Michael Butler and Michael Leuschel and S.
    Ramesh},
  title = {Automatic Testing from Formal Specifications},
  booktitle = {Proceedings of the International Conference on Tests and Proofs (TAP)
    2007},
  year = {2007},
  volume = {LNCS},
  publisher = {Springer-Verlag},
  abstract = {In this article, we consider model oriented formal specifica-

     tion languages. We generate test cases by performing symbolic execution

     over a model, and from the test cases obtain a Java program. This
    Java

     program acts as a test driver and when it is run in conjunction with
    the

     implementation then testing is performed in an automatic manner.
    Our

     approach makes the testing cycle fully automatic. The main contribution

     of our work is that we perform automatic testing even when the models

     are non-deterministic.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\automatic_testing_from_formal_specifications.pdf},
  read = {yes},
  relevance = {4},
  timestamp = {2007.02.16},
}

@INPROCEEDINGS{Shukla2004,
  author = {R. Shukla and P. Strooper and D. Carrington},
  title = {A Framework for Reliability Assessment of Software Components},
  booktitle = {Component-Based Software Engineering: 7th International Symposium,
    CBSE 2004, Edinburgh, UK, May 24-25, 2004. Proceedings},
  year = {2004},
  editor = {Ivica Crnkovic, Judith A. Stafford, Heinz W. Schmidt, et al.},
  volume = {3054 / 2004},
  pages = {272--279},
  publisher = {Springer-Verlag GmbH},
  abstract = {This paper proposes a conceptual framework for the reliability assessment
    of software components that incorporates test case execution and
    output evaluation. Determining an operational profile and test output
    evaluation are two difficult and important problems that must be
    addressed in such a framework. Determining an operational profile
    is difficult, because it requires anticipating the future use of
    the component. An expected result is needed for each test case to
    evaluate the test result and a test oracle is used to generate these
    expected results. The framework combines statistical testing and
    test oracles implemented as self-checking versions of the implementations.
    The framework is illustrated using two examples that were chosen
    to identify the issues that must be addressed to provide tool support
    for the framework.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Shukla04_mycomments.doc},
  doi = {10.1007/b97813},
  pdf = {D:\Ilinca\Docs\Related work\A Framework for Reliability Assessment of Software Components.pdf},
  read = {yes},
  relevance = {4},
  url = {http://springerlink.metapress.com/app/home/contribution.asp?wasp=ff1c3b88abb7441aa86174e572d4c35e&referrer=parent&backto=searcharticlesresults,1,1;},
}

@ARTICLE{Tillmann2006,
  author = {Nikolai Tillmann and Wolfram Schulte},
  title = {Unit Tests Reloaded: Parameterized Unit Testing with Symbolic Execution},
  journal = {IEEE Software},
  year = {2006},
  volume = {23},
  pages = {38--47},
  number = {4},
  abstract = {Unit tests are becoming popular. Are there ways to automate the generation
    of good unit tests? Parameterized unit tests are unit tests that
    depend on inputs. PUTs describe behavior more concisely than traditional
    unit tests. We use symbolic execution techniques and constraint
    solving to find inputs for PUTs that achieve high code coverage,
    to turn existing unit tests into PUTs, and to generate entirely
    new PUTs that describe an existing implementation's behavior. Traditional
    testing benefits from these techniques because test inputs - including
    the behavior of entire classes - can often be generated automatically
    from compact PUTs.},
  address = {Los Alamitos, CA, USA},
  doi = {http://dx.doi.org/10.1109/MS.2006.117},
  issn = {0740-7459},
  pdf = {D:\Ilinca\Docs\Related work\not read yet\Unit_tests_reloaded_parameterized_unit_testing_with_symbolic_execution.pdf},
  publisher = {IEEE Computer Society Press},
  read = {no},
  relevance = {6},
  url = {http://portal.acm.org/citation.cfm?id=1159169.1159389&coll=&dl=acm&CFID=15151515&CFTOKEN=6184618#},
}

@INPROCEEDINGS{Tonella2004,
  author = {Paolo Tonella},
  title = {Evolutionary testing of classes},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
    on Software testing and analysis},
  year = {2004},
  pages = {119--128},
  address = {New York, NY, USA},
  publisher = {ACM Press},
  abstract = {Object oriented programming promotes reuse of classes in multiple
    contexts. Thus, a class is designed and implemented with several
    usage scenarios in mind, some of which possibly open and generic.
    Correspondingly, the unit testing of classes cannot make too strict
    assumptions on the actual method invocation sequences, since these
    vary from application to application. In this paper, a genetic algorithm
    is exploited to automatically produce test cases for the unit testing
    of classes in a generic usage scenario. Test cases are described
    by chromosomes, which include information on which objects to create,
    which methods to invoke and which values to use as inputs. The proposed
    algorithm mutates them with the aim of maximizing a given coverage
    measure. The implementation of the algorithm and its application
    to classes from the Java standard library are described.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Tonella04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007528},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
  pdf = {D:\Ilinca\Docs\Related work\Evolutionary Testing of Classes.pdf},
  read = {yes},
  relevance = {6},
  url = {http://portal.acm.org/citation.cfm?id=1007528},
}

@TECHREPORT{Torshizi2006,
  author = {F. Torshizi and J. Ostroff},
  title = {ESpec -- a Tool for Agile Development via Early Testable Specifications},
  institution = {York University, Canada},
  year = {2006},
  number = {CS-2006-04},
  owner = {Ilinca},
  timestamp = {2008.01.21},
}

@TECHREPORT{upl:MBT:06,
  author = {M. Utting and A. Pretschner and B. Legeard},
  title = {A taxonomy of model-based testing},
  institution = {Department of Computer Science, The University of Waikato (New Zealand)},
  year = {2006},
  number = {04/2006},
  month = {April},
}

@TECHREPORT{Venolia2005,
  author = {Gina D. Venolia and Robert DeLine and Thomas LaToza},
  title = {Software Development at Microsoft Observed},
  institution = {Microsoft Research},
  year = {2005},
  number = {MSR-TR-2005-140},
  month = {October},
  abstract = {To understand Microsoft developers typical tools and work habits
    and their level of satisfaction with these, we performed two surveys
    and eleven interviews with developers across all business divisions.
    This report provides a summary of the resulting data. From the set
    of potential problems we gave them, the top three that Microsoft
    developers agree they have are: understanding the rationale behind
    a piece of code (66%); having to switch tasks often because of requests
    from teammates or managers (62%); and being aware of changes to
    code elsewhere that impact their own code (61%). The most notable
    take-away from the data is that developers go to great lengths to
    create and maintain rich mental models of code and dont rely on
    external representations. The mental nature of these models requires
    frequent, disruptive, face-to-face meetings to keep individuals
    models in sync, which greatly slow the rate at which a newcomer
    to a team can become productive. These interruptions also burden
    more senior development team members, as they have to recover what
    they were doing in the code following the interruption from team
    members.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Software_development_at_microsoft_observed.pdf},
  read = {no},
  relevance = {5},
  timestamp = {2006.11.10},
  url = {http://research.microsoft.com/research/pubs/view.aspx?type=Technical%20Report&id=994},
}

@INPROCEEDINGS{Visser2004,
  author = {Willem Visser and Corina S. Pasareanu and Sarfraz Khurshid},
  title = {{Test input generation with Java PathFinder}},
  booktitle = {Proc. Intl. Symp. on Software testing and analysis},
  year = {2004},
  pages = {97--107},
  abstract = {We show how model checking and symbolic execution can be used to generate
    test inputs to achieve structural coverage of code that manipulates
    complex data structures. We focus on obtaining branch-coverage during
    unit testing of some of the core methods of the red-black tree implementation
    in the Java TreeMap library, using the Java PathFinder model checker.
    Three different test generation techniques will be introduced and
    compared, namely, straight model checking of the code, model checking
    used in a black-box fashion to generate all inputs up to a fixed
    size, and lastly, model checking used during white-box test input
    generation. The main contribution of this work is to show how efficient
    white-box test input generation can be done for code manipulating
    complex data, taking into account complex method preconditions.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Visser04_mycomments.doc},
  doi = {http://doi.acm.org/10.1145/1007512.1007526},
  isbn = {1-58113-820-2},
  pdf = {D:\Ilinca\Docs\Related work\Test input generation with Java PathFinder.pdf},
  read = {yes},
  relevance = {6},
}

@INPROCEEDINGS{Werner2004,
  author = {Edith Werner and Helmut Neukirchen and Jens Grabowski},
  title = {{Self-adaptive Functional Testing of Services Working in Continuously
    Changing Contexts}},
  booktitle = {Proceedings of the 3rd Workshop on System Testing and Validation
    (SV04), December 2004, Fraunhofer Book Series},
  year = {2004},
  month = dec,
  abstract = {Web Services and ad-hoc networks are examples for systems that work
    in continuously changing contexts. The services provided by such
    systems can be tested in a laboratory environment using the standard
    conformance testing methodology. Testing the inter-working of services
    with other services in all contexts is an impossible task, because
    the possible usages of services may not be known or may increase
    due to the development of new services. Thus, errors may occur and
    have to be detected and handled at runtime. Our approach to this
    problem is self-adaptive functional testing, which is a combination
    of monitoring and active testing at runtime. In order to locate
    an error which has been detected during monitoring, existing test
    cases are adapted automatically to the specific situation and applied
    afterwards at runtime.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Werner04_mycomments.doc},
  pdf = {D:\Ilinca\Docs\Related work\Self_Adaptive_Functional_Testing_of_Services.pdf},
  read = {yes},
  relevance = {2},
  special_observations = {Met Edith Werner and Helmut Neukirchen in TAROT summer school in Paris
    (July 2005).},
}


@ARTICLE{weyukerJeng:RT:91,
  author = {E. Weyuker and B. Jeng},
  title = {{Analyzing Partition Testing Strategies}},
  journal = {IEEE TSE},
  year = {1991},
  volume = {17},
  pages = {703--711},
  number = {7},
}

@ARTICLE{267915,
  author = {M. Wood and M. Roper and A. Brooks and J. Miller},
  title = {Comparing and combining software defect detection techniques: a replicated
    empirical study},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {1997},
  volume = {22},
  pages = {262--277},
  number = {6},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/267896.267915},
  issn = {0163-5948},
  publisher = {ACM},
}

@INPROCEEDINGS{Xie2005,
  author = {Tao Xie and Darko Marinov and Wolfram Schulte and David Notkin},
  title = {{Symstra: A Framework for Generating Object-Oriented Unit Tests using
    Symbolic Execution}},
  booktitle = {Proc. Intl. Conf. on Tools and Algorithms for the Construction and
    Analysis of Systems},
  year = {2005},
  pages = {365--381},
  month = {April},
  abstract = {Object-oriented unit tests consist of sequences of method invocations.
    Behavior of an invocation depends on the methods arguments and
    the state of the receiver at the beginning of the invocation. Correspondingly,
    generating unit tests involves two tasks: generating method sequences
    that build relevant receiver object states and generating relevant
    method arguments. This paper proposes Symstra, a framework that
    achieves both test generation tasks using symbolic execution of
    method sequences with symbolic arguments. The paper defines symbolic
    states of object-oriented programs and novel comparisons of states.
    Given a set of methods from the class under test and a bound on
    the length of sequences, Symstra systematically explores the object-state
    space of the class and prunes this exploration based on the state
    comparisons. Experimental results show that Symstra generates unit
    tests that achieve higher branch coverage faster than the existing
    test-generation techniques based on concrete method arguments.},
  comments = {https://se.inf.ethz.ch/internal/people/ciupa/related_work/comments/Xie05_mycomments.doc},
  location = {Edinburgh, UK},
  pdf = {D:\Ilinca\Docs\Related work\Symstra.pdf},
  read = {yes},
  relevance = {6},
}

@INPROCEEDINGS{Xie2005a,
  author = {Tao Xie and David Notkin},
  title = {Automatically Identifying Special and Common Unit Tests for Object-Oriented
    Programs},
  booktitle = {16th IEEE International Symposium on Software Reliability Engineering},
  year = {2005},
  pages = {277--287},
  month = nov,
  abstract = {Developers often create common tests and special tests, which exercise
    common behaviors and special behaviors of the class under test,
    respectively. Although manually created tests are valuable, developers
    often overlook some special or even common tests. We have developed
    a new approach for automatically identifying special and common
    unit tests for a class without requiring any specification. Given
    a class, we automatically generate test inputs and identify common
    and special tests among the generated tests. Developers can inspect
    these identified tests and use them to augment existing tests. Our
    approach is based on statistical algebraic abstractions, program
    properties (in the form of algebraic specifications) dynamically
    inferred based on a set of predefined abstraction templates. We
    use statistical algebraic abstractions to characterize program behaviors
    and identify special and common tests. Our initial experience has
    shown that a relatively small number of common and special tests
    can be identified among a large number of generated tests and these
    identified tests expose common and special behaviors that deserve
    developers attention.},
  owner = {Ilinca},
  pdf = {D:\Ilinca\Docs\Related work\Automatically identifying special and common unit tests for oo programs.pdf},
  read = {yes},
  relevance = {5},
  timestamp = {2006.05.19},
  url = {http://www-static.cc.gatech.edu/~csallnch/jcrasher/uses/xie05automatically-abstract.html},
}

@INPROCEEDINGS{Xu2004,
  author = {Guoqing Xu and Zongyuang Yang},
  title = {JMLAutoTest: A Novel Automated Testing Framework Based on JML and
    JUnit},
  booktitle = {Formal Approaches to Software Testing: Third International Workshop,
    FATES 2003, Montreal, Quebec, Canada, October 6th, 2003. Revised
    Papers},
  year = {2004},
  editor = {Alexandre Petrenko, Andreas Ulrich},
  volume = {2931 / 2004},
  pages = {70--85},
  publisher = {Springer-Verlag GmbH},
  abstract = {Writing specifications using Java Modeling Language has been accepted
    for a long time as a practical approach to increasing the correctness
    and quality of Java programs. However, the current JML testing system
    (the JML and JUnit framework) can only generate skeletons of test
    fixture and test case class. Writing codes for generating test cases,
    especially those with a complicated data structure is still a labor-intensive
    job in the test for programs annotated with JML specifications.
    This paper presents JMLAutoTest, a novel framework for automated
    testing of Java programs annotated with JML specifications. Firstly,
    given a method, three test classes (a skeleton of test client class,
    a JUnit test class and a test case class) can be generated. Secondly,
    JMLAutoTest can generate all nonisomorphic test cases that satisfy
    the requirements defined in the test client class. Thirdly, JMLAutoTest
    can avoid most meaningless cases by running the test in a double-phase
    way which saves much time of exploring meaningless cases in the
    test. This method can be adopted in the testing not only for Java
    programs, but also for programs written with other languages. Finally,
    JMLAutoTest executes the method and uses JML runtime assertion checker
    to decide whether its post-condition is violated. That is whether
    the method works correctly.},
  doi = {10.1007/b95400},
  pdf = {D:\Ilinca\Docs\Related work\JMLAutoTest.pdf},
  read = {yes},
  relevance = {4},
  url = {http://springerlink.metapress.com/app/home/contribution.asp?wasp=f3901fa4fa5e4a22a1bbd3b089ae8436&referrer=parent&backto=searcharticlesresults,1,1;},
}

@ARTICLE{zeller:deltaDebugging:02,
  author = {A. Zeller and R. Hildebrandt},
  title = {Simplifying and isolating failure-inducing input},
  journal = {IEEE TSE},
  year = {2002},
  volume = {SE-28},
  pages = {183--202},
  number = {2},
  month = {February},
}

@INPROCEEDINGS{Zhang1995,
  author = {Kaizhong Zhang and Jason Tsong-Li Wang and Dennis Shasha},
  title = {On the Editing Distance between Undirected Acyclic Graphs and Related
    Problems},
  booktitle = {Proceedings of the 6th Annual Symposium on Combinatorial Pattern
    Matching},
  year = {1995},
  pages = {395-407},
  publisher = {Springer Verlag, Berlin},
  owner = {Ilinca},
  timestamp = {2007.04.26},
}

@MISC{AsmLTest,
  title = {Parameter generation in the AsmL Test Generator tool},
  howpublished = {on web page},
  comment = {- They use ADF (Access Driven Filtering) to generate parameters. -
    The user has to write a IsInterestingParameter () predicate (it
    should also be the invariant, I think) - User has to do a lot of
    configuration},
  read = {yes},
  relevance = {5},
}

@MISC{EiffelBase,
  title = {The EiffelBase Library. Eiffel Software Inc. http://www.eiffel.com/},
  owner = {Ilinca},
  timestamp = {2007.01.24},
  url = {www.eiffel.com},
}

@MISC{Gobo,
  title = {The Gobo Eiffel Library. http://www.gobosoft.com/},
  howpublished = {web site},
  owner = {Ilinca},
  timestamp = {2007.01.24},
  url = {www.gobosoft.com},
}

@MISC{Jtest,
  title = {{Jtest}. {Parasoft Corporation}. http://www.parasoft.com/},
  owner = {Ilinca},
  timestamp = {2007.01.22},
  url = {http://www.parasoft.com/jsp/products/home.jsp?product=Jtest},
}

@ARTICLE{392549,
  title = {{IEEE standard classification for software anomalies}},
  journal = {{IEEE Std 1044-1993}},
  year = {2 Jun 1994},
}


@article{267590,
 author = {H. Zhu and P. Hall and J. May},
 title = {Software unit test coverage and adequacy},
 journal = {ACM Comput. Surv.},
 volume = {29},
 number = {4},
 year = {1997},
 issn = {0360-0300},
 pages = {366--427},
 doi = {http://doi.acm.org/10.1145/267580.267590},
 publisher = {ACM},
 address = {New York, NY, USA},
 }

@INPROCEEDINGS{Morasca2004,
  author = {Sandro Morasca and Stefano Serra-Capizzano},
  title = {On the analytical comparison of testing techniques},
  booktitle = {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT international symposium
	on Software testing and analysis},
  year = {2004},
  pages = {154--164},
  address = {New York, NY, USA},
  publisher = {ACM},
  doi = {http://doi.acm.org/10.1145/1007512.1007533},
  isbn = {1-58113-820-2},
  location = {Boston, Massachusetts, USA},
}
